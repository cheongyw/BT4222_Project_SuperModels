{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D6qOq0CPml_G"
   },
   "source": [
    "### LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "aqswqyzCBl11",
    "outputId": "9da210d7-fdb6-4eda-dbaa-53d8c7192725"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: pandas in /usr/local/lib/python3.6/dist-packages (0.24.2)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.14.6)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.5.3)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.5.0->pandas) (1.11.0)\n"
     ]
    }
   ],
   "source": [
    "# restart kernel after running this cell\n",
    "# !pip install pandas --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "id": "fIFW9iPEA8u3",
    "outputId": "6c70cb93-7856-4b1d-a040-b144fb91c638"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wei Han\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Wei\n",
      "[nltk_data]     Han\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Wei\n",
      "[nltk_data]     Han\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import scipy\n",
    "import re\n",
    "import ast\n",
    "import pickle\n",
    "import time\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, PunktSentenceTokenizer\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from scipy.spatial.distance import cdist, cosine\n",
    "import time\n",
    "import multiprocessing\n",
    "from gensim.models import Word2Vec, FastText\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import math\n",
    "import json\n",
    "import random\n",
    "\n",
    "from IPython import display\n",
    "from matplotlib import cm\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.data import Dataset\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from itertools import chain\n",
    "import time\n",
    "import multiprocessing\n",
    "import copy\n",
    "import pickle\n",
    "import datetime as dt\n",
    "import h5py\n",
    "import re\n",
    "from subprocess import call\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import load_model\n",
    "from keras import Model\n",
    "from keras import backend as K\n",
    "from keras.utils import to_categorical\n",
    "import gc\n",
    "from keras.layers import Input, LSTM, Bidirectional, Dense, Embedding, Flatten\n",
    "from gensim.models import Word2Vec\n",
    "%matplotlib inline\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_p = pd.read_pickle(\"./Data/combined.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['time',\n",
       " 'assetCode',\n",
       " 'assetName',\n",
       " 'volume',\n",
       " 'close',\n",
       " 'open',\n",
       " 'returnsClosePrevRaw1',\n",
       " 'returnsOpenPrevRaw1',\n",
       " 'returnsClosePrevMktres1',\n",
       " 'returnsOpenPrevMktres1',\n",
       " 'returnsClosePrevRaw10',\n",
       " 'returnsOpenPrevRaw10',\n",
       " 'returnsClosePrevMktres10',\n",
       " 'returnsOpenPrevMktres10',\n",
       " 'returnsOpenNextMktres10',\n",
       " 'universe',\n",
       " 'open_close',\n",
       " 'oc_average',\n",
       " 'turnover',\n",
       " 'open_close_relative',\n",
       " 'turnover_relative',\n",
       " 'volume_relative',\n",
       " 'returnsOpenPrevMktres1_relative',\n",
       " 'returnsOpenPrevMktres10_relative',\n",
       " 'urgency_min',\n",
       " 'urgency_count',\n",
       " 'takeSequence_min',\n",
       " 'takeSequence_max',\n",
       " 'bodySize_mean',\n",
       " 'wordCount_mean',\n",
       " 'sentenceCount_mean',\n",
       " 'companyCount_mean',\n",
       " 'marketCommentary_mean',\n",
       " 'relevance_mean',\n",
       " 'sentimentNegative_mean',\n",
       " 'sentimentNeutral_mean',\n",
       " 'sentimentPositive_mean',\n",
       " 'sentimentWordCount_mean',\n",
       " 'noveltyCount12H_mean',\n",
       " 'noveltyCount24H_mean',\n",
       " 'noveltyCount3D_mean',\n",
       " 'noveltyCount5D_mean',\n",
       " 'noveltyCount7D_mean',\n",
       " 'volumeCounts12H_mean',\n",
       " 'volumeCounts24H_mean',\n",
       " 'volumeCounts3D_mean',\n",
       " 'volumeCounts5D_mean',\n",
       " 'volumeCounts7D_mean',\n",
       " 'headline',\n",
       " 'dayofweek',\n",
       " 'month']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_p.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "aRHMnKEyvSzb",
    "outputId": "855c555e-e355-430f-d921-6b0c18e75d3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9.71 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "combined_data = pd.read_pickle('./Data/new_df.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data.sort_values('time', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jP5-PVwVYdJW"
   },
   "outputs": [],
   "source": [
    "# store resuls\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "AG7VOEKuNqDW",
    "outputId": "c3e33794-f48d-48d2-b6cb-c01b709d9f2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 16 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# month from 0 to 11\n",
    "combined_data['month'] = combined_data['month'] - 1\n",
    "\n",
    "# missing value imputation\n",
    "\n",
    "group_mean_features = [\n",
    "    'returnsClosePrevMktres1',\n",
    "    'returnsOpenPrevMktres1',\n",
    "    'returnsClosePrevMktres10',\n",
    "    'returnsOpenPrevMktres10',\n",
    "    'returnsOpenPrevMktres1_relative',\n",
    "    'returnsOpenPrevMktres10_relative',\n",
    "]\n",
    "\n",
    "combined_data['headline'].fillna('', inplace=True)\n",
    "combined_data['urgency_min'].fillna(5.0, inplace=True)\n",
    "\n",
    "combined_data[group_mean_features] = combined_data.groupby(\"assetCode\")[group_mean_features].transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "combined_data = combined_data[~combined_data['returnsClosePrevMktres10'].isnull()]\n",
    "\n",
    "combined_data.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "IAOb-QZ5RUGK",
    "outputId": "45b3e7f2-d280-4353-b006-9edf10f62365",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3916997, 101)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "colab_type": "code",
    "id": "xkMQyDd3wdHV",
    "outputId": "b943065c-bb4a-4c9f-9657-b09015a0d40e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['time', 'assetCode', 'assetName', 'volume', 'close', 'open',\n",
       "       'returnsClosePrevRaw1', 'returnsOpenPrevRaw1',\n",
       "       'returnsClosePrevMktres1', 'returnsOpenPrevMktres1',\n",
       "       ...\n",
       "       'noveltyCount3D_mean_stan', 'noveltyCount5D_mean_stan',\n",
       "       'noveltyCount7D_mean_stan', 'volumeCounts12H_mean_stan',\n",
       "       'volumeCounts24H_mean_stan', 'volumeCounts3D_mean_stan',\n",
       "       'volumeCounts5D_mean_stan', 'volumeCounts7D_mean_stan',\n",
       "       'dayofweek_stan', 'month_stan'],\n",
       "      dtype='object', length=101)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 440
    },
    "colab_type": "code",
    "id": "D4anoNVSxxyK",
    "outputId": "1a5bd821-de5b-4e20-aa18-1ab99388c231"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>assetCode</th>\n",
       "      <th>assetName</th>\n",
       "      <th>volume</th>\n",
       "      <th>close</th>\n",
       "      <th>open</th>\n",
       "      <th>returnsClosePrevRaw1</th>\n",
       "      <th>returnsOpenPrevRaw1</th>\n",
       "      <th>returnsClosePrevMktres1</th>\n",
       "      <th>returnsOpenPrevMktres1</th>\n",
       "      <th>...</th>\n",
       "      <th>noveltyCount3D_mean_stan</th>\n",
       "      <th>noveltyCount5D_mean_stan</th>\n",
       "      <th>noveltyCount7D_mean_stan</th>\n",
       "      <th>volumeCounts12H_mean_stan</th>\n",
       "      <th>volumeCounts24H_mean_stan</th>\n",
       "      <th>volumeCounts3D_mean_stan</th>\n",
       "      <th>volumeCounts5D_mean_stan</th>\n",
       "      <th>volumeCounts7D_mean_stan</th>\n",
       "      <th>dayofweek_stan</th>\n",
       "      <th>month_stan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4053824</th>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>WEB.O</td>\n",
       "      <td>Web.com Group Inc</td>\n",
       "      <td>313808.0</td>\n",
       "      <td>21.150000</td>\n",
       "      <td>21.049999</td>\n",
       "      <td>0.007143</td>\n",
       "      <td>0.009592</td>\n",
       "      <td>0.017020</td>\n",
       "      <td>0.008069</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2735347</th>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>FMBI.O</td>\n",
       "      <td>First Midwest Bancorp Inc</td>\n",
       "      <td>241696.0</td>\n",
       "      <td>25.230000</td>\n",
       "      <td>25.230000</td>\n",
       "      <td>-0.000792</td>\n",
       "      <td>-0.002372</td>\n",
       "      <td>0.000730</td>\n",
       "      <td>-0.002542</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1554928</th>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>NG.A</td>\n",
       "      <td>NovaGold Resources Inc</td>\n",
       "      <td>2227458.0</td>\n",
       "      <td>4.560000</td>\n",
       "      <td>4.740000</td>\n",
       "      <td>-0.031847</td>\n",
       "      <td>0.062780</td>\n",
       "      <td>-0.029414</td>\n",
       "      <td>0.063522</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4070552</th>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>PSXP.N</td>\n",
       "      <td>Phillips 66 Partners LP</td>\n",
       "      <td>241176.0</td>\n",
       "      <td>48.639999</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>0.012490</td>\n",
       "      <td>-0.012346</td>\n",
       "      <td>0.015263</td>\n",
       "      <td>-0.013469</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2967626</th>\n",
       "      <td>2016-12-30</td>\n",
       "      <td>GTLS.O</td>\n",
       "      <td>Chart Industries Inc</td>\n",
       "      <td>183875.0</td>\n",
       "      <td>36.020000</td>\n",
       "      <td>36.400002</td>\n",
       "      <td>-0.011526</td>\n",
       "      <td>-0.003286</td>\n",
       "      <td>-0.002773</td>\n",
       "      <td>-0.005509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              time assetCode                  assetName     volume      close  \\\n",
       "4053824 2016-12-30     WEB.O          Web.com Group Inc   313808.0  21.150000   \n",
       "2735347 2016-12-30    FMBI.O  First Midwest Bancorp Inc   241696.0  25.230000   \n",
       "1554928 2016-12-30      NG.A     NovaGold Resources Inc  2227458.0   4.560000   \n",
       "4070552 2016-12-30    PSXP.N    Phillips 66 Partners LP   241176.0  48.639999   \n",
       "2967626 2016-12-30    GTLS.O       Chart Industries Inc   183875.0  36.020000   \n",
       "\n",
       "              open  returnsClosePrevRaw1  returnsOpenPrevRaw1  \\\n",
       "4053824  21.049999              0.007143             0.009592   \n",
       "2735347  25.230000             -0.000792            -0.002372   \n",
       "1554928   4.740000             -0.031847             0.062780   \n",
       "4070552  48.000000              0.012490            -0.012346   \n",
       "2967626  36.400002             -0.011526            -0.003286   \n",
       "\n",
       "         returnsClosePrevMktres1  returnsOpenPrevMktres1  ...  \\\n",
       "4053824                 0.017020                0.008069  ...   \n",
       "2735347                 0.000730               -0.002542  ...   \n",
       "1554928                -0.029414                0.063522  ...   \n",
       "4070552                 0.015263               -0.013469  ...   \n",
       "2967626                -0.002773               -0.005509  ...   \n",
       "\n",
       "         noveltyCount3D_mean_stan  noveltyCount5D_mean_stan  \\\n",
       "4053824                       0.0                       0.0   \n",
       "2735347                       0.0                       0.0   \n",
       "1554928                       0.0                       0.0   \n",
       "4070552                       0.0                       0.0   \n",
       "2967626                       0.0                       0.0   \n",
       "\n",
       "         noveltyCount7D_mean_stan  volumeCounts12H_mean_stan  \\\n",
       "4053824                       0.0                        0.0   \n",
       "2735347                       0.0                        0.0   \n",
       "1554928                       0.0                        0.0   \n",
       "4070552                       0.0                        0.0   \n",
       "2967626                       0.0                        0.0   \n",
       "\n",
       "         volumeCounts24H_mean_stan  volumeCounts3D_mean_stan  \\\n",
       "4053824                        0.0                       0.0   \n",
       "2735347                        0.0                       0.0   \n",
       "1554928                        0.0                       0.0   \n",
       "4070552                        0.0                       0.0   \n",
       "2967626                        0.0                       0.0   \n",
       "\n",
       "         volumeCounts5D_mean_stan  volumeCounts7D_mean_stan  dayofweek_stan  \\\n",
       "4053824                       0.0                       0.0             0.0   \n",
       "2735347                       0.0                       0.0             0.0   \n",
       "1554928                       0.0                       0.0             0.0   \n",
       "4070552                       0.0                       0.0             0.0   \n",
       "2967626                       0.0                       0.0             0.0   \n",
       "\n",
       "         month_stan  \n",
       "4053824         0.0  \n",
       "2735347         0.0  \n",
       "1554928         0.0  \n",
       "4070552         0.0  \n",
       "2967626         0.0  \n",
       "\n",
       "[5 rows x 101 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "eO3Mnm8i8B8n",
    "outputId": "3a3f8f1f-d4a6-41d1-be52-8c87d9b77d93"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3430,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_data['assetCode'].unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dzJ237G48FXJ"
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # sampling\n",
    "\n",
    "# stocks = train_data['assetCode'].unique()\n",
    "\n",
    "# # N = min(100, stocks.shape[0])\n",
    "# N = stocks.shape[0]\n",
    "\n",
    "# np.random.seed(1)\n",
    "# selected_stocks = set(np.random.choice(stocks, N))\n",
    "# train_data = train_data[train_data['assetCode'].map(lambda x: x in selected_stocks)]\n",
    "# test_data = test_data[test_data['assetCode'].map(lambda x: x in selected_stocks)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cqIPpkuWAbJ0"
   },
   "source": [
    "### Model Building v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oUyw6Lkv1sxG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 361116 samples, validate on 165816 samples\n",
      "Epoch 1/1\n",
      "361116/361116 [==============================] - 88s 244us/step - loss: 0.6682 - acc: 0.5884 - val_loss: 0.7136 - val_acc: 0.5301\n",
      "current sigma: 0.263989\n",
      "Train on 361116 samples, validate on 165816 samples\n",
      "Epoch 1/1\n",
      "361116/361116 [==============================] - 100s 278us/step - loss: 0.6473 - acc: 0.6185 - val_loss: 0.7216 - val_acc: 0.5315\n",
      "current sigma: 0.291399\n",
      "Train on 361116 samples, validate on 165816 samples\n",
      "Epoch 1/1\n",
      "361116/361116 [==============================] - 94s 260us/step - loss: 0.6319 - acc: 0.6358 - val_loss: 0.7355 - val_acc: 0.5262\n",
      "current sigma: 0.240066\n",
      "Year 2011: sigma=0.240066\n",
      "[[46519 41597]\n",
      " [36974 40726]]\n",
      "Train on 369432 samples, validate on 168336 samples\n",
      "Epoch 1/1\n",
      "369432/369432 [==============================] - 86s 234us/step - loss: 0.6679 - acc: 0.5930 - val_loss: 0.6866 - val_acc: 0.5653\n",
      "current sigma: 0.825409\n",
      "Train on 369432 samples, validate on 168336 samples\n",
      "Epoch 1/1\n",
      "369432/369432 [==============================] - 88s 239us/step - loss: 0.6552 - acc: 0.6093 - val_loss: 0.6940 - val_acc: 0.5635\n",
      "current sigma: 0.846958\n",
      "Train on 369432 samples, validate on 168336 samples\n",
      "Epoch 1/1\n",
      "369432/369432 [==============================] - 88s 239us/step - loss: 0.6443 - acc: 0.6217 - val_loss: 0.7095 - val_acc: 0.5585\n",
      "current sigma: 0.793952\n",
      "Year 2012: sigma=0.793952\n",
      "[[42408 42020]\n",
      " [32297 51611]]\n",
      "Train on 369684 samples, validate on 160020 samples\n",
      "Epoch 1/1\n",
      "369684/369684 [==============================] - 85s 231us/step - loss: 0.6645 - acc: 0.5921 - val_loss: 0.6930 - val_acc: 0.5648\n",
      "current sigma: 0.668645\n",
      "Train on 369684 samples, validate on 160020 samples\n",
      "Epoch 1/1\n",
      "369684/369684 [==============================] - 90s 243us/step - loss: 0.6449 - acc: 0.6186 - val_loss: 0.7173 - val_acc: 0.5595\n",
      "current sigma: 0.609352\n",
      "Year 2013: sigma=0.609352\n",
      "[[50736 30678]\n",
      " [39812 38794]]\n",
      "Train on 350028 samples, validate on 165816 samples\n",
      "Epoch 1/1\n",
      "350028/350028 [==============================] - 83s 238us/step - loss: 0.6587 - acc: 0.5982 - val_loss: 0.7078 - val_acc: 0.5267\n",
      "current sigma: 0.269909\n",
      "Train on 350028 samples, validate on 165816 samples\n",
      "Epoch 1/1\n",
      "350028/350028 [==============================] - 86s 247us/step - loss: 0.6394 - acc: 0.6253 - val_loss: 0.7273 - val_acc: 0.5133\n",
      "current sigma: 0.163212\n",
      "Year 2014: sigma=0.163212\n",
      "[[42704 37987]\n",
      " [42710 42415]]\n",
      "Train on 378000 samples, validate on 180432 samples\n",
      "Epoch 1/1\n",
      "378000/378000 [==============================] - 90s 238us/step - loss: 0.6693 - acc: 0.5876 - val_loss: 0.7075 - val_acc: 0.5262\n",
      "current sigma: 0.432352\n",
      "Train on 378000 samples, validate on 180432 samples\n",
      "Epoch 1/1\n",
      "378000/378000 [==============================] - 91s 240us/step - loss: 0.6454 - acc: 0.6217 - val_loss: 0.7322 - val_acc: 0.5164\n",
      "current sigma: 0.385548\n",
      "Year 2015: sigma=0.385548\n",
      "[[56099 29842]\n",
      " [57408 37083]]\n",
      "Train on 2187864 samples, validate on 184968 samples\n",
      "Epoch 1/1\n",
      "2187864/2187864 [==============================] - 513s 235us/step - loss: 0.6722 - acc: 0.5809 - val_loss: 0.7055 - val_acc: 0.5402\n",
      "current sigma: 0.490362\n",
      "Train on 2187864 samples, validate on 184968 samples\n",
      "Epoch 1/1\n",
      "2187864/2187864 [==============================] - 475s 217us/step - loss: 0.6597 - acc: 0.6005 - val_loss: 0.7125 - val_acc: 0.5374\n",
      "current sigma: 0.433380\n",
      "Year 2016: sigma=0.433380\n",
      "[[42566 48370]\n",
      " [37190 56842]]\n",
      "Wall time: 41min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "v_str = 'v1'\n",
    "\n",
    "train_end_years = [2011, 2012, 2013, 2014, 2015, 2016]\n",
    "\n",
    "lstm_features_list = [\n",
    "    'returnsClosePrevRaw1',\n",
    "    'returnsOpenPrevRaw1',\n",
    "    'returnsClosePrevMktres1',\n",
    "    'returnsOpenPrevMktres1',\n",
    "    'returnsClosePrevRaw10',\n",
    "    'returnsOpenPrevRaw10',\n",
    "    'returnsClosePrevMktres10',\n",
    "    'returnsOpenPrevMktres10',\n",
    "    'open_close',\n",
    "    'oc_average',\n",
    "    'turnover',\n",
    "    'open_close_relative',\n",
    "    'turnover_relative',\n",
    "    'volume_relative',\n",
    "    'returnsOpenPrevMktres1_relative',\n",
    "    'returnsOpenPrevMktres10_relative',\n",
    "]\n",
    "\n",
    "\n",
    "additional_features_list = [\n",
    "    'urgency_min',\n",
    "    'urgency_count',\n",
    "    'takeSequence_min',\n",
    "    'takeSequence_max',\n",
    "    'bodySize_mean',\n",
    "    'wordCount_mean',\n",
    "    'sentenceCount_mean',\n",
    "    'companyCount_mean',\n",
    "    'marketCommentary_mean',\n",
    "    'relevance_mean',\n",
    "    'noveltyCount12H_mean',\n",
    "    'noveltyCount24H_mean',\n",
    "    'noveltyCount3D_mean',\n",
    "    'noveltyCount5D_mean',\n",
    "    'noveltyCount7D_mean',\n",
    "    'volumeCounts12H_mean',\n",
    "    'volumeCounts24H_mean',\n",
    "    'volumeCounts3D_mean',\n",
    "    'volumeCounts5D_mean',\n",
    "    'volumeCounts7D_mean',\n",
    "]\n",
    "\n",
    "label_encoded_features = [\n",
    "    'dayofweek',\n",
    "    'month',\n",
    "]\n",
    "\n",
    "continuous_features_list = lstm_features_list + additional_features_list\n",
    "features_list = lstm_features_list + additional_features_list + label_encoded_features\n",
    "\n",
    "for train_end_year in train_end_years:\n",
    "  # a trick to get testing stats\n",
    "  if train_end_year == 2016:\n",
    "    train_end_year = 2015\n",
    "    train_start, train_end = pd.to_datetime('2011-01-01'), pd.to_datetime('2016-06-30')\n",
    "    test_start, test_end = pd.to_datetime('%d-07-01' % (train_end_year + 1)), pd.to_datetime('%d-12-31' % (train_end_year + 1))\n",
    "    train_end_year = 2016\n",
    "  else:\n",
    "    train_start, train_end = pd.to_datetime('%d-01-01' % train_end_year), pd.to_datetime('%d-12-31' % train_end_year)\n",
    "    test_start, test_end = pd.to_datetime('%d-01-01' % (train_end_year + 1)), pd.to_datetime('%d-06-30' % (train_end_year + 1))\n",
    "  \n",
    "  \n",
    "  # divide into train and test periods\n",
    "\n",
    "  train_data = combined_data.set_index('time')[train_start: train_end]\n",
    "  test_data = combined_data.set_index('time')[test_start: test_end]\n",
    "\n",
    "  from sklearn.preprocessing import StandardScaler\n",
    "  scaler = StandardScaler()\n",
    "  train_data[continuous_features_list] = scaler.fit_transform(train_data[continuous_features_list])\n",
    "  test_data[continuous_features_list] = scaler.transform(test_data[continuous_features_list])\n",
    "\n",
    "  # reshape data\n",
    "  word2vec_features_list = []\n",
    "\n",
    "  def reshape_data(data, sequence_len=21, features_list=features_list):\n",
    "    data_list, ann_list, embedding_list, word2vec_list, target_list, returns_list, date_list = [], [], [], [], [], [], []\n",
    "\n",
    "    for assetCode, asset_data in data.groupby('assetCode'):\n",
    "      # binarize the target variable\n",
    "      returns = asset_data['returnsOpenNextMktres10']\n",
    "      target = (asset_data['returnsOpenNextMktres10'] > 0).astype(int)\n",
    "      dates = asset_data.index\n",
    "\n",
    "      # arrays\n",
    "      lstm_data = asset_data[lstm_features_list].values\n",
    "      ann_data = asset_data[additional_features_list].values\n",
    "      embedding_data = asset_data[label_encoded_features].values\n",
    "      word2vec_data = asset_data[word2vec_features_list].values\n",
    "\n",
    "      col_mean = np.nanmean(lstm_data, axis = 0) \n",
    "      inds = np.where(np.isnan(lstm_data)) \n",
    "      lstm_data[inds] = np.take(col_mean, inds[1])\n",
    "\n",
    "      reshaped_data = np.repeat(lstm_data[:, np.newaxis, :], sequence_len, axis=1)\n",
    "      for i in range(reshaped_data.shape[1]):\n",
    "        reshaped_data[:, i, :] = np.roll(reshaped_data[:, i, :], i, axis=0)\n",
    "\n",
    "      # discard the top rows and reverse the order of the shifted sequence to reflect the correct order\n",
    "      reshaped_data = reshaped_data[sequence_len-1:, ::-1, :]\n",
    "      ann_data = ann_data[sequence_len-1:, :]\n",
    "      embedding_data = embedding_data[sequence_len-1:, :]\n",
    "      word2vec_data = word2vec_data[sequence_len-1:, :]\n",
    "      target = target.values[sequence_len-1:]\n",
    "      returns = returns.values[sequence_len-1:]\n",
    "      dates = dates[sequence_len-1:]\n",
    "\n",
    "      data_list.append(reshaped_data)\n",
    "      ann_list.append(ann_data)\n",
    "      embedding_list.append(embedding_data)\n",
    "      word2vec_list.append(word2vec_data)\n",
    "      target_list.append(target)\n",
    "      returns_list.append(returns)\n",
    "      date_list.append(dates)\n",
    "\n",
    "    reshaped_data, ann_data, embedding_data, word2vec_data, target, returns, dates = np.concatenate(data_list), np.concatenate(ann_list), np.concatenate(embedding_list), np.concatenate(word2vec_list), np.concatenate(target_list), np.concatenate(returns_list), np.concatenate(date_list)\n",
    "\n",
    "    return (reshaped_data, ann_data, embedding_data, word2vec_data), target, returns, dates\n",
    "\n",
    "  x_train_tuple, y_train, train_returns, train_dates = reshape_data(train_data)\n",
    "  x_test_tuple, y_test, test_returns, test_dates = reshape_data(test_data)\n",
    "\n",
    "  x_train_lstm, x_train_ann, x_train_embedding, x_train_word2vec = x_train_tuple\n",
    "  x_test_lstm, x_test_ann, x_test_embedding, x_test_word2vec = x_test_tuple\n",
    "  \n",
    "  # create model\n",
    "  from keras.layers import Input, Embedding, LSTM, Dense\n",
    "  from keras.models import Model\n",
    "\n",
    "  lstm_params = {\n",
    "      'batch_size': 252,\n",
    "      'epochs': 5,\n",
    "  }\n",
    "\n",
    "  train_len = x_train_lstm.shape[0]\n",
    "  test_len = x_test_lstm.shape[0]\n",
    "\n",
    "  # tweak to match with batch_size\n",
    "  train_until = train_len - train_len % lstm_params['batch_size']\n",
    "  test_until = test_len - test_len % lstm_params['batch_size']\n",
    "\n",
    "  x_train_lstm, x_train_ann, x_train_embedding, x_train_word2vec = x_train_lstm[:train_until], x_train_ann[:train_until], x_train_embedding[:train_until], x_train_word2vec[:train_until]\n",
    "  y_train = y_train[:train_until]\n",
    "  train_returns = train_returns[:train_until]\n",
    "  train_dates = train_dates[:train_until]\n",
    "  x_test_lstm, x_test_ann, x_test_embedding, x_test_word2vec = x_test_lstm[:test_until], x_test_ann[:test_until], x_test_embedding[:test_until], x_test_word2vec[:test_until]\n",
    "  y_test = y_test[:test_until]\n",
    "  test_returns = test_returns[:test_until]\n",
    "  test_dates = test_dates[:test_until]\n",
    "\n",
    "  train_len = x_train_lstm.shape[0]\n",
    "  test_len = x_test_lstm.shape[0]\n",
    "\n",
    "  np.random.seed(1)\n",
    "  train_shuffle = np.random.permutation(range(train_len))\n",
    "  test_shuffle = np.random.permutation(range(test_len))\n",
    "  x_train_lstm, x_train_ann, x_train_embedding, x_train_word2vec, y_train, train_returns, train_dates = x_train_lstm[train_shuffle], x_train_ann[train_shuffle], x_train_embedding[train_shuffle], x_train_word2vec[train_shuffle], y_train[train_shuffle], train_returns[train_shuffle], train_dates[train_shuffle]\n",
    "  x_test_lstm, x_test_ann, x_test_embedding, x_test_word2vec, y_test, test_returns, test_dates = x_test_lstm[test_shuffle], x_test_ann[test_shuffle], x_test_embedding[test_shuffle], x_test_word2vec[test_shuffle], y_test[test_shuffle], test_returns[test_shuffle], test_dates[test_shuffle]\n",
    "\n",
    "  def create_model():\n",
    "    sequence_len = x_train_lstm.shape[1]\n",
    "\n",
    "    lstm_input = Input(shape=(sequence_len, len(lstm_features_list)), name='lstm_input')\n",
    "    lstm_out = LSTM(21)(lstm_input)\n",
    "\n",
    "    additional_input = Input(shape=(len(additional_features_list),), name='additional_input')\n",
    "  #   word2vec_input = Input(shape=(len(word2vec_features_list),), name='word2vec_input')\n",
    "\n",
    "  #   concat_layers = [lstm_out, additional_input, word2vec_input]\n",
    "    concat_layers = [lstm_out, additional_input]\n",
    "\n",
    "  #   input_layers = [lstm_input, additional_input, word2vec_input]\n",
    "    input_layers = [lstm_input, additional_input]\n",
    "\n",
    "    # embeddings\n",
    "    for i in range(x_train_embedding.shape[1]):\n",
    "      vocab_size = x_train_embedding[:, i].max() + 1\n",
    "      y = Input(shape=(1,), dtype='int32', name='embedding_input_%d' % i)\n",
    "      input_layers.append(y)\n",
    "      y = Embedding(input_dim=vocab_size, output_dim=21)(y)\n",
    "      concat_layers.append(Flatten()(y))\n",
    "\n",
    "    x = keras.layers.concatenate(concat_layers)\n",
    "\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "\n",
    "    main_output = Dense(1, activation='sigmoid', name='output')(x)\n",
    "\n",
    "    model = Model(inputs=input_layers, outputs=[main_output])\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "  \n",
    "  def get_sigma(scaled_scores, test_returns, test_dates):\n",
    "    '''\n",
    "    scaled_scores: numpy array\n",
    "    test_returns: numpy array\n",
    "    test_dates: numpy array\n",
    "    '''\n",
    "\n",
    "    grouped_dict = pd.Series(scaled_scores).index.groupby(test_dates)\n",
    "\n",
    "    day_scores = []\n",
    "    for ts in grouped_dict:\n",
    "      grouped_index = grouped_dict[ts]\n",
    "      day_score = np.dot(test_returns[grouped_index], scaled_scores[grouped_index])\n",
    "      day_scores.append(day_score)\n",
    "\n",
    "    return np.mean(day_scores) / np.std(day_scores, ddof=1)\n",
    "\n",
    "\n",
    "  prev_sigma = None\n",
    "  combined_model = create_model()\n",
    "\n",
    "  # train for at most 500 epochs\n",
    "  for i in range(500):\n",
    "    # x_train_list = [x_train_lstm, x_train_ann] + [x_train_word2vec] + [x_train_embedding[:, i] for i in range(x_train_embedding.shape[1])]\n",
    "    x_train_list = [x_train_lstm, x_train_ann] + [x_train_embedding[:, i] for i in range(x_train_embedding.shape[1])]\n",
    "    # x_test_list = [x_test_lstm, x_test_ann] + [x_test_word2vec] + [x_test_embedding[:, i] for i in range(x_test_embedding.shape[1])]\n",
    "    x_test_list = [x_test_lstm, x_test_ann] + [x_test_embedding[:, i] for i in range(x_test_embedding.shape[1])]\n",
    "\n",
    "    combined_model.fit(x_train_list,\n",
    "                   y_train,\n",
    "                   batch_size=lstm_params['batch_size'],\n",
    "                   epochs=1,\n",
    "                   verbose=1,\n",
    "                   validation_data=(x_test_list, y_test),\n",
    "                   shuffle=False)\n",
    "\n",
    "    scaled_scores = combined_model.predict(x_test_list, batch_size=lstm_params['batch_size']).reshape(-1) * 2 - 1\n",
    "    current_sigma = get_sigma(scaled_scores, test_returns, test_dates)\n",
    "    \n",
    "    print('current sigma: %f' % current_sigma)\n",
    "\n",
    "    # early stopping if sigma score does not improve for 1 epoch\n",
    "    if prev_sigma is not None and current_sigma < prev_sigma:\n",
    "      break\n",
    "    else:\n",
    "      prev_sigma = current_sigma\n",
    "  \n",
    "  pred_classes = (scaled_scores > 0).astype(int)\n",
    "  from sklearn.metrics import confusion_matrix\n",
    "  conf_mat = confusion_matrix(y_test, pred_classes)\n",
    "  \n",
    "  print('Year %d: sigma=%f' % (train_end_year, current_sigma))\n",
    "  print(conf_mat)\n",
    "  \n",
    "  if v_str not in results:\n",
    "    results[v_str] = {}\n",
    "  results[v_str][train_end_year] = {}\n",
    "  results[v_str][train_end_year]['y_test'] = y_test\n",
    "  results[v_str][train_end_year]['scaled_scores'] = scaled_scores\n",
    "  results[v_str][train_end_year]['sigma_score'] = current_sigma\n",
    "  results[v_str][train_end_year]['conf_mat'] = conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"sliding_final_output_from_results_dict_V1.p\"\n",
    "fileobj = open(filename, \"wb\")\n",
    "pickle.dump(results, fileobj)\n",
    "fileobj.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vjotmgVKslpP"
   },
   "source": [
    "### Model Building v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LohKJr69sg64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 361116 samples, validate on 165816 samples\n",
      "Epoch 1/1\n",
      "361116/361116 [==============================] - 86s 237us/step - loss: 0.6691 - acc: 0.5866 - val_loss: 0.7100 - val_acc: 0.5332\n",
      "current sigma: 0.265445\n",
      "Train on 361116 samples, validate on 165816 samples\n",
      "Epoch 1/1\n",
      "361116/361116 [==============================] - 87s 241us/step - loss: 0.6499 - acc: 0.6148 - val_loss: 0.7174 - val_acc: 0.5300\n",
      "current sigma: 0.271875\n",
      "Train on 361116 samples, validate on 165816 samples\n",
      "Epoch 1/1\n",
      "361116/361116 [==============================] - 88s 243us/step - loss: 0.6349 - acc: 0.6338 - val_loss: 0.7339 - val_acc: 0.5214\n",
      "current sigma: 0.238026\n",
      "Year 2011: sigma=0.238026\n",
      "[[45092 43024]\n",
      " [36329 41371]]\n",
      "Train on 369432 samples, validate on 168336 samples\n",
      "Epoch 1/1\n",
      "369432/369432 [==============================] - 87s 234us/step - loss: 0.6681 - acc: 0.5919 - val_loss: 0.6868 - val_acc: 0.5641\n",
      "current sigma: 0.827156\n",
      "Train on 369432 samples, validate on 168336 samples\n",
      "Epoch 1/1\n",
      "369432/369432 [==============================] - 88s 239us/step - loss: 0.6554 - acc: 0.6091 - val_loss: 0.6916 - val_acc: 0.5649\n",
      "current sigma: 0.835352\n",
      "Train on 369432 samples, validate on 168336 samples\n",
      "Epoch 1/1\n",
      "369432/369432 [==============================] - 89s 240us/step - loss: 0.6453 - acc: 0.6207 - val_loss: 0.7053 - val_acc: 0.5604\n",
      "current sigma: 0.764929\n",
      "Year 2012: sigma=0.764929\n",
      "[[43534 40894]\n",
      " [33101 50807]]\n",
      "Train on 369684 samples, validate on 160020 samples\n",
      "Epoch 1/1\n",
      "369684/369684 [==============================] - 88s 237us/step - loss: 0.6648 - acc: 0.5918 - val_loss: 0.6912 - val_acc: 0.5672\n",
      "current sigma: 0.680076\n",
      "Train on 369684 samples, validate on 160020 samples\n",
      "Epoch 1/1\n",
      "369684/369684 [==============================] - 94s 253us/step - loss: 0.6457 - acc: 0.6185 - val_loss: 0.7116 - val_acc: 0.5611\n",
      "current sigma: 0.625804\n",
      "Year 2013: sigma=0.625804\n",
      "[[49058 32356]\n",
      " [37870 40736]]\n",
      "Train on 350028 samples, validate on 165816 samples\n",
      "Epoch 1/1\n",
      "350028/350028 [==============================] - 87s 249us/step - loss: 0.6598 - acc: 0.5963 - val_loss: 0.7076 - val_acc: 0.5216\n",
      "current sigma: 0.271255\n",
      "Train on 350028 samples, validate on 165816 samples\n",
      "Epoch 1/1\n",
      "350028/350028 [==============================] - 88s 251us/step - loss: 0.6388 - acc: 0.6254 - val_loss: 0.7336 - val_acc: 0.5060\n",
      "current sigma: 0.026845\n",
      "Year 2014: sigma=0.026845\n",
      "[[42675 38016]\n",
      " [43893 41232]]\n",
      "Train on 378000 samples, validate on 180432 samples\n",
      "Epoch 1/1\n",
      "378000/378000 [==============================] - 94s 250us/step - loss: 0.6709 - acc: 0.5847 - val_loss: 0.7073 - val_acc: 0.5324\n",
      "current sigma: 0.475984\n",
      "Train on 378000 samples, validate on 180432 samples\n",
      "Epoch 1/1\n",
      "378000/378000 [==============================] - 97s 257us/step - loss: 0.6462 - acc: 0.6208 - val_loss: 0.7336 - val_acc: 0.5241\n",
      "current sigma: 0.385804\n",
      "Year 2015: sigma=0.385804\n",
      "[[61490 24451]\n",
      " [61418 33073]]\n",
      "Train on 2187864 samples, validate on 184968 samples\n",
      "Epoch 1/1\n",
      "2187864/2187864 [==============================] - 490s 224us/step - loss: 0.6726 - acc: 0.5796 - val_loss: 0.7001 - val_acc: 0.5430\n",
      "current sigma: 0.499245\n",
      "Train on 2187864 samples, validate on 184968 samples\n",
      "Epoch 1/1\n",
      "2187864/2187864 [==============================] - 486s 222us/step - loss: 0.6594 - acc: 0.6006 - val_loss: 0.7075 - val_acc: 0.5410\n",
      "current sigma: 0.450828\n",
      "Year 2016: sigma=0.450828\n",
      "[[39987 50949]\n",
      " [33948 60084]]\n",
      "Wall time: 41min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "v_str = 'v2'\n",
    "\n",
    "train_end_years = [2011, 2012, 2013, 2014, 2015, 2016]\n",
    "\n",
    "lstm_features_list = [\n",
    "    'returnsClosePrevRaw1',\n",
    "    'returnsOpenPrevRaw1',\n",
    "    'returnsClosePrevMktres1',\n",
    "    'returnsOpenPrevMktres1',\n",
    "    'returnsClosePrevRaw10',\n",
    "    'returnsOpenPrevRaw10',\n",
    "    'returnsClosePrevMktres10',\n",
    "    'returnsOpenPrevMktres10',\n",
    "    'open_close',\n",
    "    'oc_average',\n",
    "    'turnover',\n",
    "    'open_close_relative',\n",
    "    'turnover_relative',\n",
    "    'volume_relative',\n",
    "    'returnsOpenPrevMktres1_relative',\n",
    "    'returnsOpenPrevMktres10_relative',\n",
    "]\n",
    "\n",
    "\n",
    "additional_features_list = [\n",
    "    'urgency_min',\n",
    "    'urgency_count',\n",
    "    'takeSequence_min',\n",
    "    'takeSequence_max',\n",
    "    'bodySize_mean',\n",
    "    'wordCount_mean',\n",
    "    'sentenceCount_mean',\n",
    "    'companyCount_mean',\n",
    "    'marketCommentary_mean',\n",
    "    'relevance_mean',\n",
    "    'sentimentNegative_mean',\n",
    "    'sentimentNeutral_mean',\n",
    "    'sentimentPositive_mean',\n",
    "    'sentimentWordCount_mean',\n",
    "    'noveltyCount12H_mean',\n",
    "    'noveltyCount24H_mean',\n",
    "    'noveltyCount3D_mean',\n",
    "    'noveltyCount5D_mean',\n",
    "    'noveltyCount7D_mean',\n",
    "    'volumeCounts12H_mean',\n",
    "    'volumeCounts24H_mean',\n",
    "    'volumeCounts3D_mean',\n",
    "    'volumeCounts5D_mean',\n",
    "    'volumeCounts7D_mean',\n",
    "]\n",
    "\n",
    "label_encoded_features = [\n",
    "    'dayofweek',\n",
    "    'month',\n",
    "]\n",
    "\n",
    "continuous_features_list = lstm_features_list + additional_features_list\n",
    "features_list = lstm_features_list + additional_features_list + label_encoded_features\n",
    "\n",
    "for train_end_year in train_end_years:\n",
    "  # a trick to get testing stats\n",
    "  if train_end_year == 2016:\n",
    "    train_end_year = 2015\n",
    "    train_start, train_end = pd.to_datetime('2011-01-01'), pd.to_datetime('2016-06-30')\n",
    "    test_start, test_end = pd.to_datetime('%d-07-01' % (train_end_year + 1)), pd.to_datetime('%d-12-31' % (train_end_year + 1))\n",
    "    train_end_year = 2016\n",
    "  else:\n",
    "    train_start, train_end = pd.to_datetime('%d-01-01' % train_end_year), pd.to_datetime('%d-12-31' % train_end_year)\n",
    "    test_start, test_end = pd.to_datetime('%d-01-01' % (train_end_year + 1)), pd.to_datetime('%d-06-30' % (train_end_year + 1))\n",
    "  \n",
    "  \n",
    "  # divide into train and test periods\n",
    "\n",
    "  train_data = combined_data.set_index('time')[train_start: train_end]\n",
    "  test_data = combined_data.set_index('time')[test_start: test_end]\n",
    "\n",
    "  from sklearn.preprocessing import StandardScaler\n",
    "  scaler = StandardScaler()\n",
    "  train_data[continuous_features_list] = scaler.fit_transform(train_data[continuous_features_list])\n",
    "  test_data[continuous_features_list] = scaler.transform(test_data[continuous_features_list])\n",
    "\n",
    "  # reshape data\n",
    "  word2vec_features_list = []\n",
    "\n",
    "  def reshape_data(data, sequence_len=21, features_list=features_list):\n",
    "    data_list, ann_list, embedding_list, word2vec_list, target_list, returns_list, date_list = [], [], [], [], [], [], []\n",
    "\n",
    "    for assetCode, asset_data in data.groupby('assetCode'):\n",
    "      # binarize the target variable\n",
    "      returns = asset_data['returnsOpenNextMktres10']\n",
    "      target = (asset_data['returnsOpenNextMktres10'] > 0).astype(int)\n",
    "      dates = asset_data.index\n",
    "\n",
    "      # arrays\n",
    "      lstm_data = asset_data[lstm_features_list].values\n",
    "      ann_data = asset_data[additional_features_list].values\n",
    "      embedding_data = asset_data[label_encoded_features].values\n",
    "      word2vec_data = asset_data[word2vec_features_list].values\n",
    "\n",
    "      col_mean = np.nanmean(lstm_data, axis = 0) \n",
    "      inds = np.where(np.isnan(lstm_data)) \n",
    "      lstm_data[inds] = np.take(col_mean, inds[1])\n",
    "\n",
    "      reshaped_data = np.repeat(lstm_data[:, np.newaxis, :], sequence_len, axis=1)\n",
    "      for i in range(reshaped_data.shape[1]):\n",
    "        reshaped_data[:, i, :] = np.roll(reshaped_data[:, i, :], i, axis=0)\n",
    "\n",
    "      # discard the top rows and reverse the order of the shifted sequence to reflect the correct order\n",
    "      reshaped_data = reshaped_data[sequence_len-1:, ::-1, :]\n",
    "      ann_data = ann_data[sequence_len-1:, :]\n",
    "      embedding_data = embedding_data[sequence_len-1:, :]\n",
    "      word2vec_data = word2vec_data[sequence_len-1:, :]\n",
    "      target = target.values[sequence_len-1:]\n",
    "      returns = returns.values[sequence_len-1:]\n",
    "      dates = dates[sequence_len-1:]\n",
    "\n",
    "      data_list.append(reshaped_data)\n",
    "      ann_list.append(ann_data)\n",
    "      embedding_list.append(embedding_data)\n",
    "      word2vec_list.append(word2vec_data)\n",
    "      target_list.append(target)\n",
    "      returns_list.append(returns)\n",
    "      date_list.append(dates)\n",
    "\n",
    "    reshaped_data, ann_data, embedding_data, word2vec_data, target, returns, dates = np.concatenate(data_list), np.concatenate(ann_list), np.concatenate(embedding_list), np.concatenate(word2vec_list), np.concatenate(target_list), np.concatenate(returns_list), np.concatenate(date_list)\n",
    "\n",
    "    return (reshaped_data, ann_data, embedding_data, word2vec_data), target, returns, dates\n",
    "\n",
    "  x_train_tuple, y_train, train_returns, train_dates = reshape_data(train_data)\n",
    "  x_test_tuple, y_test, test_returns, test_dates = reshape_data(test_data)\n",
    "\n",
    "  x_train_lstm, x_train_ann, x_train_embedding, x_train_word2vec = x_train_tuple\n",
    "  x_test_lstm, x_test_ann, x_test_embedding, x_test_word2vec = x_test_tuple\n",
    "  \n",
    "  # create model\n",
    "  from keras.layers import Input, Embedding, LSTM, Dense\n",
    "  from keras.models import Model\n",
    "\n",
    "  lstm_params = {\n",
    "      'batch_size': 252,\n",
    "      'epochs': 5,\n",
    "  }\n",
    "\n",
    "  train_len = x_train_lstm.shape[0]\n",
    "  test_len = x_test_lstm.shape[0]\n",
    "\n",
    "  # tweak to match with batch_size\n",
    "  train_until = train_len - train_len % lstm_params['batch_size']\n",
    "  test_until = test_len - test_len % lstm_params['batch_size']\n",
    "\n",
    "  x_train_lstm, x_train_ann, x_train_embedding, x_train_word2vec = x_train_lstm[:train_until], x_train_ann[:train_until], x_train_embedding[:train_until], x_train_word2vec[:train_until]\n",
    "  y_train = y_train[:train_until]\n",
    "  train_returns = train_returns[:train_until]\n",
    "  train_dates = train_dates[:train_until]\n",
    "  x_test_lstm, x_test_ann, x_test_embedding, x_test_word2vec = x_test_lstm[:test_until], x_test_ann[:test_until], x_test_embedding[:test_until], x_test_word2vec[:test_until]\n",
    "  y_test = y_test[:test_until]\n",
    "  test_returns = test_returns[:test_until]\n",
    "  test_dates = test_dates[:test_until]\n",
    "\n",
    "  train_len = x_train_lstm.shape[0]\n",
    "  test_len = x_test_lstm.shape[0]\n",
    "\n",
    "  np.random.seed(1)\n",
    "  train_shuffle = np.random.permutation(range(train_len))\n",
    "  test_shuffle = np.random.permutation(range(test_len))\n",
    "  x_train_lstm, x_train_ann, x_train_embedding, x_train_word2vec, y_train, train_returns, train_dates = x_train_lstm[train_shuffle], x_train_ann[train_shuffle], x_train_embedding[train_shuffle], x_train_word2vec[train_shuffle], y_train[train_shuffle], train_returns[train_shuffle], train_dates[train_shuffle]\n",
    "  x_test_lstm, x_test_ann, x_test_embedding, x_test_word2vec, y_test, test_returns, test_dates = x_test_lstm[test_shuffle], x_test_ann[test_shuffle], x_test_embedding[test_shuffle], x_test_word2vec[test_shuffle], y_test[test_shuffle], test_returns[test_shuffle], test_dates[test_shuffle]\n",
    "\n",
    "  def create_model():\n",
    "    sequence_len = x_train_lstm.shape[1]\n",
    "\n",
    "    lstm_input = Input(shape=(sequence_len, len(lstm_features_list)), name='lstm_input')\n",
    "    lstm_out = LSTM(21)(lstm_input)\n",
    "\n",
    "    additional_input = Input(shape=(len(additional_features_list),), name='additional_input')\n",
    "  #   word2vec_input = Input(shape=(len(word2vec_features_list),), name='word2vec_input')\n",
    "\n",
    "  #   concat_layers = [lstm_out, additional_input, word2vec_input]\n",
    "    concat_layers = [lstm_out, additional_input]\n",
    "\n",
    "  #   input_layers = [lstm_input, additional_input, word2vec_input]\n",
    "    input_layers = [lstm_input, additional_input]\n",
    "\n",
    "    # embeddings\n",
    "    for i in range(x_train_embedding.shape[1]):\n",
    "      vocab_size = x_train_embedding[:, i].max() + 1\n",
    "      y = Input(shape=(1,), dtype='int32', name='embedding_input_%d' % i)\n",
    "      input_layers.append(y)\n",
    "      y = Embedding(input_dim=vocab_size, output_dim=21)(y)\n",
    "      concat_layers.append(Flatten()(y))\n",
    "\n",
    "    x = keras.layers.concatenate(concat_layers)\n",
    "\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "\n",
    "    main_output = Dense(1, activation='sigmoid', name='output')(x)\n",
    "\n",
    "    model = Model(inputs=input_layers, outputs=[main_output])\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "  \n",
    "  def get_sigma(scaled_scores, test_returns, test_dates):\n",
    "    '''\n",
    "    scaled_scores: numpy array\n",
    "    test_returns: numpy array\n",
    "    test_dates: numpy array\n",
    "    '''\n",
    "\n",
    "    grouped_dict = pd.Series(scaled_scores).index.groupby(test_dates)\n",
    "\n",
    "    day_scores = []\n",
    "    for ts in grouped_dict:\n",
    "      grouped_index = grouped_dict[ts]\n",
    "      day_score = np.dot(test_returns[grouped_index], scaled_scores[grouped_index])\n",
    "      day_scores.append(day_score)\n",
    "\n",
    "    return np.mean(day_scores) / np.std(day_scores, ddof=1)\n",
    "\n",
    "\n",
    "  prev_sigma = None\n",
    "  combined_model = create_model()\n",
    "\n",
    "  # train for at most 500 epochs\n",
    "  for i in range(500):\n",
    "    # x_train_list = [x_train_lstm, x_train_ann] + [x_train_word2vec] + [x_train_embedding[:, i] for i in range(x_train_embedding.shape[1])]\n",
    "    x_train_list = [x_train_lstm, x_train_ann] + [x_train_embedding[:, i] for i in range(x_train_embedding.shape[1])]\n",
    "    # x_test_list = [x_test_lstm, x_test_ann] + [x_test_word2vec] + [x_test_embedding[:, i] for i in range(x_test_embedding.shape[1])]\n",
    "    x_test_list = [x_test_lstm, x_test_ann] + [x_test_embedding[:, i] for i in range(x_test_embedding.shape[1])]\n",
    "\n",
    "    combined_model.fit(x_train_list,\n",
    "                   y_train,\n",
    "                   batch_size=lstm_params['batch_size'],\n",
    "                   epochs=1,\n",
    "                   verbose=1,\n",
    "                   validation_data=(x_test_list, y_test),\n",
    "                   shuffle=False)\n",
    "\n",
    "    scaled_scores = combined_model.predict(x_test_list, batch_size=lstm_params['batch_size']).reshape(-1) * 2 - 1\n",
    "    current_sigma = get_sigma(scaled_scores, test_returns, test_dates)\n",
    "    \n",
    "    print('current sigma: %f' % current_sigma)\n",
    "\n",
    "    # early stopping if sigma score does not improve for 1 epoch\n",
    "    if prev_sigma is not None and current_sigma < prev_sigma:\n",
    "      break\n",
    "    else:\n",
    "      prev_sigma = current_sigma\n",
    "  \n",
    "  pred_classes = (scaled_scores > 0).astype(int)\n",
    "  from sklearn.metrics import confusion_matrix\n",
    "  conf_mat = confusion_matrix(y_test, pred_classes)\n",
    "  \n",
    "  print('Year %d: sigma=%f' % (train_end_year, current_sigma))\n",
    "  print(conf_mat)\n",
    "  \n",
    "  if v_str not in results:\n",
    "    results[v_str] = {}\n",
    "  results[v_str][train_end_year] = {}\n",
    "  results[v_str][train_end_year]['y_test'] = y_test\n",
    "  results[v_str][train_end_year]['scaled_scores'] = scaled_scores\n",
    "  results[v_str][train_end_year]['sigma_score'] = current_sigma\n",
    "  results[v_str][train_end_year]['conf_mat'] = conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"sliding_final_output_from_results_dict_V1_and_V2.p\"\n",
    "fileobj = open(filename, \"wb\")\n",
    "pickle.dump(results, fileobj)\n",
    "fileobj.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XCaqrE8mu9LT"
   },
   "source": [
    "### Model Building v4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cs0cYrfVu6kZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 361116 samples, validate on 165816 samples\n",
      "Epoch 1/1\n",
      "361116/361116 [==============================] - 93s 257us/step - loss: 0.6697 - acc: 0.5858 - val_loss: 0.7101 - val_acc: 0.5344\n",
      "current sigma: 0.264845\n",
      "Train on 361116 samples, validate on 165816 samples\n",
      "Epoch 1/1\n",
      "361116/361116 [==============================] - 85s 237us/step - loss: 0.6505 - acc: 0.6150 - val_loss: 0.7170 - val_acc: 0.5273\n",
      "current sigma: 0.269737\n",
      "Train on 361116 samples, validate on 165816 samples\n",
      "Epoch 1/1\n",
      "361116/361116 [==============================] - 84s 234us/step - loss: 0.6353 - acc: 0.6334 - val_loss: 0.7310 - val_acc: 0.5212\n",
      "current sigma: 0.251577\n",
      "Year 2011: sigma=0.251577\n",
      "[[43324 44792]\n",
      " [34606 43094]]\n",
      "Train on 369432 samples, validate on 168336 samples\n",
      "Epoch 1/1\n",
      "369432/369432 [==============================] - 84s 228us/step - loss: 0.6691 - acc: 0.5909 - val_loss: 0.6870 - val_acc: 0.5642\n",
      "current sigma: 0.811922\n",
      "Train on 369432 samples, validate on 168336 samples\n",
      "Epoch 1/1\n",
      "369432/369432 [==============================] - 87s 235us/step - loss: 0.6565 - acc: 0.6082 - val_loss: 0.6923 - val_acc: 0.5654\n",
      "current sigma: 0.885421\n",
      "Train on 369432 samples, validate on 168336 samples\n",
      "Epoch 1/1\n",
      "369432/369432 [==============================] - 86s 233us/step - loss: 0.6464 - acc: 0.6195 - val_loss: 0.7039 - val_acc: 0.5635\n",
      "current sigma: 0.861726\n",
      "Year 2012: sigma=0.861726\n",
      "[[40995 43433]\n",
      " [30038 53870]]\n",
      "Train on 369684 samples, validate on 160020 samples\n",
      "Epoch 1/1\n",
      "369684/369684 [==============================] - 84s 228us/step - loss: 0.6662 - acc: 0.5901 - val_loss: 0.6879 - val_acc: 0.5678\n",
      "current sigma: 0.699201\n",
      "Train on 369684 samples, validate on 160020 samples\n",
      "Epoch 1/1\n",
      "369684/369684 [==============================] - 86s 233us/step - loss: 0.6473 - acc: 0.6165 - val_loss: 0.7060 - val_acc: 0.5616\n",
      "current sigma: 0.642337\n",
      "Year 2013: sigma=0.642337\n",
      "[[50382 31032]\n",
      " [39115 39491]]\n",
      "Train on 350028 samples, validate on 165816 samples\n",
      "Epoch 1/1\n",
      "350028/350028 [==============================] - 81s 233us/step - loss: 0.6596 - acc: 0.5972 - val_loss: 0.7131 - val_acc: 0.5217\n",
      "current sigma: 0.219956\n",
      "Train on 350028 samples, validate on 165816 samples\n",
      "Epoch 1/1\n",
      "350028/350028 [==============================] - 82s 235us/step - loss: 0.6381 - acc: 0.6260 - val_loss: 0.7405 - val_acc: 0.5078\n",
      "current sigma: -0.018428\n",
      "Year 2014: sigma=-0.018428\n",
      "[[43414 37277]\n",
      " [44344 40781]]\n",
      "Train on 378000 samples, validate on 180432 samples\n",
      "Epoch 1/1\n",
      "378000/378000 [==============================] - 94s 249us/step - loss: 0.6711 - acc: 0.5840 - val_loss: 0.7064 - val_acc: 0.5312\n",
      "current sigma: 0.449023\n",
      "Train on 378000 samples, validate on 180432 samples\n",
      "Epoch 1/1\n",
      "378000/378000 [==============================] - 91s 239us/step - loss: 0.6476 - acc: 0.6188 - val_loss: 0.7317 - val_acc: 0.5187\n",
      "current sigma: 0.377148\n",
      "Year 2015: sigma=0.377148\n",
      "[[58140 27801]\n",
      " [59036 35455]]\n",
      "Train on 2187864 samples, validate on 184968 samples\n",
      "Epoch 1/1\n",
      "2187864/2187864 [==============================] - 461s 211us/step - loss: 0.6732 - acc: 0.5785 - val_loss: 0.7025 - val_acc: 0.5426\n",
      "current sigma: 0.488049\n",
      "Train on 2187864 samples, validate on 184968 samples\n",
      "Epoch 1/1\n",
      "2187864/2187864 [==============================] - 467s 214us/step - loss: 0.6603 - acc: 0.5994 - val_loss: 0.7119 - val_acc: 0.5431\n",
      "current sigma: 0.503293\n",
      "Train on 2187864 samples, validate on 184968 samples\n",
      "Epoch 1/1\n",
      "2187864/2187864 [==============================] - 469s 214us/step - loss: 0.6516 - acc: 0.6114 - val_loss: 0.7153 - val_acc: 0.5425\n",
      "current sigma: 0.485014\n",
      "Year 2016: sigma=0.485014\n",
      "[[45284 45652]\n",
      " [38972 55060]]\n",
      "Wall time: 47min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# remark for Wei Han: add industry-neutralised sentiments\n",
    "\n",
    "v_str = 'v4'\n",
    "\n",
    "train_end_years = [2011, 2012, 2013, 2014, 2015, 2016]\n",
    "\n",
    "lstm_features_list = [\n",
    "    'returnsClosePrevRaw1',\n",
    "    'returnsOpenPrevRaw1',\n",
    "    'returnsClosePrevMktres1',\n",
    "    'returnsOpenPrevMktres1',\n",
    "    'returnsClosePrevRaw10',\n",
    "    'returnsOpenPrevRaw10',\n",
    "    'returnsClosePrevMktres10',\n",
    "    'returnsOpenPrevMktres10',\n",
    "    'open_close',\n",
    "    'oc_average',\n",
    "    'turnover',\n",
    "    'open_close_relative',\n",
    "    'turnover_relative',\n",
    "    'volume_relative',\n",
    "    'returnsOpenPrevMktres1_relative',\n",
    "    'returnsOpenPrevMktres10_relative',\n",
    "]\n",
    "\n",
    "\n",
    "additional_features_list = [\n",
    "    'urgency_min',\n",
    "    'urgency_count',\n",
    "    'takeSequence_min',\n",
    "    'takeSequence_max',\n",
    "    'bodySize_mean',\n",
    "    'wordCount_mean',\n",
    "    'sentenceCount_mean',\n",
    "    'companyCount_mean',\n",
    "    'marketCommentary_mean',\n",
    "    'relevance_mean',\n",
    "    'sentimentNegative_mean',\n",
    "    'sentimentNeutral_mean',\n",
    "    'sentimentPositive_mean',\n",
    "    'sentimentWordCount_mean',\n",
    "    'sentimentNegative_mean_stan',\n",
    "    'sentimentNeutral_mean_stan',\n",
    "    'sentimentPositive_mean_stan',\n",
    "    'sentimentWordCount_mean_stan',\n",
    "    'noveltyCount12H_mean',\n",
    "    'noveltyCount24H_mean',\n",
    "    'noveltyCount3D_mean',\n",
    "    'noveltyCount5D_mean',\n",
    "    'noveltyCount7D_mean',\n",
    "    'volumeCounts12H_mean',\n",
    "    'volumeCounts24H_mean',\n",
    "    'volumeCounts3D_mean',\n",
    "    'volumeCounts5D_mean',\n",
    "    'volumeCounts7D_mean',\n",
    "]\n",
    "\n",
    "label_encoded_features = [\n",
    "    'dayofweek',\n",
    "    'month',\n",
    "]\n",
    "\n",
    "continuous_features_list = lstm_features_list + additional_features_list\n",
    "features_list = lstm_features_list + additional_features_list + label_encoded_features\n",
    "\n",
    "for train_end_year in train_end_years:\n",
    "  # a trick to get testing stats\n",
    "  if train_end_year == 2016:\n",
    "    train_end_year = 2015\n",
    "    train_start, train_end = pd.to_datetime('2011-01-01'), pd.to_datetime('2016-06-30')\n",
    "    test_start, test_end = pd.to_datetime('%d-07-01' % (train_end_year + 1)), pd.to_datetime('%d-12-31' % (train_end_year + 1))\n",
    "    train_end_year = 2016\n",
    "  else:\n",
    "    train_start, train_end = pd.to_datetime('%d-01-01' % train_end_year), pd.to_datetime('%d-12-31' % train_end_year)\n",
    "    test_start, test_end = pd.to_datetime('%d-01-01' % (train_end_year + 1)), pd.to_datetime('%d-06-30' % (train_end_year + 1))\n",
    "  \n",
    "  \n",
    "  # divide into train and test periods\n",
    "\n",
    "  train_data = combined_data.set_index('time')[train_start: train_end]\n",
    "  test_data = combined_data.set_index('time')[test_start: test_end]\n",
    "\n",
    "  from sklearn.preprocessing import StandardScaler\n",
    "  scaler = StandardScaler()\n",
    "  train_data[continuous_features_list] = scaler.fit_transform(train_data[continuous_features_list])\n",
    "  test_data[continuous_features_list] = scaler.transform(test_data[continuous_features_list])\n",
    "\n",
    "  # reshape data\n",
    "  word2vec_features_list = []\n",
    "\n",
    "  def reshape_data(data, sequence_len=21, features_list=features_list):\n",
    "    data_list, ann_list, embedding_list, word2vec_list, target_list, returns_list, date_list = [], [], [], [], [], [], []\n",
    "\n",
    "    for assetCode, asset_data in data.groupby('assetCode'):\n",
    "      # binarize the target variable\n",
    "      returns = asset_data['returnsOpenNextMktres10']\n",
    "      target = (asset_data['returnsOpenNextMktres10'] > 0).astype(int)\n",
    "      dates = asset_data.index\n",
    "\n",
    "      # arrays\n",
    "      lstm_data = asset_data[lstm_features_list].values\n",
    "      ann_data = asset_data[additional_features_list].values\n",
    "      embedding_data = asset_data[label_encoded_features].values\n",
    "      word2vec_data = asset_data[word2vec_features_list].values\n",
    "\n",
    "      col_mean = np.nanmean(lstm_data, axis = 0) \n",
    "      inds = np.where(np.isnan(lstm_data)) \n",
    "      lstm_data[inds] = np.take(col_mean, inds[1])\n",
    "\n",
    "      reshaped_data = np.repeat(lstm_data[:, np.newaxis, :], sequence_len, axis=1)\n",
    "      for i in range(reshaped_data.shape[1]):\n",
    "        reshaped_data[:, i, :] = np.roll(reshaped_data[:, i, :], i, axis=0)\n",
    "\n",
    "      # discard the top rows and reverse the order of the shifted sequence to reflect the correct order\n",
    "      reshaped_data = reshaped_data[sequence_len-1:, ::-1, :]\n",
    "      ann_data = ann_data[sequence_len-1:, :]\n",
    "      embedding_data = embedding_data[sequence_len-1:, :]\n",
    "      word2vec_data = word2vec_data[sequence_len-1:, :]\n",
    "      target = target.values[sequence_len-1:]\n",
    "      returns = returns.values[sequence_len-1:]\n",
    "      dates = dates[sequence_len-1:]\n",
    "\n",
    "      data_list.append(reshaped_data)\n",
    "      ann_list.append(ann_data)\n",
    "      embedding_list.append(embedding_data)\n",
    "      word2vec_list.append(word2vec_data)\n",
    "      target_list.append(target)\n",
    "      returns_list.append(returns)\n",
    "      date_list.append(dates)\n",
    "\n",
    "    reshaped_data, ann_data, embedding_data, word2vec_data, target, returns, dates = np.concatenate(data_list), np.concatenate(ann_list), np.concatenate(embedding_list), np.concatenate(word2vec_list), np.concatenate(target_list), np.concatenate(returns_list), np.concatenate(date_list)\n",
    "\n",
    "    return (reshaped_data, ann_data, embedding_data, word2vec_data), target, returns, dates\n",
    "\n",
    "  x_train_tuple, y_train, train_returns, train_dates = reshape_data(train_data)\n",
    "  x_test_tuple, y_test, test_returns, test_dates = reshape_data(test_data)\n",
    "\n",
    "  x_train_lstm, x_train_ann, x_train_embedding, x_train_word2vec = x_train_tuple\n",
    "  x_test_lstm, x_test_ann, x_test_embedding, x_test_word2vec = x_test_tuple\n",
    "  \n",
    "  # create model\n",
    "  from keras.layers import Input, Embedding, LSTM, Dense\n",
    "  from keras.models import Model\n",
    "\n",
    "  lstm_params = {\n",
    "      'batch_size': 252,\n",
    "      'epochs': 5,\n",
    "  }\n",
    "\n",
    "  train_len = x_train_lstm.shape[0]\n",
    "  test_len = x_test_lstm.shape[0]\n",
    "\n",
    "  # tweak to match with batch_size\n",
    "  train_until = train_len - train_len % lstm_params['batch_size']\n",
    "  test_until = test_len - test_len % lstm_params['batch_size']\n",
    "\n",
    "  x_train_lstm, x_train_ann, x_train_embedding, x_train_word2vec = x_train_lstm[:train_until], x_train_ann[:train_until], x_train_embedding[:train_until], x_train_word2vec[:train_until]\n",
    "  y_train = y_train[:train_until]\n",
    "  train_returns = train_returns[:train_until]\n",
    "  train_dates = train_dates[:train_until]\n",
    "  x_test_lstm, x_test_ann, x_test_embedding, x_test_word2vec = x_test_lstm[:test_until], x_test_ann[:test_until], x_test_embedding[:test_until], x_test_word2vec[:test_until]\n",
    "  y_test = y_test[:test_until]\n",
    "  test_returns = test_returns[:test_until]\n",
    "  test_dates = test_dates[:test_until]\n",
    "\n",
    "  train_len = x_train_lstm.shape[0]\n",
    "  test_len = x_test_lstm.shape[0]\n",
    "\n",
    "  np.random.seed(1)\n",
    "  train_shuffle = np.random.permutation(range(train_len))\n",
    "  test_shuffle = np.random.permutation(range(test_len))\n",
    "  x_train_lstm, x_train_ann, x_train_embedding, x_train_word2vec, y_train, train_returns, train_dates = x_train_lstm[train_shuffle], x_train_ann[train_shuffle], x_train_embedding[train_shuffle], x_train_word2vec[train_shuffle], y_train[train_shuffle], train_returns[train_shuffle], train_dates[train_shuffle]\n",
    "  x_test_lstm, x_test_ann, x_test_embedding, x_test_word2vec, y_test, test_returns, test_dates = x_test_lstm[test_shuffle], x_test_ann[test_shuffle], x_test_embedding[test_shuffle], x_test_word2vec[test_shuffle], y_test[test_shuffle], test_returns[test_shuffle], test_dates[test_shuffle]\n",
    "\n",
    "  def create_model():\n",
    "    sequence_len = x_train_lstm.shape[1]\n",
    "\n",
    "    lstm_input = Input(shape=(sequence_len, len(lstm_features_list)), name='lstm_input')\n",
    "    lstm_out = LSTM(21)(lstm_input)\n",
    "\n",
    "    additional_input = Input(shape=(len(additional_features_list),), name='additional_input')\n",
    "  #   word2vec_input = Input(shape=(len(word2vec_features_list),), name='word2vec_input')\n",
    "\n",
    "  #   concat_layers = [lstm_out, additional_input, word2vec_input]\n",
    "    concat_layers = [lstm_out, additional_input]\n",
    "\n",
    "  #   input_layers = [lstm_input, additional_input, word2vec_input]\n",
    "    input_layers = [lstm_input, additional_input]\n",
    "\n",
    "    # embeddings\n",
    "    for i in range(x_train_embedding.shape[1]):\n",
    "      vocab_size = x_train_embedding[:, i].max() + 1\n",
    "      y = Input(shape=(1,), dtype='int32', name='embedding_input_%d' % i)\n",
    "      input_layers.append(y)\n",
    "      y = Embedding(input_dim=vocab_size, output_dim=21)(y)\n",
    "      concat_layers.append(Flatten()(y))\n",
    "\n",
    "    x = keras.layers.concatenate(concat_layers)\n",
    "\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "\n",
    "    main_output = Dense(1, activation='sigmoid', name='output')(x)\n",
    "\n",
    "    model = Model(inputs=input_layers, outputs=[main_output])\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "  \n",
    "  def get_sigma(scaled_scores, test_returns, test_dates):\n",
    "    '''\n",
    "    scaled_scores: numpy array\n",
    "    test_returns: numpy array\n",
    "    test_dates: numpy array\n",
    "    '''\n",
    "\n",
    "    grouped_dict = pd.Series(scaled_scores).index.groupby(test_dates)\n",
    "\n",
    "    day_scores = []\n",
    "    for ts in grouped_dict:\n",
    "      grouped_index = grouped_dict[ts]\n",
    "      day_score = np.dot(test_returns[grouped_index], scaled_scores[grouped_index])\n",
    "      day_scores.append(day_score)\n",
    "\n",
    "    return np.mean(day_scores) / np.std(day_scores, ddof=1)\n",
    "\n",
    "\n",
    "  prev_sigma = None\n",
    "  combined_model = create_model()\n",
    "\n",
    "  # train for at most 500 epochs\n",
    "  for i in range(500):\n",
    "    # x_train_list = [x_train_lstm, x_train_ann] + [x_train_word2vec] + [x_train_embedding[:, i] for i in range(x_train_embedding.shape[1])]\n",
    "    x_train_list = [x_train_lstm, x_train_ann] + [x_train_embedding[:, i] for i in range(x_train_embedding.shape[1])]\n",
    "    # x_test_list = [x_test_lstm, x_test_ann] + [x_test_word2vec] + [x_test_embedding[:, i] for i in range(x_test_embedding.shape[1])]\n",
    "    x_test_list = [x_test_lstm, x_test_ann] + [x_test_embedding[:, i] for i in range(x_test_embedding.shape[1])]\n",
    "\n",
    "    combined_model.fit(x_train_list,\n",
    "                   y_train,\n",
    "                   batch_size=lstm_params['batch_size'],\n",
    "                   epochs=1,\n",
    "                   verbose=1,\n",
    "                   validation_data=(x_test_list, y_test),\n",
    "                   shuffle=False)\n",
    "\n",
    "    scaled_scores = combined_model.predict(x_test_list, batch_size=lstm_params['batch_size']).reshape(-1) * 2 - 1\n",
    "    current_sigma = get_sigma(scaled_scores, test_returns, test_dates)\n",
    "    \n",
    "    print('current sigma: %f' % current_sigma)\n",
    "\n",
    "    # early stopping if sigma score does not improve for 1 epoch\n",
    "    if prev_sigma is not None and current_sigma < prev_sigma:\n",
    "      break\n",
    "    else:\n",
    "      prev_sigma = current_sigma\n",
    "  \n",
    "  pred_classes = (scaled_scores > 0).astype(int)\n",
    "  from sklearn.metrics import confusion_matrix\n",
    "  conf_mat = confusion_matrix(y_test, pred_classes)\n",
    "  \n",
    "  print('Year %d: sigma=%f' % (train_end_year, current_sigma))\n",
    "  print(conf_mat)\n",
    "  \n",
    "  if v_str not in results:\n",
    "    results[v_str] = {}\n",
    "  results[v_str][train_end_year] = {}\n",
    "  results[v_str][train_end_year]['y_test'] = y_test\n",
    "  results[v_str][train_end_year]['scaled_scores'] = scaled_scores\n",
    "  results[v_str][train_end_year]['sigma_score'] = current_sigma\n",
    "  results[v_str][train_end_year]['conf_mat'] = conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"sliding_final_output_from_results_dict_V4.p\"\n",
    "fileobj = open(filename, \"wb\")\n",
    "pickle.dump(results, fileobj)\n",
    "fileobj.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oCd1VaAevU9A"
   },
   "source": [
    "### Model Building v5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "00wD52ycvJTk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 361116 samples, validate on 165816 samples\n",
      "Epoch 1/1\n",
      "361116/361116 [==============================] - 94s 259us/step - loss: 0.6696 - acc: 0.5857 - val_loss: 0.7175 - val_acc: 0.5237\n",
      "current sigma: 0.173767\n",
      "Train on 361116 samples, validate on 165816 samples\n",
      "Epoch 1/1\n",
      "361116/361116 [==============================] - 97s 269us/step - loss: 0.6477 - acc: 0.6174 - val_loss: 0.7339 - val_acc: 0.5217\n",
      "current sigma: 0.216586\n",
      "Train on 361116 samples, validate on 165816 samples\n",
      "Epoch 1/1\n",
      "361116/361116 [==============================] - 96s 266us/step - loss: 0.6319 - acc: 0.6359 - val_loss: 0.7479 - val_acc: 0.5187\n",
      "current sigma: 0.208302\n",
      "Year 2011: sigma=0.208302\n",
      "[[38651 49465]\n",
      " [30346 47354]]\n",
      "Train on 369432 samples, validate on 168336 samples\n",
      "Epoch 1/1\n",
      "369432/369432 [==============================] - 90s 243us/step - loss: 0.6690 - acc: 0.5909 - val_loss: 0.6883 - val_acc: 0.5626\n",
      "current sigma: 0.792931\n",
      "Train on 369432 samples, validate on 168336 samples\n",
      "Epoch 1/1\n",
      "369432/369432 [==============================] - 91s 247us/step - loss: 0.6529 - acc: 0.6116 - val_loss: 0.6993 - val_acc: 0.5609\n",
      "current sigma: 0.834593\n",
      "Train on 369432 samples, validate on 168336 samples\n",
      "Epoch 1/1\n",
      "369432/369432 [==============================] - 91s 247us/step - loss: 0.6397 - acc: 0.6260 - val_loss: 0.7119 - val_acc: 0.5589\n",
      "current sigma: 0.784137\n",
      "Year 2012: sigma=0.784137\n",
      "[[41138 43290]\n",
      " [30963 52945]]\n",
      "Train on 369684 samples, validate on 160020 samples\n",
      "Epoch 1/1\n",
      "369684/369684 [==============================] - 88s 237us/step - loss: 0.6652 - acc: 0.5910 - val_loss: 0.6888 - val_acc: 0.5650\n",
      "current sigma: 0.665126\n",
      "Train on 369684 samples, validate on 160020 samples\n",
      "Epoch 1/1\n",
      "369684/369684 [==============================] - 90s 244us/step - loss: 0.6455 - acc: 0.6185 - val_loss: 0.6987 - val_acc: 0.5636\n",
      "current sigma: 0.662090\n",
      "Year 2013: sigma=0.662090\n",
      "[[52657 28757]\n",
      " [41081 37525]]\n",
      "Train on 350028 samples, validate on 165816 samples\n",
      "Epoch 1/1\n",
      "350028/350028 [==============================] - 86s 247us/step - loss: 0.6610 - acc: 0.5954 - val_loss: 0.7078 - val_acc: 0.5280\n",
      "current sigma: 0.330395\n",
      "Train on 350028 samples, validate on 165816 samples\n",
      "Epoch 1/1\n",
      "350028/350028 [==============================] - 87s 248us/step - loss: 0.6391 - acc: 0.6256 - val_loss: 0.7251 - val_acc: 0.5209\n",
      "current sigma: 0.211456\n",
      "Year 2014: sigma=0.211456\n",
      "[[44678 36013]\n",
      " [43436 41689]]\n",
      "Train on 378000 samples, validate on 180432 samples\n",
      "Epoch 1/1\n",
      "378000/378000 [==============================] - 91s 241us/step - loss: 0.6690 - acc: 0.5867 - val_loss: 0.7129 - val_acc: 0.5256\n",
      "current sigma: 0.341132\n",
      "Train on 378000 samples, validate on 180432 samples\n",
      "Epoch 1/1\n",
      "378000/378000 [==============================] - 93s 246us/step - loss: 0.6462 - acc: 0.6190 - val_loss: 0.7321 - val_acc: 0.5135\n",
      "current sigma: 0.314295\n",
      "Year 2015: sigma=0.314295\n",
      "[[56291 29650]\n",
      " [58128 36363]]\n",
      "Train on 2187864 samples, validate on 184968 samples\n",
      "Epoch 1/1\n",
      "2187864/2187864 [==============================] - 558s 255us/step - loss: 0.6724 - acc: 0.5803 - val_loss: 0.7002 - val_acc: 0.5432\n",
      "current sigma: 0.542966\n",
      "Train on 2187864 samples, validate on 184968 samples\n",
      "Epoch 1/1\n",
      "2187864/2187864 [==============================] - 484s 221us/step - loss: 0.6588 - acc: 0.6013 - val_loss: 0.7086 - val_acc: 0.5396\n",
      "current sigma: 0.533894\n",
      "Year 2016: sigma=0.533894\n",
      "[[45654 45282]\n",
      " [39881 54151]]\n",
      "Wall time: 51min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# remark for Wei Han: add other industry-neutralised features\n",
    "\n",
    "v_str = 'v5'\n",
    "\n",
    "train_end_years = [2011, 2012, 2013, 2014, 2015, 2016]\n",
    "\n",
    "lstm_features_list = [\n",
    "    'returnsClosePrevRaw1',\n",
    "    'returnsOpenPrevRaw1',\n",
    "    'returnsClosePrevMktres1',\n",
    "    'returnsOpenPrevMktres1',\n",
    "    'returnsClosePrevRaw10',\n",
    "    'returnsOpenPrevRaw10',\n",
    "    'returnsClosePrevMktres10',\n",
    "    'returnsOpenPrevMktres10',\n",
    "    'open_close',\n",
    "    'oc_average',\n",
    "    'turnover',\n",
    "    'open_close_relative',\n",
    "    'turnover_relative',\n",
    "    'volume_relative',\n",
    "    'returnsOpenPrevMktres1_relative',\n",
    "    'returnsOpenPrevMktres10_relative',\n",
    "    'returnsClosePrevRaw1_stan',\n",
    "    'returnsOpenPrevRaw1_stan',\n",
    "    'returnsClosePrevMktres1_stan',\n",
    "    'returnsOpenPrevMktres1_stan',\n",
    "    'returnsClosePrevRaw10_stan',\n",
    "    'returnsOpenPrevRaw10_stan',\n",
    "    'returnsClosePrevMktres10_stan',\n",
    "    'returnsOpenPrevMktres10_stan',\n",
    "    'open_close_stan',\n",
    "    'oc_average_stan',\n",
    "    'turnover_stan',\n",
    "    'open_close_relative_stan',\n",
    "    'turnover_relative_stan',\n",
    "    'volume_relative_stan',\n",
    "    'returnsOpenPrevMktres1_relative_stan',\n",
    "    'returnsOpenPrevMktres10_relative_stan',\n",
    "]\n",
    "\n",
    "\n",
    "additional_features_list = [\n",
    "    'urgency_min',\n",
    "    'urgency_count',\n",
    "    'takeSequence_min',\n",
    "    'takeSequence_max',\n",
    "    'bodySize_mean',\n",
    "    'wordCount_mean',\n",
    "    'sentenceCount_mean',\n",
    "    'companyCount_mean',\n",
    "    'marketCommentary_mean',\n",
    "    'relevance_mean',\n",
    "    'sentimentNegative_mean',\n",
    "    'sentimentNeutral_mean',\n",
    "    'sentimentPositive_mean',\n",
    "    'sentimentWordCount_mean',\n",
    "    'sentimentNegative_mean_stan',\n",
    "    'sentimentNeutral_mean_stan',\n",
    "    'sentimentPositive_mean_stan',\n",
    "    'sentimentWordCount_mean_stan',\n",
    "    'noveltyCount12H_mean',\n",
    "    'noveltyCount24H_mean',\n",
    "    'noveltyCount3D_mean',\n",
    "    'noveltyCount5D_mean',\n",
    "    'noveltyCount7D_mean',\n",
    "    'volumeCounts12H_mean',\n",
    "    'volumeCounts24H_mean',\n",
    "    'volumeCounts3D_mean',\n",
    "    'volumeCounts5D_mean',\n",
    "    'volumeCounts7D_mean',\n",
    "]\n",
    "\n",
    "label_encoded_features = [\n",
    "    'dayofweek',\n",
    "    'month',\n",
    "]\n",
    "\n",
    "continuous_features_list = lstm_features_list + additional_features_list\n",
    "features_list = lstm_features_list + additional_features_list + label_encoded_features\n",
    "\n",
    "for train_end_year in train_end_years:\n",
    "  # a trick to get testing stats\n",
    "  if train_end_year == 2016:\n",
    "    train_end_year = 2015\n",
    "    train_start, train_end = pd.to_datetime('2011-01-01'), pd.to_datetime('2016-06-30')\n",
    "    test_start, test_end = pd.to_datetime('%d-07-01' % (train_end_year + 1)), pd.to_datetime('%d-12-31' % (train_end_year + 1))\n",
    "    train_end_year = 2016\n",
    "  else:\n",
    "    train_start, train_end = pd.to_datetime('%d-01-01' % train_end_year), pd.to_datetime('%d-12-31' % train_end_year)\n",
    "    test_start, test_end = pd.to_datetime('%d-01-01' % (train_end_year + 1)), pd.to_datetime('%d-06-30' % (train_end_year + 1))\n",
    "  \n",
    "  \n",
    "  # divide into train and test periods\n",
    "\n",
    "  train_data = combined_data.set_index('time')[train_start: train_end]\n",
    "  test_data = combined_data.set_index('time')[test_start: test_end]\n",
    "\n",
    "  from sklearn.preprocessing import StandardScaler\n",
    "  scaler = StandardScaler()\n",
    "  train_data[continuous_features_list] = scaler.fit_transform(train_data[continuous_features_list])\n",
    "  test_data[continuous_features_list] = scaler.transform(test_data[continuous_features_list])\n",
    "\n",
    "  # reshape data\n",
    "  word2vec_features_list = []\n",
    "\n",
    "  def reshape_data(data, sequence_len=21, features_list=features_list):\n",
    "    data_list, ann_list, embedding_list, word2vec_list, target_list, returns_list, date_list = [], [], [], [], [], [], []\n",
    "\n",
    "    for assetCode, asset_data in data.groupby('assetCode'):\n",
    "      # binarize the target variable\n",
    "      returns = asset_data['returnsOpenNextMktres10']\n",
    "      target = (asset_data['returnsOpenNextMktres10'] > 0).astype(int)\n",
    "      dates = asset_data.index\n",
    "\n",
    "      # arrays\n",
    "      lstm_data = asset_data[lstm_features_list].values\n",
    "      ann_data = asset_data[additional_features_list].values\n",
    "      embedding_data = asset_data[label_encoded_features].values\n",
    "      word2vec_data = asset_data[word2vec_features_list].values\n",
    "\n",
    "      col_mean = np.nanmean(lstm_data, axis = 0) \n",
    "      inds = np.where(np.isnan(lstm_data)) \n",
    "      lstm_data[inds] = np.take(col_mean, inds[1])\n",
    "\n",
    "      reshaped_data = np.repeat(lstm_data[:, np.newaxis, :], sequence_len, axis=1)\n",
    "      for i in range(reshaped_data.shape[1]):\n",
    "        reshaped_data[:, i, :] = np.roll(reshaped_data[:, i, :], i, axis=0)\n",
    "\n",
    "      # discard the top rows and reverse the order of the shifted sequence to reflect the correct order\n",
    "      reshaped_data = reshaped_data[sequence_len-1:, ::-1, :]\n",
    "      ann_data = ann_data[sequence_len-1:, :]\n",
    "      embedding_data = embedding_data[sequence_len-1:, :]\n",
    "      word2vec_data = word2vec_data[sequence_len-1:, :]\n",
    "      target = target.values[sequence_len-1:]\n",
    "      returns = returns.values[sequence_len-1:]\n",
    "      dates = dates[sequence_len-1:]\n",
    "\n",
    "      data_list.append(reshaped_data)\n",
    "      ann_list.append(ann_data)\n",
    "      embedding_list.append(embedding_data)\n",
    "      word2vec_list.append(word2vec_data)\n",
    "      target_list.append(target)\n",
    "      returns_list.append(returns)\n",
    "      date_list.append(dates)\n",
    "\n",
    "    reshaped_data, ann_data, embedding_data, word2vec_data, target, returns, dates = np.concatenate(data_list), np.concatenate(ann_list), np.concatenate(embedding_list), np.concatenate(word2vec_list), np.concatenate(target_list), np.concatenate(returns_list), np.concatenate(date_list)\n",
    "\n",
    "    return (reshaped_data, ann_data, embedding_data, word2vec_data), target, returns, dates\n",
    "\n",
    "  x_train_tuple, y_train, train_returns, train_dates = reshape_data(train_data)\n",
    "  x_test_tuple, y_test, test_returns, test_dates = reshape_data(test_data)\n",
    "\n",
    "  x_train_lstm, x_train_ann, x_train_embedding, x_train_word2vec = x_train_tuple\n",
    "  x_test_lstm, x_test_ann, x_test_embedding, x_test_word2vec = x_test_tuple\n",
    "  \n",
    "  # create model\n",
    "  from keras.layers import Input, Embedding, LSTM, Dense\n",
    "  from keras.models import Model\n",
    "\n",
    "  lstm_params = {\n",
    "      'batch_size': 252,\n",
    "      'epochs': 5,\n",
    "  }\n",
    "\n",
    "  train_len = x_train_lstm.shape[0]\n",
    "  test_len = x_test_lstm.shape[0]\n",
    "\n",
    "  # tweak to match with batch_size\n",
    "  train_until = train_len - train_len % lstm_params['batch_size']\n",
    "  test_until = test_len - test_len % lstm_params['batch_size']\n",
    "\n",
    "  x_train_lstm, x_train_ann, x_train_embedding, x_train_word2vec = x_train_lstm[:train_until], x_train_ann[:train_until], x_train_embedding[:train_until], x_train_word2vec[:train_until]\n",
    "  y_train = y_train[:train_until]\n",
    "  train_returns = train_returns[:train_until]\n",
    "  train_dates = train_dates[:train_until]\n",
    "  x_test_lstm, x_test_ann, x_test_embedding, x_test_word2vec = x_test_lstm[:test_until], x_test_ann[:test_until], x_test_embedding[:test_until], x_test_word2vec[:test_until]\n",
    "  y_test = y_test[:test_until]\n",
    "  test_returns = test_returns[:test_until]\n",
    "  test_dates = test_dates[:test_until]\n",
    "\n",
    "  train_len = x_train_lstm.shape[0]\n",
    "  test_len = x_test_lstm.shape[0]\n",
    "\n",
    "  np.random.seed(1)\n",
    "  train_shuffle = np.random.permutation(range(train_len))\n",
    "  test_shuffle = np.random.permutation(range(test_len))\n",
    "  x_train_lstm, x_train_ann, x_train_embedding, x_train_word2vec, y_train, train_returns, train_dates = x_train_lstm[train_shuffle], x_train_ann[train_shuffle], x_train_embedding[train_shuffle], x_train_word2vec[train_shuffle], y_train[train_shuffle], train_returns[train_shuffle], train_dates[train_shuffle]\n",
    "  x_test_lstm, x_test_ann, x_test_embedding, x_test_word2vec, y_test, test_returns, test_dates = x_test_lstm[test_shuffle], x_test_ann[test_shuffle], x_test_embedding[test_shuffle], x_test_word2vec[test_shuffle], y_test[test_shuffle], test_returns[test_shuffle], test_dates[test_shuffle]\n",
    "\n",
    "  def create_model():\n",
    "    sequence_len = x_train_lstm.shape[1]\n",
    "\n",
    "    lstm_input = Input(shape=(sequence_len, len(lstm_features_list)), name='lstm_input')\n",
    "    lstm_out = LSTM(21)(lstm_input)\n",
    "\n",
    "    additional_input = Input(shape=(len(additional_features_list),), name='additional_input')\n",
    "  #   word2vec_input = Input(shape=(len(word2vec_features_list),), name='word2vec_input')\n",
    "\n",
    "  #   concat_layers = [lstm_out, additional_input, word2vec_input]\n",
    "    concat_layers = [lstm_out, additional_input]\n",
    "\n",
    "  #   input_layers = [lstm_input, additional_input, word2vec_input]\n",
    "    input_layers = [lstm_input, additional_input]\n",
    "\n",
    "    # embeddings\n",
    "    for i in range(x_train_embedding.shape[1]):\n",
    "      vocab_size = x_train_embedding[:, i].max() + 1\n",
    "      y = Input(shape=(1,), dtype='int32', name='embedding_input_%d' % i)\n",
    "      input_layers.append(y)\n",
    "      y = Embedding(input_dim=vocab_size, output_dim=21)(y)\n",
    "      concat_layers.append(Flatten()(y))\n",
    "\n",
    "    x = keras.layers.concatenate(concat_layers)\n",
    "\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "\n",
    "    main_output = Dense(1, activation='sigmoid', name='output')(x)\n",
    "\n",
    "    model = Model(inputs=input_layers, outputs=[main_output])\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "  \n",
    "  def get_sigma(scaled_scores, test_returns, test_dates):\n",
    "    '''\n",
    "    scaled_scores: numpy array\n",
    "    test_returns: numpy array\n",
    "    test_dates: numpy array\n",
    "    '''\n",
    "\n",
    "    grouped_dict = pd.Series(scaled_scores).index.groupby(test_dates)\n",
    "\n",
    "    day_scores = []\n",
    "    for ts in grouped_dict:\n",
    "      grouped_index = grouped_dict[ts]\n",
    "      day_score = np.dot(test_returns[grouped_index], scaled_scores[grouped_index])\n",
    "      day_scores.append(day_score)\n",
    "\n",
    "    return np.mean(day_scores) / np.std(day_scores, ddof=1)\n",
    "\n",
    "\n",
    "  prev_sigma = None\n",
    "  combined_model = create_model()\n",
    "\n",
    "  # train for at most 500 epochs\n",
    "  for i in range(500):\n",
    "    # x_train_list = [x_train_lstm, x_train_ann] + [x_train_word2vec] + [x_train_embedding[:, i] for i in range(x_train_embedding.shape[1])]\n",
    "    x_train_list = [x_train_lstm, x_train_ann] + [x_train_embedding[:, i] for i in range(x_train_embedding.shape[1])]\n",
    "    # x_test_list = [x_test_lstm, x_test_ann] + [x_test_word2vec] + [x_test_embedding[:, i] for i in range(x_test_embedding.shape[1])]\n",
    "    x_test_list = [x_test_lstm, x_test_ann] + [x_test_embedding[:, i] for i in range(x_test_embedding.shape[1])]\n",
    "\n",
    "    combined_model.fit(x_train_list,\n",
    "                   y_train,\n",
    "                   batch_size=lstm_params['batch_size'],\n",
    "                   epochs=1,\n",
    "                   verbose=1,\n",
    "                   validation_data=(x_test_list, y_test),\n",
    "                   shuffle=False)\n",
    "\n",
    "    scaled_scores = combined_model.predict(x_test_list, batch_size=lstm_params['batch_size']).reshape(-1) * 2 - 1\n",
    "    current_sigma = get_sigma(scaled_scores, test_returns, test_dates)\n",
    "    \n",
    "    print('current sigma: %f' % current_sigma)\n",
    "\n",
    "    # early stopping if sigma score does not improve for 1 epoch\n",
    "    if prev_sigma is not None and current_sigma < prev_sigma:\n",
    "      break\n",
    "    else:\n",
    "      prev_sigma = current_sigma\n",
    "  \n",
    "  pred_classes = (scaled_scores > 0).astype(int)\n",
    "  from sklearn.metrics import confusion_matrix\n",
    "  conf_mat = confusion_matrix(y_test, pred_classes)\n",
    "  \n",
    "  print('Year %d: sigma=%f' % (train_end_year, current_sigma))\n",
    "  print(conf_mat)\n",
    "  \n",
    "  if v_str not in results:\n",
    "    results[v_str] = {}\n",
    "  results[v_str][train_end_year] = {}\n",
    "  results[v_str][train_end_year]['y_test'] = y_test\n",
    "  results[v_str][train_end_year]['scaled_scores'] = scaled_scores\n",
    "  results[v_str][train_end_year]['sigma_score'] = current_sigma\n",
    "  results[v_str][train_end_year]['conf_mat'] = conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"sliding_final_output_from_results_dict_V5.p\"\n",
    "fileobj = open(filename, \"wb\")\n",
    "pickle.dump(results, fileobj)\n",
    "fileobj.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HIEJm4B1vjLE"
   },
   "source": [
    "### Model Building v6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1391
    },
    "colab_type": "code",
    "id": "VemV8OpSvUI3",
    "outputId": "0fe9b6a7-0cfe-4c0f-8d8f-410e2681f015",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 361116 samples, validate on 165816 samples\n",
      "Epoch 1/1\n",
      "361116/361116 [==============================] - 77s 214us/step - loss: 0.6733 - acc: 0.5808 - val_loss: 0.7168 - val_acc: 0.5303\n",
      "current sigma: 0.171958\n",
      "Train on 361116 samples, validate on 165816 samples\n",
      "Epoch 1/1\n",
      "361116/361116 [==============================] - 86s 239us/step - loss: 0.6530 - acc: 0.6098 - val_loss: 0.7224 - val_acc: 0.5323\n",
      "current sigma: 0.266378\n",
      "Train on 361116 samples, validate on 165816 samples\n",
      "Epoch 1/1\n",
      "361116/361116 [==============================] - 91s 253us/step - loss: 0.6389 - acc: 0.6282 - val_loss: 0.7293 - val_acc: 0.5322\n",
      "current sigma: 0.296818\n",
      "Train on 361116 samples, validate on 165816 samples\n",
      "Epoch 1/1\n",
      "361116/361116 [==============================] - 106s 295us/step - loss: 0.6286 - acc: 0.6402 - val_loss: 0.7391 - val_acc: 0.5291\n",
      "current sigma: 0.301103\n",
      "Train on 361116 samples, validate on 165816 samples\n",
      "Epoch 1/1\n",
      "361116/361116 [==============================] - 94s 261us/step - loss: 0.6208 - acc: 0.6478 - val_loss: 0.7472 - val_acc: 0.5263\n",
      "current sigma: 0.306225\n",
      "Train on 361116 samples, validate on 165816 samples\n",
      "Epoch 1/1\n",
      "361116/361116 [==============================] - 100s 276us/step - loss: 0.6150 - acc: 0.6541 - val_loss: 0.7536 - val_acc: 0.5254\n",
      "current sigma: 0.312567\n",
      "Train on 361116 samples, validate on 165816 samples\n",
      "Epoch 1/1\n",
      "361116/361116 [==============================] - 92s 254us/step - loss: 0.6106 - acc: 0.6590 - val_loss: 0.7586 - val_acc: 0.5234\n",
      "current sigma: 0.303233\n",
      "Year 2011: sigma=0.303233\n",
      "[[39746 48370]\n",
      " [30654 47046]]\n",
      "Train on 369432 samples, validate on 168336 samples\n",
      "Epoch 1/1\n",
      "369432/369432 [==============================] - 91s 247us/step - loss: 0.6709 - acc: 0.5877 - val_loss: 0.6882 - val_acc: 0.5648\n",
      "current sigma: 0.786031\n",
      "Train on 369432 samples, validate on 168336 samples\n",
      "Epoch 1/1\n",
      "369432/369432 [==============================] - 92s 249us/step - loss: 0.6536 - acc: 0.6096 - val_loss: 0.6976 - val_acc: 0.5604\n",
      "current sigma: 0.774108\n",
      "Year 2012: sigma=0.774108\n",
      "[[38037 46391]\n",
      " [27608 56300]]\n",
      "Train on 369684 samples, validate on 160020 samples\n",
      "Epoch 1/1\n",
      "369684/369684 [==============================] - 90s 243us/step - loss: 0.6687 - acc: 0.5862 - val_loss: 0.6821 - val_acc: 0.5677\n",
      "current sigma: 0.681290\n",
      "Train on 369684 samples, validate on 160020 samples\n",
      "Epoch 1/1\n",
      "369684/369684 [==============================] - 94s 255us/step - loss: 0.6471 - acc: 0.6166 - val_loss: 0.6988 - val_acc: 0.5717\n",
      "current sigma: 0.698852\n",
      "Train on 369684 samples, validate on 160020 samples\n",
      "Epoch 1/1\n",
      "369684/369684 [==============================] - 88s 238us/step - loss: 0.6371 - acc: 0.6266 - val_loss: 0.7132 - val_acc: 0.5709\n",
      "current sigma: 0.704105\n",
      "Train on 369684 samples, validate on 160020 samples\n",
      "Epoch 1/1\n",
      "369684/369684 [==============================] - 88s 238us/step - loss: 0.6297 - acc: 0.6339 - val_loss: 0.7282 - val_acc: 0.5641\n",
      "current sigma: 0.671013\n",
      "Year 2013: sigma=0.671013\n",
      "[[47527 33887]\n",
      " [35864 42742]]\n",
      "Train on 350028 samples, validate on 165816 samples\n",
      "Epoch 1/1\n",
      "350028/350028 [==============================] - 82s 235us/step - loss: 0.6635 - acc: 0.5925 - val_loss: 0.7007 - val_acc: 0.5346\n",
      "current sigma: 0.458058\n",
      "Train on 350028 samples, validate on 165816 samples\n",
      "Epoch 1/1\n",
      "350028/350028 [==============================] - 86s 246us/step - loss: 0.6411 - acc: 0.6230 - val_loss: 0.7208 - val_acc: 0.5286\n",
      "current sigma: 0.332549\n",
      "Year 2014: sigma=0.332549\n",
      "[[44014 36677]\n",
      " [41486 43639]]\n",
      "Train on 378000 samples, validate on 180432 samples\n",
      "Epoch 1/1\n",
      "378000/378000 [==============================] - 95s 251us/step - loss: 0.6726 - acc: 0.5828 - val_loss: 0.7082 - val_acc: 0.5215\n",
      "current sigma: 0.328826\n",
      "Train on 378000 samples, validate on 180432 samples\n",
      "Epoch 1/1\n",
      "378000/378000 [==============================] - 96s 254us/step - loss: 0.6489 - acc: 0.6162 - val_loss: 0.7239 - val_acc: 0.5187\n",
      "current sigma: 0.358404\n",
      "Train on 378000 samples, validate on 180432 samples\n",
      "Epoch 1/1\n",
      "378000/378000 [==============================] - 98s 260us/step - loss: 0.6366 - acc: 0.6311 - val_loss: 0.7376 - val_acc: 0.5175\n",
      "current sigma: 0.348603\n",
      "Year 2015: sigma=0.348603\n",
      "[[58679 27262]\n",
      " [59796 34695]]\n",
      "Train on 2187864 samples, validate on 184968 samples\n",
      "Epoch 1/1\n",
      "2187864/2187864 [==============================] - 610s 279us/step - loss: 0.6740 - acc: 0.5779 - val_loss: 0.7002 - val_acc: 0.5424\n",
      "current sigma: 0.482190\n",
      "Train on 2187864 samples, validate on 184968 samples\n",
      "Epoch 1/1\n",
      "2187864/2187864 [==============================] - 533s 244us/step - loss: 0.6622 - acc: 0.5969 - val_loss: 0.7157 - val_acc: 0.5351\n",
      "current sigma: 0.407380\n",
      "Year 2016: sigma=0.407380\n",
      "[[44898 46038]\n",
      " [39945 54087]]\n",
      "Wall time: 1h 7min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# remark for Wei Han: add sentiments and other industry-neutralised features from v5\n",
    "\n",
    "v_str = 'v6'\n",
    "\n",
    "train_end_years = [2011, 2012, 2013, 2014, 2015, 2016]\n",
    "\n",
    "lstm_features_list = [\n",
    "    'returnsClosePrevRaw1',\n",
    "    'returnsOpenPrevRaw1',\n",
    "    'returnsClosePrevMktres1',\n",
    "    'returnsOpenPrevMktres1',\n",
    "    'returnsClosePrevRaw10',\n",
    "    'returnsOpenPrevRaw10',\n",
    "    'returnsClosePrevMktres10',\n",
    "    'returnsOpenPrevMktres10',\n",
    "    'open_close',\n",
    "    'oc_average',\n",
    "    'turnover',\n",
    "    'open_close_relative',\n",
    "    'turnover_relative',\n",
    "    'volume_relative',\n",
    "    'returnsOpenPrevMktres1_relative',\n",
    "    'returnsOpenPrevMktres10_relative',\n",
    "    'returnsClosePrevRaw1_stan',\n",
    "    'returnsOpenPrevRaw1_stan',\n",
    "    'returnsClosePrevMktres1_stan',\n",
    "    'returnsOpenPrevMktres1_stan',\n",
    "    'returnsClosePrevRaw10_stan',\n",
    "    'returnsOpenPrevRaw10_stan',\n",
    "    'returnsClosePrevMktres10_stan',\n",
    "    'returnsOpenPrevMktres10_stan',\n",
    "    'open_close_stan',\n",
    "    'oc_average_stan',\n",
    "    'turnover_stan',\n",
    "    'open_close_relative_stan',\n",
    "    'turnover_relative_stan',\n",
    "    'volume_relative_stan',\n",
    "    'returnsOpenPrevMktres1_relative_stan',\n",
    "    'returnsOpenPrevMktres10_relative_stan',\n",
    "]\n",
    "\n",
    "\n",
    "additional_features_list = [\n",
    "    'urgency_min',\n",
    "    'urgency_count',\n",
    "    'takeSequence_min',\n",
    "    'takeSequence_max',\n",
    "    'bodySize_mean',\n",
    "    'wordCount_mean',\n",
    "    'sentenceCount_mean',\n",
    "    'companyCount_mean',\n",
    "    'marketCommentary_mean',\n",
    "    'relevance_mean',\n",
    "    'sentimentNegative_mean',\n",
    "    'sentimentNeutral_mean',\n",
    "    'sentimentPositive_mean',\n",
    "    'sentimentWordCount_mean',\n",
    "    'sentimentNegative_mean_stan',\n",
    "    'sentimentNeutral_mean_stan',\n",
    "    'sentimentPositive_mean_stan',\n",
    "    'sentimentWordCount_mean_stan',\n",
    "    'noveltyCount12H_mean',\n",
    "    'noveltyCount24H_mean',\n",
    "    'noveltyCount3D_mean',\n",
    "    'noveltyCount5D_mean',\n",
    "    'noveltyCount7D_mean',\n",
    "    'volumeCounts12H_mean',\n",
    "    'volumeCounts24H_mean',\n",
    "    'volumeCounts3D_mean',\n",
    "    'volumeCounts5D_mean',\n",
    "    'volumeCounts7D_mean',\n",
    "]\n",
    "\n",
    "label_encoded_features = [\n",
    "    'dayofweek',\n",
    "    'month',\n",
    "]\n",
    "\n",
    "continuous_features_list = lstm_features_list + additional_features_list\n",
    "features_list = lstm_features_list + additional_features_list + label_encoded_features\n",
    "\n",
    "for train_end_year in train_end_years:\n",
    "  # a trick to get testing stats\n",
    "  if train_end_year == 2016:\n",
    "    train_end_year = 2015\n",
    "    train_start, train_end = pd.to_datetime('2011-01-01'), pd.to_datetime('2016-06-30')\n",
    "    test_start, test_end = pd.to_datetime('%d-07-01' % (train_end_year + 1)), pd.to_datetime('%d-12-31' % (train_end_year + 1))\n",
    "    train_end_year = 2016\n",
    "  else:\n",
    "    train_start, train_end = pd.to_datetime('%d-01-01' % train_end_year), pd.to_datetime('%d-12-31' % train_end_year)\n",
    "    test_start, test_end = pd.to_datetime('%d-01-01' % (train_end_year + 1)), pd.to_datetime('%d-06-30' % (train_end_year + 1))\n",
    "  \n",
    "  \n",
    "  # divide into train and test periods\n",
    "\n",
    "  train_data = combined_data.set_index('time')[train_start: train_end]\n",
    "  test_data = combined_data.set_index('time')[test_start: test_end]\n",
    "\n",
    "  from sklearn.preprocessing import StandardScaler\n",
    "  scaler = StandardScaler()\n",
    "  train_data[continuous_features_list] = scaler.fit_transform(train_data[continuous_features_list])\n",
    "  test_data[continuous_features_list] = scaler.transform(test_data[continuous_features_list])\n",
    "\n",
    "  # reshape data\n",
    "  word2vec_features_list = []\n",
    "\n",
    "  def reshape_data(data, sequence_len=21, features_list=features_list):\n",
    "    data_list, ann_list, embedding_list, word2vec_list, target_list, returns_list, date_list = [], [], [], [], [], [], []\n",
    "\n",
    "    for assetCode, asset_data in data.groupby('assetCode'):\n",
    "      # binarize the target variable\n",
    "      returns = asset_data['returnsOpenNextMktres10']\n",
    "      target = (asset_data['returnsOpenNextMktres10'] > 0).astype(int)\n",
    "      dates = asset_data.index\n",
    "\n",
    "      # arrays\n",
    "      lstm_data = asset_data[lstm_features_list].values\n",
    "      ann_data = asset_data[additional_features_list].values\n",
    "      embedding_data = asset_data[label_encoded_features].values\n",
    "      word2vec_data = asset_data[word2vec_features_list].values\n",
    "\n",
    "      col_mean = np.nanmean(lstm_data, axis = 0) \n",
    "      inds = np.where(np.isnan(lstm_data)) \n",
    "      lstm_data[inds] = np.take(col_mean, inds[1])\n",
    "\n",
    "      reshaped_data = np.repeat(lstm_data[:, np.newaxis, :], sequence_len, axis=1)\n",
    "      for i in range(reshaped_data.shape[1]):\n",
    "        reshaped_data[:, i, :] = np.roll(reshaped_data[:, i, :], i, axis=0)\n",
    "\n",
    "      # discard the top rows and reverse the order of the shifted sequence to reflect the correct order\n",
    "      reshaped_data = reshaped_data[sequence_len-1:, ::-1, :]\n",
    "      ann_data = ann_data[sequence_len-1:, :]\n",
    "      embedding_data = embedding_data[sequence_len-1:, :]\n",
    "      word2vec_data = word2vec_data[sequence_len-1:, :]\n",
    "      target = target.values[sequence_len-1:]\n",
    "      returns = returns.values[sequence_len-1:]\n",
    "      dates = dates[sequence_len-1:]\n",
    "\n",
    "      data_list.append(reshaped_data)\n",
    "      ann_list.append(ann_data)\n",
    "      embedding_list.append(embedding_data)\n",
    "      word2vec_list.append(word2vec_data)\n",
    "      target_list.append(target)\n",
    "      returns_list.append(returns)\n",
    "      date_list.append(dates)\n",
    "\n",
    "    reshaped_data, ann_data, embedding_data, word2vec_data, target, returns, dates = np.concatenate(data_list), np.concatenate(ann_list), np.concatenate(embedding_list), np.concatenate(word2vec_list), np.concatenate(target_list), np.concatenate(returns_list), np.concatenate(date_list)\n",
    "\n",
    "    return (reshaped_data, ann_data, embedding_data, word2vec_data), target, returns, dates\n",
    "\n",
    "  x_train_tuple, y_train, train_returns, train_dates = reshape_data(train_data)\n",
    "  x_test_tuple, y_test, test_returns, test_dates = reshape_data(test_data)\n",
    "\n",
    "  x_train_lstm, x_train_ann, x_train_embedding, x_train_word2vec = x_train_tuple\n",
    "  x_test_lstm, x_test_ann, x_test_embedding, x_test_word2vec = x_test_tuple\n",
    "  \n",
    "  # create model\n",
    "  from keras.layers import Input, Embedding, LSTM, Dense\n",
    "  from keras.models import Model\n",
    "\n",
    "  lstm_params = {\n",
    "      'batch_size': 252,\n",
    "      'epochs': 5,\n",
    "  }\n",
    "\n",
    "  train_len = x_train_lstm.shape[0]\n",
    "  test_len = x_test_lstm.shape[0]\n",
    "\n",
    "  # tweak to match with batch_size\n",
    "  train_until = train_len - train_len % lstm_params['batch_size']\n",
    "  test_until = test_len - test_len % lstm_params['batch_size']\n",
    "\n",
    "  x_train_lstm, x_train_ann, x_train_embedding, x_train_word2vec = x_train_lstm[:train_until], x_train_ann[:train_until], x_train_embedding[:train_until], x_train_word2vec[:train_until]\n",
    "  y_train = y_train[:train_until]\n",
    "  train_returns = train_returns[:train_until]\n",
    "  train_dates = train_dates[:train_until]\n",
    "  x_test_lstm, x_test_ann, x_test_embedding, x_test_word2vec = x_test_lstm[:test_until], x_test_ann[:test_until], x_test_embedding[:test_until], x_test_word2vec[:test_until]\n",
    "  y_test = y_test[:test_until]\n",
    "  test_returns = test_returns[:test_until]\n",
    "  test_dates = test_dates[:test_until]\n",
    "\n",
    "  train_len = x_train_lstm.shape[0]\n",
    "  test_len = x_test_lstm.shape[0]\n",
    "\n",
    "  np.random.seed(1)\n",
    "  train_shuffle = np.random.permutation(range(train_len))\n",
    "  test_shuffle = np.random.permutation(range(test_len))\n",
    "  x_train_lstm, x_train_ann, x_train_embedding, x_train_word2vec, y_train, train_returns, train_dates = x_train_lstm[train_shuffle], x_train_ann[train_shuffle], x_train_embedding[train_shuffle], x_train_word2vec[train_shuffle], y_train[train_shuffle], train_returns[train_shuffle], train_dates[train_shuffle]\n",
    "  x_test_lstm, x_test_ann, x_test_embedding, x_test_word2vec, y_test, test_returns, test_dates = x_test_lstm[test_shuffle], x_test_ann[test_shuffle], x_test_embedding[test_shuffle], x_test_word2vec[test_shuffle], y_test[test_shuffle], test_returns[test_shuffle], test_dates[test_shuffle]\n",
    "\n",
    "  def create_model():\n",
    "    sequence_len = x_train_lstm.shape[1]\n",
    "\n",
    "    lstm_input = Input(shape=(sequence_len, len(lstm_features_list)), name='lstm_input')\n",
    "    lstm_out = LSTM(21)(lstm_input)\n",
    "\n",
    "    additional_input = Input(shape=(len(additional_features_list),), name='additional_input')\n",
    "  #   word2vec_input = Input(shape=(len(word2vec_features_list),), name='word2vec_input')\n",
    "\n",
    "  #   concat_layers = [lstm_out, additional_input, word2vec_input]\n",
    "    concat_layers = [lstm_out, additional_input]\n",
    "\n",
    "  #   input_layers = [lstm_input, additional_input, word2vec_input]\n",
    "    input_layers = [lstm_input, additional_input]\n",
    "\n",
    "    # embeddings\n",
    "    for i in range(x_train_embedding.shape[1]):\n",
    "      vocab_size = x_train_embedding[:, i].max() + 1\n",
    "      y = Input(shape=(1,), dtype='int32', name='embedding_input_%d' % i)\n",
    "      input_layers.append(y)\n",
    "      y = Embedding(input_dim=vocab_size, output_dim=21)(y)\n",
    "      concat_layers.append(Flatten()(y))\n",
    "\n",
    "    x = keras.layers.concatenate(concat_layers)\n",
    "\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dropout(.2)(x)\n",
    "\n",
    "    main_output = Dense(1, activation='sigmoid', name='output')(x)\n",
    "\n",
    "    model = Model(inputs=input_layers, outputs=[main_output])\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "  \n",
    "  def get_sigma(scaled_scores, test_returns, test_dates):\n",
    "    '''\n",
    "    scaled_scores: numpy array\n",
    "    test_returns: numpy array\n",
    "    test_dates: numpy array\n",
    "    '''\n",
    "\n",
    "    grouped_dict = pd.Series(scaled_scores).index.groupby(test_dates)\n",
    "\n",
    "    day_scores = []\n",
    "    for ts in grouped_dict:\n",
    "      grouped_index = grouped_dict[ts]\n",
    "      day_score = np.dot(test_returns[grouped_index], scaled_scores[grouped_index])\n",
    "      day_scores.append(day_score)\n",
    "\n",
    "    return np.mean(day_scores) / np.std(day_scores, ddof=1)\n",
    "\n",
    "\n",
    "  prev_sigma = None\n",
    "  combined_model = create_model()\n",
    "\n",
    "  # train for at most 500 epochs\n",
    "  for i in range(500):\n",
    "    # x_train_list = [x_train_lstm, x_train_ann] + [x_train_word2vec] + [x_train_embedding[:, i] for i in range(x_train_embedding.shape[1])]\n",
    "    x_train_list = [x_train_lstm, x_train_ann] + [x_train_embedding[:, i] for i in range(x_train_embedding.shape[1])]\n",
    "    # x_test_list = [x_test_lstm, x_test_ann] + [x_test_word2vec] + [x_test_embedding[:, i] for i in range(x_test_embedding.shape[1])]\n",
    "    x_test_list = [x_test_lstm, x_test_ann] + [x_test_embedding[:, i] for i in range(x_test_embedding.shape[1])]\n",
    "\n",
    "    combined_model.fit(x_train_list,\n",
    "                   y_train,\n",
    "                   batch_size=lstm_params['batch_size'],\n",
    "                   epochs=1,\n",
    "                   verbose=1,\n",
    "                   validation_data=(x_test_list, y_test),\n",
    "                   shuffle=False)\n",
    "\n",
    "    scaled_scores = combined_model.predict(x_test_list, batch_size=lstm_params['batch_size']).reshape(-1) * 2 - 1\n",
    "    current_sigma = get_sigma(scaled_scores, test_returns, test_dates)\n",
    "    \n",
    "    print('current sigma: %f' % current_sigma)\n",
    "\n",
    "    # early stopping if sigma score does not improve for 1 epoch\n",
    "    if prev_sigma is not None and current_sigma < prev_sigma:\n",
    "      break\n",
    "    else:\n",
    "      prev_sigma = current_sigma\n",
    "  \n",
    "  pred_classes = (scaled_scores > 0).astype(int)\n",
    "  from sklearn.metrics import confusion_matrix\n",
    "  conf_mat = confusion_matrix(y_test, pred_classes)\n",
    "  \n",
    "  print('Year %d: sigma=%f' % (train_end_year, current_sigma))\n",
    "  print(conf_mat)\n",
    "  \n",
    "  if v_str not in results:\n",
    "    results[v_str] = {}\n",
    "  results[v_str][train_end_year] = {}\n",
    "  results[v_str][train_end_year]['y_test'] = y_test\n",
    "  results[v_str][train_end_year]['scaled_scores'] = scaled_scores\n",
    "  results[v_str][train_end_year]['sigma_score'] = current_sigma\n",
    "  results[v_str][train_end_year]['conf_mat'] = conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"sliding_final_output_from_results_dict_V6.p\"\n",
    "fileobj = open(filename, \"wb\")\n",
    "pickle.dump(results, fileobj)\n",
    "fileobj.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tVJmTtrBwpJT"
   },
   "source": [
    "### Model Building v7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FOdU2j6Dwod1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 361116 samples, validate on 165816 samples\n",
      "Epoch 1/1\n",
      "361116/361116 [==============================] - 75s 209us/step - loss: 0.6737 - acc: 0.5781 - val_loss: 0.7116 - val_acc: 0.5323\n",
      "current sigma: 0.212841\n",
      "Train on 361116 samples, validate on 165816 samples\n",
      "Epoch 1/1\n",
      "361116/361116 [==============================] - 82s 226us/step - loss: 0.6551 - acc: 0.6079 - val_loss: 0.7214 - val_acc: 0.5262\n",
      "current sigma: 0.254792\n",
      "Train on 361116 samples, validate on 165816 samples\n",
      "Epoch 1/1\n",
      "361116/361116 [==============================] - 87s 241us/step - loss: 0.6399 - acc: 0.6273 - val_loss: 0.7325 - val_acc: 0.5223\n",
      "current sigma: 0.216180\n",
      "Year 2011: sigma=0.216180\n",
      "[[41366 46750]\n",
      " [32460 45240]]\n",
      "Train on 369432 samples, validate on 168336 samples\n",
      "Epoch 1/1\n",
      "369432/369432 [==============================] - 86s 233us/step - loss: 0.6734 - acc: 0.5833 - val_loss: 0.6862 - val_acc: 0.5631\n",
      "current sigma: 0.743157\n",
      "Train on 369432 samples, validate on 168336 samples\n",
      "Epoch 1/1\n",
      "369432/369432 [==============================] - 91s 245us/step - loss: 0.6580 - acc: 0.6068 - val_loss: 0.6918 - val_acc: 0.5600\n",
      "current sigma: 0.743098\n",
      "Year 2012: sigma=0.743098\n",
      "[[43260 41168]\n",
      " [32907 51001]]\n",
      "Train on 369684 samples, validate on 160020 samples\n",
      "Epoch 1/1\n",
      "369684/369684 [==============================] - 87s 236us/step - loss: 0.6736 - acc: 0.5773 - val_loss: 0.6835 - val_acc: 0.5600\n",
      "current sigma: 0.640166\n",
      "Train on 369684 samples, validate on 160020 samples\n",
      "Epoch 1/1\n",
      "369684/369684 [==============================] - 90s 243us/step - loss: 0.6511 - acc: 0.6125 - val_loss: 0.6966 - val_acc: 0.5670\n",
      "current sigma: 0.684307\n",
      "Train on 369684 samples, validate on 160020 samples\n",
      "Epoch 1/1\n",
      "369684/369684 [==============================] - 90s 242us/step - loss: 0.6389 - acc: 0.6255 - val_loss: 0.7052 - val_acc: 0.5687\n",
      "current sigma: 0.661086\n",
      "Year 2013: sigma=0.661086\n",
      "[[45931 35483]\n",
      " [33536 45070]]\n",
      "Train on 350028 samples, validate on 165816 samples\n",
      "Epoch 1/1\n",
      "350028/350028 [==============================] - 85s 242us/step - loss: 0.6661 - acc: 0.5861 - val_loss: 0.6990 - val_acc: 0.5281\n",
      "current sigma: 0.307044\n",
      "Train on 350028 samples, validate on 165816 samples\n",
      "Epoch 1/1\n",
      "350028/350028 [==============================] - 86s 247us/step - loss: 0.6460 - acc: 0.6165 - val_loss: 0.7106 - val_acc: 0.5319\n",
      "current sigma: 0.399838\n",
      "Train on 350028 samples, validate on 165816 samples\n",
      "Epoch 1/1\n",
      "350028/350028 [==============================] - 87s 249us/step - loss: 0.6329 - acc: 0.6327 - val_loss: 0.7232 - val_acc: 0.5285\n",
      "current sigma: 0.351743\n",
      "Year 2014: sigma=0.351743\n",
      "[[45718 34973]\n",
      " [43215 41910]]\n",
      "Train on 378000 samples, validate on 180432 samples\n",
      "Epoch 1/1\n",
      "378000/378000 [==============================] - 92s 243us/step - loss: 0.6757 - acc: 0.5767 - val_loss: 0.7084 - val_acc: 0.5100\n",
      "current sigma: 0.195781\n",
      "Train on 378000 samples, validate on 180432 samples\n",
      "Epoch 1/1\n",
      "378000/378000 [==============================] - 96s 253us/step - loss: 0.6545 - acc: 0.6085 - val_loss: 0.7238 - val_acc: 0.5086\n",
      "current sigma: 0.183718\n",
      "Year 2015: sigma=0.183718\n",
      "[[50561 35380]\n",
      " [53289 41202]]\n",
      "Train on 2187864 samples, validate on 184968 samples\n",
      "Epoch 1/1\n",
      "2187864/2187864 [==============================] - 613s 280us/step - loss: 0.6753 - acc: 0.5751 - val_loss: 0.6995 - val_acc: 0.5417\n",
      "current sigma: 0.462434\n",
      "Train on 2187864 samples, validate on 184968 samples\n",
      "Epoch 1/1\n",
      "2187864/2187864 [==============================] - 498s 228us/step - loss: 0.6637 - acc: 0.5941 - val_loss: 0.7140 - val_acc: 0.5364\n",
      "current sigma: 0.471795\n",
      "Train on 2187864 samples, validate on 184968 samples\n",
      "Epoch 1/1\n",
      "2187864/2187864 [==============================] - 493s 225us/step - loss: 0.6559 - acc: 0.6049 - val_loss: 0.7208 - val_acc: 0.5364\n",
      "current sigma: 0.514144\n",
      "Train on 2187864 samples, validate on 184968 samples\n",
      "Epoch 1/1\n",
      "2187864/2187864 [==============================] - 496s 227us/step - loss: 0.6502 - acc: 0.6123 - val_loss: 0.7245 - val_acc: 0.5351\n",
      "current sigma: 0.527568\n",
      "Train on 2187864 samples, validate on 184968 samples\n",
      "Epoch 1/1\n",
      "2187864/2187864 [==============================] - 493s 225us/step - loss: 0.6459 - acc: 0.6177 - val_loss: 0.7276 - val_acc: 0.5343\n",
      "current sigma: 0.519554\n",
      "Year 2016: sigma=0.519554\n",
      "[[47804 43132]\n",
      " [43000 51032]]\n",
      "Wall time: 1h 18min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# remark for Wei Han: add sentiments and other industry-neutralised features from v5\n",
    "\n",
    "v_str = 'v7'\n",
    "\n",
    "train_end_years = [2011, 2012, 2013, 2014, 2015, 2016]\n",
    "\n",
    "lstm_features_list = [\n",
    "    'returnsClosePrevRaw1',\n",
    "    'returnsOpenPrevRaw1',\n",
    "    'returnsClosePrevMktres1',\n",
    "    'returnsOpenPrevMktres1',\n",
    "    'returnsClosePrevRaw10',\n",
    "    'returnsOpenPrevRaw10',\n",
    "    'returnsClosePrevMktres10',\n",
    "    'returnsOpenPrevMktres10',\n",
    "    'open_close',\n",
    "    'oc_average',\n",
    "    'turnover',\n",
    "    'open_close_relative',\n",
    "    'turnover_relative',\n",
    "    'volume_relative',\n",
    "    'returnsOpenPrevMktres1_relative',\n",
    "    'returnsOpenPrevMktres10_relative',\n",
    "    'returnsClosePrevRaw1_stan',\n",
    "    'returnsOpenPrevRaw1_stan',\n",
    "    'returnsClosePrevMktres1_stan',\n",
    "    'returnsOpenPrevMktres1_stan',\n",
    "    'returnsClosePrevRaw10_stan',\n",
    "    'returnsOpenPrevRaw10_stan',\n",
    "    'returnsClosePrevMktres10_stan',\n",
    "    'returnsOpenPrevMktres10_stan',\n",
    "    'open_close_stan',\n",
    "    'oc_average_stan',\n",
    "    'turnover_stan',\n",
    "    'open_close_relative_stan',\n",
    "    'turnover_relative_stan',\n",
    "    'volume_relative_stan',\n",
    "    'returnsOpenPrevMktres1_relative_stan',\n",
    "    'returnsOpenPrevMktres10_relative_stan',\n",
    "]\n",
    "\n",
    "\n",
    "additional_features_list = [\n",
    "    'urgency_min',\n",
    "    'urgency_count',\n",
    "    'takeSequence_min',\n",
    "    'takeSequence_max',\n",
    "    'bodySize_mean',\n",
    "    'wordCount_mean',\n",
    "    'sentenceCount_mean',\n",
    "    'companyCount_mean',\n",
    "    'marketCommentary_mean',\n",
    "    'relevance_mean',\n",
    "    'sentimentNegative_mean',\n",
    "    'sentimentNeutral_mean',\n",
    "    'sentimentPositive_mean',\n",
    "    'sentimentWordCount_mean',\n",
    "    'sentimentNegative_mean_stan',\n",
    "    'sentimentNeutral_mean_stan',\n",
    "    'sentimentPositive_mean_stan',\n",
    "    'sentimentWordCount_mean_stan',\n",
    "    'noveltyCount12H_mean',\n",
    "    'noveltyCount24H_mean',\n",
    "    'noveltyCount3D_mean',\n",
    "    'noveltyCount5D_mean',\n",
    "    'noveltyCount7D_mean',\n",
    "    'volumeCounts12H_mean',\n",
    "    'volumeCounts24H_mean',\n",
    "    'volumeCounts3D_mean',\n",
    "    'volumeCounts5D_mean',\n",
    "    'volumeCounts7D_mean',\n",
    "]\n",
    "\n",
    "label_encoded_features = [\n",
    "    'dayofweek',\n",
    "    'month',\n",
    "]\n",
    "\n",
    "continuous_features_list = lstm_features_list + additional_features_list\n",
    "features_list = lstm_features_list + additional_features_list + label_encoded_features\n",
    "\n",
    "for train_end_year in train_end_years:\n",
    "  # a trick to get testing stats\n",
    "  if train_end_year == 2016:\n",
    "    train_end_year = 2015\n",
    "    train_start, train_end = pd.to_datetime('2011-01-01'), pd.to_datetime('2016-06-30')\n",
    "    test_start, test_end = pd.to_datetime('%d-07-01' % (train_end_year + 1)), pd.to_datetime('%d-12-31' % (train_end_year + 1))\n",
    "    train_end_year = 2016\n",
    "  else:\n",
    "    train_start, train_end = pd.to_datetime('%d-01-01' % train_end_year), pd.to_datetime('%d-12-31' % train_end_year)\n",
    "    test_start, test_end = pd.to_datetime('%d-01-01' % (train_end_year + 1)), pd.to_datetime('%d-06-30' % (train_end_year + 1))\n",
    "  \n",
    "  \n",
    "  # divide into train and test periods\n",
    "\n",
    "  train_data = combined_data.set_index('time')[train_start: train_end]\n",
    "  test_data = combined_data.set_index('time')[test_start: test_end]\n",
    "\n",
    "  from sklearn.preprocessing import StandardScaler\n",
    "  scaler = StandardScaler()\n",
    "  train_data[continuous_features_list] = scaler.fit_transform(train_data[continuous_features_list])\n",
    "  test_data[continuous_features_list] = scaler.transform(test_data[continuous_features_list])\n",
    "\n",
    "  # reshape data\n",
    "  word2vec_features_list = []\n",
    "\n",
    "  def reshape_data(data, sequence_len=21, features_list=features_list):\n",
    "    data_list, ann_list, embedding_list, word2vec_list, target_list, returns_list, date_list = [], [], [], [], [], [], []\n",
    "\n",
    "    for assetCode, asset_data in data.groupby('assetCode'):\n",
    "      # binarize the target variable\n",
    "      returns = asset_data['returnsOpenNextMktres10']\n",
    "      target = (asset_data['returnsOpenNextMktres10'] > 0).astype(int)\n",
    "      dates = asset_data.index\n",
    "\n",
    "      # arrays\n",
    "      lstm_data = asset_data[lstm_features_list].values\n",
    "      ann_data = asset_data[additional_features_list].values\n",
    "      embedding_data = asset_data[label_encoded_features].values\n",
    "      word2vec_data = asset_data[word2vec_features_list].values\n",
    "\n",
    "      col_mean = np.nanmean(lstm_data, axis = 0) \n",
    "      inds = np.where(np.isnan(lstm_data)) \n",
    "      lstm_data[inds] = np.take(col_mean, inds[1])\n",
    "\n",
    "      reshaped_data = np.repeat(lstm_data[:, np.newaxis, :], sequence_len, axis=1)\n",
    "      for i in range(reshaped_data.shape[1]):\n",
    "        reshaped_data[:, i, :] = np.roll(reshaped_data[:, i, :], i, axis=0)\n",
    "\n",
    "      # discard the top rows and reverse the order of the shifted sequence to reflect the correct order\n",
    "      reshaped_data = reshaped_data[sequence_len-1:, ::-1, :]\n",
    "      ann_data = ann_data[sequence_len-1:, :]\n",
    "      embedding_data = embedding_data[sequence_len-1:, :]\n",
    "      word2vec_data = word2vec_data[sequence_len-1:, :]\n",
    "      target = target.values[sequence_len-1:]\n",
    "      returns = returns.values[sequence_len-1:]\n",
    "      dates = dates[sequence_len-1:]\n",
    "\n",
    "      data_list.append(reshaped_data)\n",
    "      ann_list.append(ann_data)\n",
    "      embedding_list.append(embedding_data)\n",
    "      word2vec_list.append(word2vec_data)\n",
    "      target_list.append(target)\n",
    "      returns_list.append(returns)\n",
    "      date_list.append(dates)\n",
    "\n",
    "    reshaped_data, ann_data, embedding_data, word2vec_data, target, returns, dates = np.concatenate(data_list), np.concatenate(ann_list), np.concatenate(embedding_list), np.concatenate(word2vec_list), np.concatenate(target_list), np.concatenate(returns_list), np.concatenate(date_list)\n",
    "\n",
    "    return (reshaped_data, ann_data, embedding_data, word2vec_data), target, returns, dates\n",
    "\n",
    "  x_train_tuple, y_train, train_returns, train_dates = reshape_data(train_data)\n",
    "  x_test_tuple, y_test, test_returns, test_dates = reshape_data(test_data)\n",
    "\n",
    "  x_train_lstm, x_train_ann, x_train_embedding, x_train_word2vec = x_train_tuple\n",
    "  x_test_lstm, x_test_ann, x_test_embedding, x_test_word2vec = x_test_tuple\n",
    "  \n",
    "  # create model\n",
    "  from keras.layers import Input, Embedding, LSTM, Dense\n",
    "  from keras.models import Model\n",
    "\n",
    "  lstm_params = {\n",
    "      'batch_size': 252,\n",
    "      'epochs': 5,\n",
    "  }\n",
    "\n",
    "  train_len = x_train_lstm.shape[0]\n",
    "  test_len = x_test_lstm.shape[0]\n",
    "\n",
    "  # tweak to match with batch_size\n",
    "  train_until = train_len - train_len % lstm_params['batch_size']\n",
    "  test_until = test_len - test_len % lstm_params['batch_size']\n",
    "\n",
    "  x_train_lstm, x_train_ann, x_train_embedding, x_train_word2vec = x_train_lstm[:train_until], x_train_ann[:train_until], x_train_embedding[:train_until], x_train_word2vec[:train_until]\n",
    "  y_train = y_train[:train_until]\n",
    "  train_returns = train_returns[:train_until]\n",
    "  train_dates = train_dates[:train_until]\n",
    "  x_test_lstm, x_test_ann, x_test_embedding, x_test_word2vec = x_test_lstm[:test_until], x_test_ann[:test_until], x_test_embedding[:test_until], x_test_word2vec[:test_until]\n",
    "  y_test = y_test[:test_until]\n",
    "  test_returns = test_returns[:test_until]\n",
    "  test_dates = test_dates[:test_until]\n",
    "\n",
    "  train_len = x_train_lstm.shape[0]\n",
    "  test_len = x_test_lstm.shape[0]\n",
    "\n",
    "  np.random.seed(1)\n",
    "  train_shuffle = np.random.permutation(range(train_len))\n",
    "  test_shuffle = np.random.permutation(range(test_len))\n",
    "  x_train_lstm, x_train_ann, x_train_embedding, x_train_word2vec, y_train, train_returns, train_dates = x_train_lstm[train_shuffle], x_train_ann[train_shuffle], x_train_embedding[train_shuffle], x_train_word2vec[train_shuffle], y_train[train_shuffle], train_returns[train_shuffle], train_dates[train_shuffle]\n",
    "  x_test_lstm, x_test_ann, x_test_embedding, x_test_word2vec, y_test, test_returns, test_dates = x_test_lstm[test_shuffle], x_test_ann[test_shuffle], x_test_embedding[test_shuffle], x_test_word2vec[test_shuffle], y_test[test_shuffle], test_returns[test_shuffle], test_dates[test_shuffle]\n",
    "\n",
    "  def create_model():\n",
    "    sequence_len = x_train_lstm.shape[1]\n",
    "\n",
    "    lstm_input = Input(shape=(sequence_len, len(lstm_features_list)), name='lstm_input')\n",
    "    lstm_out = LSTM(21)(lstm_input)\n",
    "\n",
    "    additional_input = Input(shape=(len(additional_features_list),), name='additional_input')\n",
    "  #   word2vec_input = Input(shape=(len(word2vec_features_list),), name='word2vec_input')\n",
    "\n",
    "  #   concat_layers = [lstm_out, additional_input, word2vec_input]\n",
    "    concat_layers = [lstm_out, additional_input]\n",
    "\n",
    "  #   input_layers = [lstm_input, additional_input, word2vec_input]\n",
    "    input_layers = [lstm_input, additional_input]\n",
    "\n",
    "    # embeddings\n",
    "    for i in range(x_train_embedding.shape[1]):\n",
    "      vocab_size = x_train_embedding[:, i].max() + 1\n",
    "      y = Input(shape=(1,), dtype='int32', name='embedding_input_%d' % i)\n",
    "      input_layers.append(y)\n",
    "      y = Embedding(input_dim=vocab_size, output_dim=21)(y)\n",
    "      concat_layers.append(Flatten()(y))\n",
    "\n",
    "    x = keras.layers.concatenate(concat_layers)\n",
    "\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dropout(.2)(x)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    x = Dropout(.2)(x)\n",
    "\n",
    "    main_output = Dense(1, activation='sigmoid', name='output')(x)\n",
    "\n",
    "    model = Model(inputs=input_layers, outputs=[main_output])\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "  \n",
    "  def get_sigma(scaled_scores, test_returns, test_dates):\n",
    "    '''\n",
    "    scaled_scores: numpy array\n",
    "    test_returns: numpy array\n",
    "    test_dates: numpy array\n",
    "    '''\n",
    "\n",
    "    grouped_dict = pd.Series(scaled_scores).index.groupby(test_dates)\n",
    "\n",
    "    day_scores = []\n",
    "    for ts in grouped_dict:\n",
    "      grouped_index = grouped_dict[ts]\n",
    "      day_score = np.dot(test_returns[grouped_index], scaled_scores[grouped_index])\n",
    "      day_scores.append(day_score)\n",
    "\n",
    "    return np.mean(day_scores) / np.std(day_scores, ddof=1)\n",
    "\n",
    "\n",
    "  prev_sigma = None\n",
    "  combined_model = create_model()\n",
    "\n",
    "  # train for at most 500 epochs\n",
    "  for i in range(500):\n",
    "    # x_train_list = [x_train_lstm, x_train_ann] + [x_train_word2vec] + [x_train_embedding[:, i] for i in range(x_train_embedding.shape[1])]\n",
    "    x_train_list = [x_train_lstm, x_train_ann] + [x_train_embedding[:, i] for i in range(x_train_embedding.shape[1])]\n",
    "    # x_test_list = [x_test_lstm, x_test_ann] + [x_test_word2vec] + [x_test_embedding[:, i] for i in range(x_test_embedding.shape[1])]\n",
    "    x_test_list = [x_test_lstm, x_test_ann] + [x_test_embedding[:, i] for i in range(x_test_embedding.shape[1])]\n",
    "\n",
    "    combined_model.fit(x_train_list,\n",
    "                   y_train,\n",
    "                   batch_size=lstm_params['batch_size'],\n",
    "                   epochs=1,\n",
    "                   verbose=1,\n",
    "                   validation_data=(x_test_list, y_test),\n",
    "                   shuffle=False)\n",
    "\n",
    "    scaled_scores = combined_model.predict(x_test_list, batch_size=lstm_params['batch_size']).reshape(-1) * 2 - 1\n",
    "    current_sigma = get_sigma(scaled_scores, test_returns, test_dates)\n",
    "    \n",
    "    print('current sigma: %f' % current_sigma)\n",
    "\n",
    "    # early stopping if sigma score does not improve for 1 epoch\n",
    "    if prev_sigma is not None and current_sigma < prev_sigma:\n",
    "      break\n",
    "    else:\n",
    "      prev_sigma = current_sigma\n",
    "  \n",
    "  pred_classes = (scaled_scores > 0).astype(int)\n",
    "  from sklearn.metrics import confusion_matrix\n",
    "  conf_mat = confusion_matrix(y_test, pred_classes)\n",
    "  \n",
    "  print('Year %d: sigma=%f' % (train_end_year, current_sigma))\n",
    "  print(conf_mat)\n",
    "  \n",
    "  if v_str not in results:\n",
    "    results[v_str] = {}\n",
    "  results[v_str][train_end_year] = {}\n",
    "  results[v_str][train_end_year]['y_test'] = y_test\n",
    "  results[v_str][train_end_year]['scaled_scores'] = scaled_scores\n",
    "  results[v_str][train_end_year]['sigma_score'] = current_sigma\n",
    "  results[v_str][train_end_year]['conf_mat'] = conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"sliding_final_output_from_results_dict_V7.p\"\n",
    "fileobj = open(filename, \"wb\")\n",
    "pickle.dump(results, fileobj)\n",
    "fileobj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filename = \"final_output_from_results_dict.p\"\n",
    "fileobj = open(filename, \"wb\")\n",
    "pickle.dump(results, fileobj)\n",
    "fileobj.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_combined_df = pd.read_pickle('./Data/combined.p')\n",
    "\n",
    "###############################################\n",
    "### RESHAPING COMBINED_DF ACCORDING TO TIME ###\n",
    "# month from 0 to 11\n",
    "original_combined_df['month'] = original_combined_df['month'] - 1\n",
    "\n",
    "# missing value imputation\n",
    "\n",
    "group_mean_features = [\n",
    "  'returnsClosePrevMktres1',\n",
    "  'returnsOpenPrevMktres1',\n",
    "  'returnsClosePrevMktres10',\n",
    "  'returnsOpenPrevMktres10',\n",
    "  'returnsOpenPrevMktres1_relative',\n",
    "  'returnsOpenPrevMktres10_relative',\n",
    "]\n",
    "\n",
    "original_combined_df['headline'].fillna('', inplace=True)\n",
    "original_combined_df['urgency_min'].fillna(5.0, inplace=True)\n",
    "\n",
    "original_combined_df[group_mean_features] = original_combined_df.groupby(\"assetCode\")[group_mean_features].transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "original_combined_df = original_combined_df[~original_combined_df['returnsClosePrevMktres10'].isnull()]\n",
    "\n",
    "original_combined_df.fillna(0, inplace=True) ### <--- up to this point to get original df in same state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "industry_df = pd.read_csv('./Data/IndustryData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['assetCode', 'Description', 'Employees', 'Industry']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "industry_df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape_wv(word2vec_features, train_start, train_end, combined_df, industry_df):\n",
    "\n",
    "  combined_df = combined_df.set_index('time')[train_start: train_end]\n",
    "  print(combined_df.shape)\n",
    "  ### RESHAPING COMBINED_DF ACCORDING TO TIME ###\n",
    "  ###############################################\n",
    "\n",
    "  assetCode = combined_df['assetCode'].values\n",
    "\n",
    "  print(\"word2vec:\", len(word2vec_features))\n",
    "\n",
    "  word2vec_features['assetCode'] = assetCode\n",
    "  \n",
    "  word2vec_features = pd.merge(word2vec_features, industry_df, how='inner', on='assetCode')\n",
    "  word2vec_features = word2vec_features[pd.notnull(word2vec_features['Industry'])]\n",
    "  word2vec_features = word2vec_features.drop(['Industry'], axis=1)\n",
    "  \n",
    "  return word2vec_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ibt29EzKs_dk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(413659, 50)\n",
      "word2vec: 413659\n",
      "(398990, 101)\n",
      "00:00:13\n",
      "imhere\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wei Han\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:148: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:00:12\n",
      "Train on 361116 samples, validate on 165816 samples\n",
      "Epoch 1/1\n",
      "361116/361116 [==============================] - 101s 279us/step - loss: 0.6692 - acc: 0.5867 - val_loss: 0.7101 - val_acc: 0.5335\n",
      "current sigma: 0.266762\n",
      "Train on 361116 samples, validate on 165816 samples\n",
      "Epoch 1/1\n",
      "361116/361116 [==============================] - 110s 304us/step - loss: 0.6498 - acc: 0.6148 - val_loss: 0.7174 - val_acc: 0.5301\n",
      "current sigma: 0.272214\n",
      "Train on 361116 samples, validate on 165816 samples\n",
      "Epoch 1/1\n",
      "361116/361116 [==============================] - 92s 254us/step - loss: 0.6349 - acc: 0.6336 - val_loss: 0.7340 - val_acc: 0.5213\n",
      "current sigma: 0.234246\n",
      "Year 2011: sigma=0.234246\n",
      "[[44842 43274]\n",
      " [36107 41593]]\n",
      "(836368, 50)\n",
      "word2vec: 836368\n",
      "(807295, 101)\n",
      "00:00:27\n",
      "imhere\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wei Han\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:148: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:00:13\n",
      "Train on 764820 samples, validate on 168336 samples\n",
      "Epoch 1/1\n",
      "764820/764820 [==============================] - 159s 207us/step - loss: 0.6701 - acc: 0.5891 - val_loss: 0.6825 - val_acc: 0.5697\n",
      "current sigma: 0.847021\n",
      "Train on 764820 samples, validate on 168336 samples\n",
      "Epoch 1/1\n",
      "764820/764820 [==============================] - 171s 224us/step - loss: 0.6547 - acc: 0.6109 - val_loss: 0.6892 - val_acc: 0.5669\n",
      "current sigma: 0.830408\n",
      "Year 2012: sigma=0.830408\n",
      "[[41834 42594]\n",
      " [30306 53602]]\n",
      "(1260731, 50)\n",
      "word2vec: 1260731\n",
      "(1216559, 101)\n",
      "00:00:51\n",
      "imhere\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wei Han\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:148: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:00:13\n",
      "Train on 1170288 samples, validate on 160020 samples\n",
      "Epoch 1/1\n",
      "1170288/1170288 [==============================] - 239s 204us/step - loss: 0.6706 - acc: 0.5856 - val_loss: 0.7017 - val_acc: 0.5442\n",
      "current sigma: 0.419029\n",
      "Train on 1170288 samples, validate on 160020 samples\n",
      "Epoch 1/1\n",
      "1170288/1170288 [==============================] - 243s 208us/step - loss: 0.6576 - acc: 0.6053 - val_loss: 0.7121 - val_acc: 0.5441\n",
      "current sigma: 0.409676\n",
      "Year 2013: sigma=0.409676\n",
      "[[41092 40322]\n",
      " [32625 45981]]\n",
      "(1662485, 50)\n",
      "word2vec: 1662485\n",
      "(1605525, 101)\n",
      "00:01:16\n",
      "imhere\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wei Han\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:148: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:00:13\n",
      "Train on 1555344 samples, validate on 165816 samples\n",
      "Epoch 1/1\n",
      "1555344/1555344 [==============================] - 314s 202us/step - loss: 0.6693 - acc: 0.5874 - val_loss: 0.7165 - val_acc: 0.5203\n",
      "current sigma: 0.064587\n",
      "Train on 1555344 samples, validate on 165816 samples\n",
      "Epoch 1/1\n",
      "1555344/1555344 [==============================] - 321s 206us/step - loss: 0.6565 - acc: 0.6065 - val_loss: 0.7273 - val_acc: 0.5106\n",
      "current sigma: -0.023169\n",
      "Year 2014: sigma=-0.023169\n",
      "[[40949 39742]\n",
      " [41415 43710]]\n",
      "(2095742, 50)\n",
      "word2vec: 2095742\n",
      "(2024360, 101)\n",
      "00:02:03\n",
      "imhere\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wei Han\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:148: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:00:14\n",
      "Train on 1969884 samples, validate on 180432 samples\n",
      "Epoch 1/1\n",
      "1969884/1969884 [==============================] - 398s 202us/step - loss: 0.6726 - acc: 0.5811 - val_loss: 0.6875 - val_acc: 0.5485\n",
      "current sigma: 0.501853\n",
      "Train on 1969884 samples, validate on 180432 samples\n",
      "Epoch 1/1\n",
      "1969884/1969884 [==============================] - 407s 207us/step - loss: 0.6611 - acc: 0.5993 - val_loss: 0.6957 - val_acc: 0.5371\n",
      "current sigma: 0.409455\n",
      "Year 2015: sigma=0.409455\n",
      "[[56806 29135]\n",
      " [54390 40101]]\n",
      "Wall time: 1h 6min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "v_str = 'v3'\n",
    "\n",
    "train_end_years = [2011, 2012, 2013, 2014, 2015]\n",
    "\n",
    "lstm_features_list = [\n",
    "    'returnsClosePrevRaw1',\n",
    "    'returnsOpenPrevRaw1',\n",
    "    'returnsClosePrevMktres1',\n",
    "    'returnsOpenPrevMktres1',\n",
    "    'returnsClosePrevRaw10',\n",
    "    'returnsOpenPrevRaw10',\n",
    "    'returnsClosePrevMktres10',\n",
    "    'returnsOpenPrevMktres10',\n",
    "    'open_close',\n",
    "    'oc_average',\n",
    "    'turnover',\n",
    "    'open_close_relative',\n",
    "    'turnover_relative',\n",
    "    'volume_relative',\n",
    "    'returnsOpenPrevMktres1_relative',\n",
    "    'returnsOpenPrevMktres10_relative',\n",
    "]\n",
    "\n",
    "\n",
    "additional_features_list = [\n",
    "    'urgency_min',\n",
    "    'urgency_count',\n",
    "    'takeSequence_min',\n",
    "    'takeSequence_max',\n",
    "    'bodySize_mean',\n",
    "    'wordCount_mean',\n",
    "    'sentenceCount_mean',\n",
    "    'companyCount_mean',\n",
    "    'marketCommentary_mean',\n",
    "    'relevance_mean',\n",
    "    'sentimentNegative_mean',\n",
    "    'sentimentNeutral_mean',\n",
    "    'sentimentPositive_mean',\n",
    "    'sentimentWordCount_mean',\n",
    "    'noveltyCount12H_mean',\n",
    "    'noveltyCount24H_mean',\n",
    "    'noveltyCount3D_mean',\n",
    "    'noveltyCount5D_mean',\n",
    "    'noveltyCount7D_mean',\n",
    "    'volumeCounts12H_mean',\n",
    "    'volumeCounts24H_mean',\n",
    "    'volumeCounts3D_mean',\n",
    "    'volumeCounts5D_mean',\n",
    "    'volumeCounts7D_mean',\n",
    "]\n",
    "\n",
    "label_encoded_features = [\n",
    "    'dayofweek',\n",
    "    'month',\n",
    "]\n",
    "\n",
    "continuous_features_list = lstm_features_list + additional_features_list\n",
    "features_list = lstm_features_list + additional_features_list + label_encoded_features\n",
    "\n",
    "for train_end_year in train_end_years:\n",
    "  # a trick to get testing stats\n",
    "  if train_end_year == 2016:\n",
    "    train_end_year = 2015\n",
    "    train_start, train_end = pd.to_datetime('2011-01-01'), pd.to_datetime('2016-06-30')\n",
    "    test_start, test_end = pd.to_datetime('%d-07-01' % (train_end_year + 1)), pd.to_datetime('%d-12-31' % (train_end_year + 1))\n",
    "    train_end_year = 2016\n",
    "  else:\n",
    "    train_start, train_end = pd.to_datetime('2011-01-01'), pd.to_datetime('%d-12-31' % train_end_year)\n",
    "    test_start, test_end = pd.to_datetime('%d-01-01' % (train_end_year + 1)), pd.to_datetime('%d-06-30' % (train_end_year + 1))\n",
    "  \n",
    "  \n",
    "  # divide into train and test periods\n",
    "\n",
    "  train_data = combined_data.set_index('time')[train_start: train_end]\n",
    "  test_data = combined_data.set_index('time')[test_start: test_end]\n",
    "\n",
    "  from sklearn.preprocessing import StandardScaler\n",
    "  scaler = StandardScaler()\n",
    "  train_data[continuous_features_list] = scaler.fit_transform(train_data[continuous_features_list])\n",
    "  test_data[continuous_features_list] = scaler.transform(test_data[continuous_features_list])\n",
    "  \n",
    "  # get word2vec features\n",
    "  def combine_with_word2vec_features(train_df, word2vec_features_filepath): #train_df is the training df, word2vec_features is the word2vec_features filepath(load data from /content/drive/My Drive/BT4222 Group/workspace/FILENAME)\n",
    "    start_time = time.time()\n",
    "\n",
    "    word2vec_features = pd.read_csv(word2vec_features_filepath,header=None) #read the file\n",
    "    header_list = []\n",
    "    for i in range(100):\n",
    "        header_list.append(\"word2vec\" + str(i+1))\n",
    "    word2vec_features.columns = header_list #set column names\n",
    "    word2vec_features[\"all\"] = word2vec_features.values.tolist() #create column with all the values combined as list\n",
    "    \n",
    "    word2vec_features = shape_wv(word2vec_features, train_start, train_end, original_combined_df, industry_df)\n",
    "    \n",
    "    train_df = train_df.reset_index() #reset indexes for both train and word2vec feature df\n",
    "    word2vec_features = word2vec_features.reset_index()\n",
    "\n",
    "    train_df[\"word2vec_features\"] = word2vec_features[\"all\"] #put the combined list values into train df\n",
    "    train_df = train_df.set_index('time') #set index as time again\n",
    "    \n",
    "    print(train_df.shape)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\n",
    "\n",
    "    return train_df\n",
    "\n",
    "  start, end = 2011, min(train_end_year, 2015)\n",
    "  word2vec_features_path = \"./Data/new_features_full_%d_%d_%dyr_word2vec.csv\" % (start, end, end - start + 1)\n",
    "  word2vec_model_path = \"./Data/new_full_%d_%d_%dyr_word2vec.wv\" % (start, end, end - start + 1)\n",
    "  train_data = combine_with_word2vec_features(train_data, word2vec_features_path)\n",
    "  print(\"imhere\")\n",
    "\n",
    "  word2vec_df = pd.DataFrame(train_data['word2vec_features'].tolist())\n",
    "  word2vec_df.index = train_data.index\n",
    "  word2vec_df.columns = ['word2vec_%d' % i for i in range(word2vec_df.shape[1])]\n",
    "\n",
    "  train_data = pd.concat([train_data.iloc[:, :-1], word2vec_df], axis=1)\n",
    "\n",
    "  # get test data word vectors\n",
    "  stop_words = stopwords.words('english')\n",
    "\n",
    "  test_data['headline'] = test_data['headline'].fillna(\"\")\n",
    "  test_data_words = []\n",
    "\n",
    "  for headline in test_data['headline']:\n",
    "    words = [word.lower() for word in word_tokenize(headline) if word.isalpha()]\n",
    "    words = [word for word in words if not word in stop_words]\n",
    "    test_data_words.append(words)\n",
    "\n",
    "\n",
    "  # get features\n",
    "  def get_features_from_word2vec(word2vec, headline_list, feature_size):\n",
    "    start_time = time.time()\n",
    "\n",
    "    features = [] # Each feature is a 1x100 vector\n",
    "    word_not_in_vocabulary = []\n",
    "\n",
    "    for words in headline_list:\n",
    "        feature = np.zeros(feature_size)\n",
    "        for word in words:\n",
    "            try:\n",
    "              feature += word2vec.wv.get_vector(word)\n",
    "            except:\n",
    "              continue\n",
    "              #word_not_in_vocabulary.append(word)\n",
    "        features.append((feature / float(len(words))).tolist())\n",
    "\n",
    "    no_nans_feature_list = list(filter(lambda x: np.isnan(x).any() == False, features))\n",
    "    transposed = zip(*no_nans_feature_list)\n",
    "    avg = lambda items: float(sum(items))/ len(items)\n",
    "    average_vector = list(map(avg, transposed)) #this average vector will be used to replace all nan vectors\n",
    "\n",
    "    features = np.array(list(map(lambda x: x if np.isnan(x).any() == False else average_vector, features)), dtype='float16').tolist()\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\n",
    "    return features\n",
    "\n",
    "  word2vec_model = Word2Vec.load(word2vec_model_path)\n",
    "  test_word2vec_features = get_features_from_word2vec(word2vec_model, test_data_words, 100)\n",
    "#   word2vec_df_test = pd.DataFrame(test_word2vec_features)\n",
    "#   word2vec_df_test.columns = ['word2vec_%d' % i for i in range(word2vec_df_test.shape[1])]\n",
    "#   word2vec_df_test.index = test_data.index\n",
    "#   test_data = pd.concat((test_data, word2vec_df_test), axis=1)\n",
    "\n",
    "  word2vec_df_test = pd.DataFrame(test_word2vec_features)\n",
    "  word2vec_df_test.columns = ['word2vec_%d' % i for i in range(word2vec_df_test.shape[1])]\n",
    "  word2vec_df_test.index = test_data.index\n",
    "  test_data = pd.concat((test_data, word2vec_df_test), axis=1)\n",
    "\n",
    "  # get word2vec feature names\n",
    "  word2vec_features_list = []\n",
    "  for col in train_data.columns:\n",
    "    if 'word2vec' in col:\n",
    "      word2vec_features_list.append(col)\n",
    "    \n",
    "  #### insert wrapper code ####  \n",
    "    \n",
    "  \n",
    "  # reshape data\n",
    "  def reshape_data(data, sequence_len=21, features_list=features_list):\n",
    "    data_list, ann_list, embedding_list, word2vec_list, target_list, returns_list, date_list = [], [], [], [], [], [], []\n",
    "\n",
    "    for assetCode, asset_data in data.groupby('assetCode'):\n",
    "      # binarize the target variable\n",
    "      returns = asset_data['returnsOpenNextMktres10']\n",
    "      target = (asset_data['returnsOpenNextMktres10'] > 0).astype(int)\n",
    "      dates = asset_data.index\n",
    "\n",
    "      # arrays\n",
    "      lstm_data = asset_data[lstm_features_list].values\n",
    "      ann_data = asset_data[additional_features_list].values\n",
    "      embedding_data = asset_data[label_encoded_features].values\n",
    "      word2vec_data = asset_data[word2vec_features_list].values\n",
    "\n",
    "      col_mean = np.nanmean(lstm_data, axis = 0) \n",
    "      inds = np.where(np.isnan(lstm_data)) \n",
    "      lstm_data[inds] = np.take(col_mean, inds[1])\n",
    "\n",
    "      reshaped_data = np.repeat(lstm_data[:, np.newaxis, :], sequence_len, axis=1)\n",
    "      for i in range(reshaped_data.shape[1]):\n",
    "        reshaped_data[:, i, :] = np.roll(reshaped_data[:, i, :], i, axis=0)\n",
    "\n",
    "      # discard the top rows and reverse the order of the shifted sequence to reflect the correct order\n",
    "      reshaped_data = reshaped_data[sequence_len-1:, ::-1, :]\n",
    "      ann_data = ann_data[sequence_len-1:, :]\n",
    "      embedding_data = embedding_data[sequence_len-1:, :]\n",
    "      word2vec_data = word2vec_data[sequence_len-1:, :]\n",
    "      target = target.values[sequence_len-1:]\n",
    "      returns = returns.values[sequence_len-1:]\n",
    "      dates = dates[sequence_len-1:]\n",
    "\n",
    "      data_list.append(reshaped_data)\n",
    "      ann_list.append(ann_data)\n",
    "      embedding_list.append(embedding_data)\n",
    "      word2vec_list.append(word2vec_data)\n",
    "      target_list.append(target)\n",
    "      returns_list.append(returns)\n",
    "      date_list.append(dates)\n",
    "\n",
    "    reshaped_data, ann_data, embedding_data, word2vec_data, target, returns, dates = np.concatenate(data_list), np.concatenate(ann_list), np.concatenate(embedding_list), np.concatenate(word2vec_list), np.concatenate(target_list), np.concatenate(returns_list), np.concatenate(date_list)\n",
    "\n",
    "    return (reshaped_data, ann_data, embedding_data, word2vec_data), target, returns, dates\n",
    "\n",
    "  x_train_tuple, y_train, train_returns, train_dates = reshape_data(train_data)\n",
    "  x_test_tuple, y_test, test_returns, test_dates = reshape_data(test_data)\n",
    "\n",
    "  x_train_lstm, x_train_ann, x_train_embedding, x_train_word2vec = x_train_tuple\n",
    "  x_test_lstm, x_test_ann, x_test_embedding, x_test_word2vec = x_test_tuple\n",
    "  \n",
    "  # create model\n",
    "  from keras.layers import Input, Embedding, LSTM, Dense\n",
    "  from keras.models import Model\n",
    "\n",
    "  lstm_params = {\n",
    "      'batch_size': 252,\n",
    "      'epochs': 5,\n",
    "  }\n",
    "\n",
    "  train_len = x_train_lstm.shape[0]\n",
    "  test_len = x_test_lstm.shape[0]\n",
    "\n",
    "  # tweak to match with batch_size\n",
    "  train_until = train_len - train_len % lstm_params['batch_size']\n",
    "  test_until = test_len - test_len % lstm_params['batch_size']\n",
    "\n",
    "  x_train_lstm, x_train_ann, x_train_embedding, x_train_word2vec = x_train_lstm[:train_until], x_train_ann[:train_until], x_train_embedding[:train_until], x_train_word2vec[:train_until]\n",
    "  y_train = y_train[:train_until]\n",
    "  train_returns = train_returns[:train_until]\n",
    "  train_dates = train_dates[:train_until]\n",
    "  x_test_lstm, x_test_ann, x_test_embedding, x_test_word2vec = x_test_lstm[:test_until], x_test_ann[:test_until], x_test_embedding[:test_until], x_test_word2vec[:test_until]\n",
    "  y_test = y_test[:test_until]\n",
    "  test_returns = test_returns[:test_until]\n",
    "  test_dates = test_dates[:test_until]\n",
    "\n",
    "  train_len = x_train_lstm.shape[0]\n",
    "  test_len = x_test_lstm.shape[0]\n",
    "\n",
    "  np.random.seed(1)\n",
    "  train_shuffle = np.random.permutation(range(train_len))\n",
    "  test_shuffle = np.random.permutation(range(test_len))\n",
    "  x_train_lstm, x_train_ann, x_train_embedding, x_train_word2vec, y_train, train_returns, train_dates = x_train_lstm[train_shuffle], x_train_ann[train_shuffle], x_train_embedding[train_shuffle], x_train_word2vec[train_shuffle], y_train[train_shuffle], train_returns[train_shuffle], train_dates[train_shuffle]\n",
    "  x_test_lstm, x_test_ann, x_test_embedding, x_test_word2vec, y_test, test_returns, test_dates = x_test_lstm[test_shuffle], x_test_ann[test_shuffle], x_test_embedding[test_shuffle], x_test_word2vec[test_shuffle], y_test[test_shuffle], test_returns[test_shuffle], test_dates[test_shuffle]\n",
    "\n",
    "  def create_model():\n",
    "    sequence_len = x_train_lstm.shape[1]\n",
    "\n",
    "    lstm_input = Input(shape=(sequence_len, len(lstm_features_list)), name='lstm_input')\n",
    "    lstm_out = LSTM(21)(lstm_input)\n",
    "\n",
    "    additional_input = Input(shape=(len(additional_features_list),), name='additional_input')\n",
    "  #   word2vec_input = Input(shape=(len(word2vec_features_list),), name='word2vec_input')\n",
    "\n",
    "  #   concat_layers = [lstm_out, additional_input, word2vec_input]\n",
    "    concat_layers = [lstm_out, additional_input]\n",
    "\n",
    "  #   input_layers = [lstm_input, additional_input, word2vec_input]\n",
    "    input_layers = [lstm_input, additional_input]\n",
    "\n",
    "    # embeddings\n",
    "    for i in range(x_train_embedding.shape[1]):\n",
    "      vocab_size = x_train_embedding[:, i].max() + 1\n",
    "      y = Input(shape=(1,), dtype='int32', name='embedding_input_%d' % i)\n",
    "      input_layers.append(y)\n",
    "      y = Embedding(input_dim=vocab_size, output_dim=21)(y)\n",
    "      concat_layers.append(Flatten()(y))\n",
    "\n",
    "    x = keras.layers.concatenate(concat_layers)\n",
    "\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "\n",
    "    main_output = Dense(1, activation='sigmoid', name='output')(x)\n",
    "\n",
    "    model = Model(inputs=input_layers, outputs=[main_output])\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "  \n",
    "  def get_sigma(scaled_scores, test_returns, test_dates):\n",
    "    '''\n",
    "    scaled_scores: numpy array\n",
    "    test_returns: numpy array\n",
    "    test_dates: numpy array\n",
    "    '''\n",
    "\n",
    "    grouped_dict = pd.Series(scaled_scores).index.groupby(test_dates)\n",
    "\n",
    "    day_scores = []\n",
    "    for ts in grouped_dict:\n",
    "      grouped_index = grouped_dict[ts]\n",
    "      day_score = np.dot(test_returns[grouped_index], scaled_scores[grouped_index])\n",
    "      day_scores.append(day_score)\n",
    "\n",
    "    return np.mean(day_scores) / np.std(day_scores, ddof=1)\n",
    "\n",
    "\n",
    "  prev_sigma = None\n",
    "  combined_model = create_model()\n",
    "\n",
    "  # train for at most 500 epochs\n",
    "  for i in range(500):\n",
    "    # x_train_list = [x_train_lstm, x_train_ann] + [x_train_word2vec] + [x_train_embedding[:, i] for i in range(x_train_embedding.shape[1])]\n",
    "    x_train_list = [x_train_lstm, x_train_ann] + [x_train_embedding[:, i] for i in range(x_train_embedding.shape[1])]\n",
    "    # x_test_list = [x_test_lstm, x_test_ann] + [x_test_word2vec] + [x_test_embedding[:, i] for i in range(x_test_embedding.shape[1])]\n",
    "    x_test_list = [x_test_lstm, x_test_ann] + [x_test_embedding[:, i] for i in range(x_test_embedding.shape[1])]\n",
    "\n",
    "    combined_model.fit(x_train_list,\n",
    "                   y_train,\n",
    "                   batch_size=lstm_params['batch_size'],\n",
    "                   epochs=1,\n",
    "                   verbose=1,\n",
    "                   validation_data=(x_test_list, y_test),\n",
    "                   shuffle=False)\n",
    "\n",
    "    scaled_scores = combined_model.predict(x_test_list, batch_size=lstm_params['batch_size']).reshape(-1) * 2 - 1\n",
    "    current_sigma = get_sigma(scaled_scores, test_returns, test_dates)\n",
    "    \n",
    "    print('current sigma: %f' % current_sigma)\n",
    "\n",
    "    # early stopping if sigma score does not improve for 1 epoch\n",
    "    if prev_sigma is not None and current_sigma < prev_sigma:\n",
    "      break\n",
    "    else:\n",
    "      prev_sigma = current_sigma\n",
    "  \n",
    "  pred_classes = (scaled_scores > 0).astype(int)\n",
    "  from sklearn.metrics import confusion_matrix\n",
    "  conf_mat = confusion_matrix(y_test, pred_classes)\n",
    "  \n",
    "  print('Year %d: sigma=%f' % (train_end_year, current_sigma))\n",
    "  print(conf_mat)\n",
    "  \n",
    "  if v_str not in results:\n",
    "    results[v_str] = {}\n",
    "  results[v_str][train_end_year] = {}\n",
    "  results[v_str][train_end_year]['y_test'] = y_test\n",
    "  results[v_str][train_end_year]['scaled_scores'] = scaled_scores\n",
    "  results[v_str][train_end_year]['sigma_score'] = current_sigma\n",
    "  results[v_str][train_end_year]['conf_mat'] = conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"growing_final_output_from_results_dict_V3.p\"\n",
    "fileobj = open(filename, \"wb\")\n",
    "pickle.dump(results, fileobj)\n",
    "fileobj.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_combined_df = pd.read_pickle('./Data/combined.p')\n",
    "\n",
    "###############################################\n",
    "### RESHAPING COMBINED_DF ACCORDING TO TIME ###\n",
    "# month from 0 to 11\n",
    "original_combined_df['month'] = original_combined_df['month'] - 1\n",
    "\n",
    "# missing value imputation\n",
    "\n",
    "group_mean_features = [\n",
    "  'returnsClosePrevMktres1',\n",
    "  'returnsOpenPrevMktres1',\n",
    "  'returnsClosePrevMktres10',\n",
    "  'returnsOpenPrevMktres10',\n",
    "  'returnsOpenPrevMktres1_relative',\n",
    "  'returnsOpenPrevMktres10_relative',\n",
    "]\n",
    "\n",
    "original_combined_df['headline'].fillna('', inplace=True)\n",
    "original_combined_df['urgency_min'].fillna(5.0, inplace=True)\n",
    "\n",
    "original_combined_df[group_mean_features] = original_combined_df.groupby(\"assetCode\")[group_mean_features].transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "original_combined_df = original_combined_df[~original_combined_df['returnsClosePrevMktres10'].isnull()]\n",
    "\n",
    "original_combined_df.fillna(0, inplace=True) ### <--- up to this point to get original df in same state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "industry_df = pd.read_csv('./Data/IndustryData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape_wv(word2vec_features, train_start, train_end, combined_df, industry_df):\n",
    "\n",
    "  combined_df = combined_df.set_index('time')[train_start: train_end]\n",
    "  print(combined_df.shape)\n",
    "  ### RESHAPING COMBINED_DF ACCORDING TO TIME ###\n",
    "  ###############################################\n",
    "\n",
    "  assetCode = combined_df['assetCode'].values\n",
    "\n",
    "  print(\"word2vec:\", len(word2vec_features))\n",
    "\n",
    "  word2vec_features['assetCode'] = assetCode\n",
    "  \n",
    "  word2vec_features = pd.merge(word2vec_features, industry_df, how='inner', on='assetCode')\n",
    "  word2vec_features = word2vec_features[pd.notnull(word2vec_features['Industry'])]\n",
    "  word2vec_features = word2vec_features.drop(['Industry'], axis=1)\n",
    "  \n",
    "  return word2vec_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(413659, 50)\n",
      "word2vec: 413659\n",
      "(398990, 101)\n",
      "00:00:13\n",
      "imhere\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wei Han\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:150: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:00:13\n",
      "file_counter: 2\n",
      "(836368, 50)\n",
      "word2vec: 836368\n",
      "(807295, 101)\n",
      "00:00:26\n",
      "imhere\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wei Han\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:150: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:00:12\n",
      "file_counter: 3\n",
      "(1260731, 50)\n",
      "word2vec: 1260731\n",
      "(1216559, 101)\n",
      "00:00:41\n",
      "imhere\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wei Han\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:150: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:00:13\n",
      "file_counter: 4\n",
      "(1662485, 50)\n",
      "word2vec: 1662485\n",
      "(1605525, 101)\n",
      "00:01:00\n",
      "imhere\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wei Han\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:150: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:00:13\n",
      "file_counter: 5\n",
      "(2095742, 50)\n",
      "word2vec: 2095742\n",
      "(2024360, 101)\n",
      "00:01:35\n",
      "imhere\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Wei Han\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:150: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:00:14\n",
      "file_counter: 6\n",
      "Wall time: 43min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "v_str = 'v3'\n",
    "\n",
    "file_counter = 1\n",
    "\n",
    "train_end_years = [2011, 2012, 2013, 2014, 2015]\n",
    "\n",
    "lstm_features_list = [\n",
    "    'returnsClosePrevRaw1',\n",
    "    'returnsOpenPrevRaw1',\n",
    "    'returnsClosePrevMktres1',\n",
    "    'returnsOpenPrevMktres1',\n",
    "    'returnsClosePrevRaw10',\n",
    "    'returnsOpenPrevRaw10',\n",
    "    'returnsClosePrevMktres10',\n",
    "    'returnsOpenPrevMktres10',\n",
    "    'open_close',\n",
    "    'oc_average',\n",
    "    'turnover',\n",
    "    'open_close_relative',\n",
    "    'turnover_relative',\n",
    "    'volume_relative',\n",
    "    'returnsOpenPrevMktres1_relative',\n",
    "    'returnsOpenPrevMktres10_relative',\n",
    "]\n",
    "\n",
    "\n",
    "additional_features_list = [\n",
    "    'urgency_min',\n",
    "    'urgency_count',\n",
    "    'takeSequence_min',\n",
    "    'takeSequence_max',\n",
    "    'bodySize_mean',\n",
    "    'wordCount_mean',\n",
    "    'sentenceCount_mean',\n",
    "    'companyCount_mean',\n",
    "    'marketCommentary_mean',\n",
    "    'relevance_mean',\n",
    "    'sentimentNegative_mean',\n",
    "    'sentimentNeutral_mean',\n",
    "    'sentimentPositive_mean',\n",
    "    'sentimentWordCount_mean',\n",
    "    'noveltyCount12H_mean',\n",
    "    'noveltyCount24H_mean',\n",
    "    'noveltyCount3D_mean',\n",
    "    'noveltyCount5D_mean',\n",
    "    'noveltyCount7D_mean',\n",
    "    'volumeCounts12H_mean',\n",
    "    'volumeCounts24H_mean',\n",
    "    'volumeCounts3D_mean',\n",
    "    'volumeCounts5D_mean',\n",
    "    'volumeCounts7D_mean',\n",
    "]\n",
    "\n",
    "label_encoded_features = [\n",
    "    'dayofweek',\n",
    "    'month',\n",
    "]\n",
    "\n",
    "continuous_features_list = lstm_features_list + additional_features_list\n",
    "features_list = lstm_features_list + additional_features_list + label_encoded_features\n",
    "\n",
    "for train_end_year in train_end_years:\n",
    "  # a trick to get testing stats\n",
    "  if train_end_year == 2016:\n",
    "    train_end_year = 2015\n",
    "    train_start, train_end = pd.to_datetime('2011-01-01'), pd.to_datetime('2016-06-30')\n",
    "    test_start, test_end = pd.to_datetime('%d-07-01' % (train_end_year + 1)), pd.to_datetime('%d-12-31' % (train_end_year + 1))\n",
    "    train_end_year = 2016\n",
    "  else:\n",
    "    train_start, train_end = pd.to_datetime('2011-01-01'), pd.to_datetime('%d-12-31' % train_end_year)\n",
    "    test_start, test_end = pd.to_datetime('%d-01-01' % (train_end_year + 1)), pd.to_datetime('%d-06-30' % (train_end_year + 1))\n",
    "  \n",
    "  \n",
    "  # divide into train and test periods\n",
    "\n",
    "  train_data = combined_data.set_index('time')[train_start: train_end]\n",
    "  test_data = combined_data.set_index('time')[test_start: test_end]\n",
    "\n",
    "  from sklearn.preprocessing import StandardScaler\n",
    "  scaler = StandardScaler()\n",
    "  train_data[continuous_features_list] = scaler.fit_transform(train_data[continuous_features_list])\n",
    "  test_data[continuous_features_list] = scaler.transform(test_data[continuous_features_list])\n",
    "  \n",
    "  # get word2vec features\n",
    "  def combine_with_word2vec_features(train_df, word2vec_features_filepath): #train_df is the training df, word2vec_features is the word2vec_features filepath(load data from /content/drive/My Drive/BT4222 Group/workspace/FILENAME)\n",
    "    start_time = time.time()\n",
    "\n",
    "    word2vec_features = pd.read_csv(word2vec_features_filepath,header=None) #read the file\n",
    "    header_list = []\n",
    "    for i in range(100):\n",
    "        header_list.append(\"word2vec\" + str(i+1))\n",
    "    word2vec_features.columns = header_list #set column names\n",
    "    word2vec_features[\"all\"] = word2vec_features.values.tolist() #create column with all the values combined as list\n",
    "    \n",
    "    word2vec_features = shape_wv(word2vec_features, train_start, train_end, original_combined_df, industry_df)\n",
    "    \n",
    "    train_df = train_df.reset_index() #reset indexes for both train and word2vec feature df\n",
    "    word2vec_features = word2vec_features.reset_index()\n",
    "\n",
    "    train_df[\"word2vec_features\"] = word2vec_features[\"all\"] #put the combined list values into train df\n",
    "    train_df = train_df.set_index('time') #set index as time again\n",
    "    \n",
    "    print(train_df.shape)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\n",
    "\n",
    "    return train_df\n",
    "\n",
    "  start, end = 2011, min(train_end_year, 2015)\n",
    "  word2vec_features_path = \"./Data/new_features_full_%d_%d_%dyr_word2vec.csv\" % (start, end, end - start + 1)\n",
    "  word2vec_model_path = \"./Data/new_full_%d_%d_%dyr_word2vec.wv\" % (start, end, end - start + 1)\n",
    "  train_data = combine_with_word2vec_features(train_data, word2vec_features_path)\n",
    "  print(\"imhere\")\n",
    "\n",
    "  word2vec_df = pd.DataFrame(train_data['word2vec_features'].tolist())\n",
    "  word2vec_df.index = train_data.index\n",
    "  word2vec_df.columns = ['word2vec_%d' % i for i in range(word2vec_df.shape[1])]\n",
    "\n",
    "  train_data = pd.concat([train_data.iloc[:, :-1], word2vec_df], axis=1)\n",
    "\n",
    "  # get test data word vectors\n",
    "  stop_words = stopwords.words('english')\n",
    "\n",
    "  test_data['headline'] = test_data['headline'].fillna(\"\")\n",
    "  test_data_words = []\n",
    "\n",
    "  for headline in test_data['headline']:\n",
    "    words = [word.lower() for word in word_tokenize(headline) if word.isalpha()]\n",
    "    words = [word for word in words if not word in stop_words]\n",
    "    test_data_words.append(words)\n",
    "\n",
    "\n",
    "  # get features\n",
    "  def get_features_from_word2vec(word2vec, headline_list, feature_size):\n",
    "    start_time = time.time()\n",
    "\n",
    "    features = [] # Each feature is a 1x100 vector\n",
    "    word_not_in_vocabulary = []\n",
    "\n",
    "    for words in headline_list:\n",
    "        feature = np.zeros(feature_size)\n",
    "        for word in words:\n",
    "            try:\n",
    "              feature += word2vec.wv.get_vector(word)\n",
    "            except:\n",
    "              continue\n",
    "              #word_not_in_vocabulary.append(word)\n",
    "        features.append((feature / float(len(words))).tolist())\n",
    "\n",
    "    no_nans_feature_list = list(filter(lambda x: np.isnan(x).any() == False, features))\n",
    "    transposed = zip(*no_nans_feature_list)\n",
    "    avg = lambda items: float(sum(items))/ len(items)\n",
    "    average_vector = list(map(avg, transposed)) #this average vector will be used to replace all nan vectors\n",
    "\n",
    "    features = np.array(list(map(lambda x: x if np.isnan(x).any() == False else average_vector, features)), dtype='float16').tolist()\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\n",
    "    return features\n",
    "\n",
    "  word2vec_model = Word2Vec.load(word2vec_model_path)\n",
    "  test_word2vec_features = get_features_from_word2vec(word2vec_model, test_data_words, 100)\n",
    "#   word2vec_df_test = pd.DataFrame(test_word2vec_features)\n",
    "#   word2vec_df_test.columns = ['word2vec_%d' % i for i in range(word2vec_df_test.shape[1])]\n",
    "#   word2vec_df_test.index = test_data.index\n",
    "#   test_data = pd.concat((test_data, word2vec_df_test), axis=1)\n",
    "\n",
    "  word2vec_df_test = pd.DataFrame(test_word2vec_features)\n",
    "  word2vec_df_test.columns = ['word2vec_%d' % i for i in range(word2vec_df_test.shape[1])]\n",
    "  word2vec_df_test.index = test_data.index\n",
    "  test_data = pd.concat((test_data, word2vec_df_test), axis=1)\n",
    "\n",
    "  # get word2vec feature names\n",
    "  word2vec_features_list = []\n",
    "  for col in train_data.columns:\n",
    "    if 'word2vec' in col:\n",
    "      word2vec_features_list.append(col)\n",
    "        \n",
    "  #### insert wrapper code ####  \n",
    "  \n",
    "  # df has multiple columns of wv, has assetCode\n",
    "  def mean_wv_by_assetCode(df, wv_names):\n",
    "\n",
    "    # group according to asset code\n",
    "    grouped = df.groupby('assetCode')\n",
    "\n",
    "    # then find the mean of the word vectors # this forms a dataframe with all the mean_word_vectors\n",
    "    mean_wv_df = grouped[wv_names].transform('mean')\n",
    "\n",
    "    # concatenate the dataframe with the asset code and industry so that the mean are identifiable\n",
    "    # mean_wv_df = pd.concat([df[['assetCode', 'Industry']], mean_wv_df], axis = 1)\n",
    "    mean_wv_df['assetCode'] = df['assetCode'].tolist()\n",
    "\n",
    "    # remove duplicated rows, there would be many\n",
    "    mean_wv_df = mean_wv_df.drop_duplicates()\n",
    "    \n",
    "    return mean_wv_df\n",
    "\n",
    "\n",
    "  def best_k_clustering(X):\n",
    "  \n",
    "    #######\n",
    "    # SVD #\n",
    "    #######\n",
    "\n",
    "    svd_transformer = TruncatedSVD(n_components = 8)\n",
    "    X_embedded = svd_transformer.fit_transform(X)\n",
    "\n",
    "    # %%time\n",
    "\n",
    "    models = {}\n",
    "\n",
    "    cluster_range = range(2, 21)\n",
    "\n",
    "\n",
    "    for k in cluster_range:\n",
    "        kmeans_model = KMeans(n_clusters=k)\n",
    "        kmeans_model.fit_transform(X_embedded)\n",
    "        models[k] = kmeans_model\n",
    "\n",
    "    # %%time\n",
    "\n",
    "    scores_list = []\n",
    "\n",
    "    for k in cluster_range:\n",
    "        model = models[k]\n",
    "        score = silhouette_score(X_embedded, model.labels_)\n",
    "        scores_list.append(score)\n",
    "\n",
    "    best_model = models[scores_list.index(max(scores_list)) + 2]\n",
    "    cluster_labels = best_model.labels_\n",
    "\n",
    "    return cluster_labels, (best_model, svd_transformer)\n",
    "\n",
    "\n",
    "  def generate_columns_features(df, columns):\n",
    "\n",
    "    grouped = df.groupby(columns)\n",
    "\n",
    "    score = lambda x: (x - x.mean()) / x.std()\n",
    "    standard_df = grouped.transform(score)\n",
    "    standard_df = standard_df.add_suffix('_stan')\n",
    "\n",
    "    df = pd.concat([df, standard_df], axis = 1)\n",
    "    return df\n",
    "\n",
    "\n",
    "  def get_v3_train_test(train_word2vec, test_word2vec, wv_names):\n",
    "    \"\"\"\n",
    "      train_word2vec and test_word2vec contain original data combined with added\n",
    "      word2vec data for train and extracted word2vec data for test (validation)\n",
    "    \"\"\"\n",
    "    global mean_wv_df, best_model, train_cluster_neut, test_cluster_neut\n",
    "    mean_wv_df = mean_wv_by_assetCode(train_word2vec, wv_names)\n",
    "    _, (best_model, svd_transformer) = best_k_clustering(mean_wv_df[wv_names])\n",
    "    train_word2vec['Cluster'] = best_model.predict(svd_transformer.transform(train_data[wv_names]))\n",
    "    test_word2vec['Cluster'] = best_model.predict(svd_transformer.transform(test_data[wv_names]))\n",
    "\n",
    "    columns = ['time', 'Cluster']\n",
    "    train_cluster_neut = generate_columns_features(train_word2vec, columns)\n",
    "    test_cluster_neut = generate_columns_features(test_word2vec, columns)\n",
    "\n",
    "    return train_cluster_neut, test_cluster_neut\n",
    "  \n",
    "  train_cluster_standardised, test_cluster_standardised = get_v3_train_test(train_data, test_data, word2vec_features_list)\n",
    "  \n",
    "  train_cluster_standardised_filename = 'train_cluster_standardised_' + str(file_counter) + \".p\"\n",
    "  train_fileobj = open(train_cluster_standardised_filename, \"wb\")\n",
    "  pickle.dump(train_cluster_standardised, train_fileobj)\n",
    "  train_fileobj.close()\n",
    "  \n",
    "  test_cluster_standardised_filename = 'test_cluster_standardised_' + str(file_counter) + \".p\"\n",
    "  test_fileobj = open(test_cluster_standardised_filename, \"wb\")\n",
    "  pickle.dump(test_cluster_standardised, test_fileobj)\n",
    "  test_fileobj.close()\n",
    "  \n",
    "  file_counter += 1\n",
    "  print(\"file_counter:\", file_counter)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_data shape: (398990, 148)\n",
      "Test_data shape: (201982, 148)\n",
      "Train on 361116 samples, validate on 165816 samples\n",
      "Epoch 1/1\n",
      "361116/361116 [==============================] - 72s 200us/step - loss: 0.6933 - acc: 0.5005 - val_loss: 0.6931 - val_acc: 0.5314\n",
      "current sigma: 0.365377\n",
      "Train on 361116 samples, validate on 165816 samples\n",
      "Epoch 1/1\n",
      "361116/361116 [==============================] - 81s 224us/step - loss: 0.6932 - acc: 0.5008 - val_loss: 0.6931 - val_acc: 0.5314\n",
      "current sigma: 0.365377\n",
      "Train on 361116 samples, validate on 165816 samples\n",
      "Epoch 1/1\n",
      "361116/361116 [==============================] - 86s 239us/step - loss: 0.6932 - acc: 0.5008 - val_loss: 0.6931 - val_acc: 0.5314\n",
      "current sigma: 0.365377\n",
      "Year 1: sigma=0.365377\n",
      "[[88116     0]\n",
      " [77700     0]]\n",
      "Train_data shape: (807295, 148)\n",
      "Test_data shape: (205057, 148)\n",
      "Train on 764820 samples, validate on 168336 samples\n",
      "Epoch 1/1\n",
      "764820/764820 [==============================] - 169s 221us/step - loss: 0.6931 - acc: 0.5080 - val_loss: 0.6933 - val_acc: 0.4985\n",
      "current sigma: -0.160585\n",
      "Train on 764820 samples, validate on 168336 samples\n",
      "Epoch 1/1\n",
      "764820/764820 [==============================] - 175s 228us/step - loss: 0.6930 - acc: 0.5083 - val_loss: 0.6933 - val_acc: 0.4985\n",
      "current sigma: -0.160585\n",
      "Train on 764820 samples, validate on 168336 samples\n",
      "Epoch 1/1\n",
      "764820/764820 [==============================] - 175s 229us/step - loss: 0.6930 - acc: 0.5083 - val_loss: 0.6933 - val_acc: 0.4985\n",
      "current sigma: -0.160585\n",
      "Train on 764820 samples, validate on 168336 samples\n",
      "Epoch 1/1\n",
      "764820/764820 [==============================] - 171s 223us/step - loss: 0.6930 - acc: 0.5083 - val_loss: 0.6933 - val_acc: 0.4985\n",
      "current sigma: -0.160585\n",
      "Train on 764820 samples, validate on 168336 samples\n",
      "Epoch 1/1\n",
      "764820/764820 [==============================] - 176s 231us/step - loss: 0.6930 - acc: 0.5083 - val_loss: 0.6933 - val_acc: 0.4985\n",
      "current sigma: -0.160585\n",
      "Train on 764820 samples, validate on 168336 samples\n",
      "Epoch 1/1\n",
      "764820/764820 [==============================] - 177s 232us/step - loss: 0.6930 - acc: 0.5083 - val_loss: 0.6933 - val_acc: 0.4985\n",
      "current sigma: -0.160585\n",
      "Train on 764820 samples, validate on 168336 samples\n",
      "Epoch 1/1\n",
      "764820/764820 [==============================] - 177s 231us/step - loss: 0.6930 - acc: 0.5083 - val_loss: 0.6933 - val_acc: 0.4985\n",
      "current sigma: -0.160585\n",
      "Train on 764820 samples, validate on 168336 samples\n",
      "Epoch 1/1\n",
      "764820/764820 [==============================] - 177s 232us/step - loss: 0.6930 - acc: 0.5083 - val_loss: 0.6933 - val_acc: 0.4985\n",
      "current sigma: -0.160585\n",
      "Train on 764820 samples, validate on 168336 samples\n",
      "Epoch 1/1\n",
      "764820/764820 [==============================] - 185s 242us/step - loss: 0.6930 - acc: 0.5083 - val_loss: 0.6933 - val_acc: 0.4985\n",
      "current sigma: -0.160585\n",
      "Train on 764820 samples, validate on 168336 samples\n",
      "Epoch 1/1\n",
      "764820/764820 [==============================] - 182s 238us/step - loss: 0.6930 - acc: 0.5083 - val_loss: 0.6933 - val_acc: 0.4985\n",
      "current sigma: -0.160585\n",
      "Train on 764820 samples, validate on 168336 samples\n",
      "Epoch 1/1\n",
      "764820/764820 [==============================] - 181s 237us/step - loss: 0.6930 - acc: 0.5083 - val_loss: 0.6933 - val_acc: 0.4985\n",
      "current sigma: -0.160585\n",
      "Train on 764820 samples, validate on 168336 samples\n",
      "Epoch 1/1\n",
      "764820/764820 [==============================] - 179s 234us/step - loss: 0.6930 - acc: 0.5083 - val_loss: 0.6933 - val_acc: 0.4985\n",
      "current sigma: -0.160585\n",
      "Train on 764820 samples, validate on 168336 samples\n",
      "Epoch 1/1\n",
      "764820/764820 [==============================] - 176s 231us/step - loss: 0.6930 - acc: 0.5083 - val_loss: 0.6933 - val_acc: 0.4985\n",
      "current sigma: -0.160585\n",
      "Train on 764820 samples, validate on 168336 samples\n",
      "Epoch 1/1\n",
      "764820/764820 [==============================] - 177s 232us/step - loss: 0.6930 - acc: 0.5083 - val_loss: 0.6933 - val_acc: 0.4985\n",
      "current sigma: -0.160585\n",
      "Train on 764820 samples, validate on 168336 samples\n",
      "Epoch 1/1\n",
      "764820/764820 [==============================] - 176s 230us/step - loss: 0.6930 - acc: 0.5083 - val_loss: 0.6933 - val_acc: 0.4985\n",
      "current sigma: -0.160585\n",
      "Train on 764820 samples, validate on 168336 samples\n",
      "Epoch 1/1\n",
      "764820/764820 [==============================] - 180s 235us/step - loss: 0.6930 - acc: 0.5083 - val_loss: 0.6933 - val_acc: 0.4985\n",
      "current sigma: -0.160585\n",
      "Train on 764820 samples, validate on 168336 samples\n",
      "Epoch 1/1\n",
      "764820/764820 [==============================] - 181s 236us/step - loss: 0.6930 - acc: 0.5083 - val_loss: 0.6933 - val_acc: 0.4985\n",
      "current sigma: -0.160585\n",
      "Train on 764820 samples, validate on 168336 samples\n",
      "Epoch 1/1\n",
      "764820/764820 [==============================] - 182s 238us/step - loss: 0.6930 - acc: 0.5083 - val_loss: 0.6933 - val_acc: 0.4985\n",
      "current sigma: -0.160585\n",
      "Train on 764820 samples, validate on 168336 samples\n",
      "Epoch 1/1\n",
      "764820/764820 [==============================] - 179s 234us/step - loss: 0.6930 - acc: 0.5083 - val_loss: 0.6933 - val_acc: 0.4985\n",
      "current sigma: -0.160585\n",
      "Train on 764820 samples, validate on 168336 samples\n",
      "Epoch 1/1\n",
      "764820/764820 [==============================] - 178s 232us/step - loss: 0.6930 - acc: 0.5083 - val_loss: 0.6933 - val_acc: 0.4985\n",
      "current sigma: -0.160585\n",
      "Train on 764820 samples, validate on 168336 samples\n",
      "Epoch 1/1\n",
      "764820/764820 [==============================] - 193s 252us/step - loss: 0.6930 - acc: 0.5083 - val_loss: 0.6933 - val_acc: 0.4985\n",
      "current sigma: -0.160585\n",
      "Train on 764820 samples, validate on 168336 samples\n",
      "Epoch 1/1\n",
      "764820/764820 [==============================] - 175s 229us/step - loss: 0.6930 - acc: 0.5083 - val_loss: 0.6933 - val_acc: 0.4985\n",
      "current sigma: -0.160585\n",
      "Train on 764820 samples, validate on 168336 samples\n",
      "Epoch 1/1\n",
      "764820/764820 [==============================] - 172s 224us/step - loss: 0.6930 - acc: 0.5083 - val_loss: 0.6933 - val_acc: 0.4985\n",
      "current sigma: -0.160585\n",
      "Train on 764820 samples, validate on 168336 samples\n",
      "Epoch 1/1\n",
      "764820/764820 [==============================] - 172s 225us/step - loss: 0.6930 - acc: 0.5083 - val_loss: 0.6933 - val_acc: 0.4985\n",
      "current sigma: -0.160585\n",
      "Train on 764820 samples, validate on 168336 samples\n",
      "Epoch 1/1\n",
      "764820/764820 [==============================] - 171s 224us/step - loss: 0.6930 - acc: 0.5083 - val_loss: 0.6933 - val_acc: 0.4985\n",
      "current sigma: -0.160585\n",
      "Train on 764820 samples, validate on 168336 samples\n",
      "Epoch 1/1\n",
      "764820/764820 [==============================] - 172s 224us/step - loss: 0.6930 - acc: 0.5083 - val_loss: 0.6933 - val_acc: 0.4985\n",
      "current sigma: -0.160585\n",
      "Train on 764820 samples, validate on 168336 samples\n",
      "Epoch 1/1\n",
      "764820/764820 [==============================] - 174s 227us/step - loss: 0.6930 - acc: 0.5083 - val_loss: 0.6933 - val_acc: 0.4985\n",
      "current sigma: -0.160585\n",
      "Train on 764820 samples, validate on 168336 samples\n",
      "Epoch 1/1\n",
      "764820/764820 [==============================] - 173s 226us/step - loss: 0.6930 - acc: 0.5083 - val_loss: 0.6933 - val_acc: 0.4985\n",
      "current sigma: -0.160585\n",
      "Train on 764820 samples, validate on 168336 samples\n",
      "Epoch 1/1\n",
      "764820/764820 [==============================] - 174s 227us/step - loss: 0.6930 - acc: 0.5083 - val_loss: 0.6933 - val_acc: 0.4985\n",
      "current sigma: -0.160585\n",
      "Train on 764820 samples, validate on 168336 samples\n",
      "Epoch 1/1\n",
      "764820/764820 [==============================] - 175s 228us/step - loss: 0.6930 - acc: 0.5083 - val_loss: 0.6933 - val_acc: 0.4985\n",
      "current sigma: -0.160585\n",
      "Train on 764820 samples, validate on 168336 samples\n",
      "Epoch 1/1\n",
      "764820/764820 [==============================] - 175s 228us/step - loss: 0.6930 - acc: 0.5083 - val_loss: 0.6933 - val_acc: 0.4985\n",
      "current sigma: -0.160585\n",
      "Train on 764820 samples, validate on 168336 samples\n",
      "Epoch 1/1\n",
      "764820/764820 [==============================] - 173s 227us/step - loss: 0.6930 - acc: 0.5083 - val_loss: 0.6933 - val_acc: 0.4985\n",
      "current sigma: -0.160585\n",
      "Train on 764820 samples, validate on 168336 samples\n",
      "Epoch 1/1\n",
      "764820/764820 [==============================] - 175s 228us/step - loss: 0.6930 - acc: 0.5083 - val_loss: 0.6933 - val_acc: 0.4985\n",
      "current sigma: -0.160585\n",
      "Train on 764820 samples, validate on 168336 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "764820/764820 [==============================] - 169s 221us/step - loss: 0.6930 - acc: 0.5083 - val_loss: 0.6933 - val_acc: 0.4985\n",
      "current sigma: -0.160585\n",
      "Train on 764820 samples, validate on 168336 samples\n",
      "Epoch 1/1\n",
      "764820/764820 [==============================] - 167s 218us/step - loss: 0.6930 - acc: 0.5083 - val_loss: 0.6933 - val_acc: 0.4985\n",
      "current sigma: -0.160585\n",
      "Train on 764820 samples, validate on 168336 samples\n",
      "Epoch 1/1\n",
      "764820/764820 [==============================] - 166s 217us/step - loss: 0.6930 - acc: 0.5083 - val_loss: 0.6933 - val_acc: 0.4985\n",
      "current sigma: -0.160585\n",
      "Train on 764820 samples, validate on 168336 samples\n",
      "Epoch 1/1\n",
      "764820/764820 [==============================] - 171s 224us/step - loss: 0.6930 - acc: 0.5083 - val_loss: 0.6933 - val_acc: 0.4985\n",
      "current sigma: -0.160585\n",
      "Train on 764820 samples, validate on 168336 samples\n",
      "Epoch 1/1\n",
      "764820/764820 [==============================] - 168s 219us/step - loss: 0.6930 - acc: 0.5083 - val_loss: 0.6933 - val_acc: 0.4985\n",
      "current sigma: -0.160585\n",
      "Train on 764820 samples, validate on 168336 samples\n",
      "Epoch 1/1\n",
      "764820/764820 [==============================] - 167s 218us/step - loss: 0.6930 - acc: 0.5083 - val_loss: 0.6933 - val_acc: 0.4985\n",
      "current sigma: -0.160585\n",
      "Train on 764820 samples, validate on 168336 samples\n",
      "Epoch 1/1\n",
      "612612/764820 [=======================>......] - ETA: 32s - loss: 0.6930 - acc: 0.5083"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1399\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1400\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# remark for Wei Han: add sentiments and other industry-neutralised features from v5\n",
    "\n",
    "v_str = 'v8'\n",
    "\n",
    "header_list = []\n",
    "for i in range(100):\n",
    "    header_list.append(\"word2vec_\" + str(i))\n",
    "\n",
    "train_end_years = [1, 2, 3, 4, 5]\n",
    "\n",
    "lstm_features_list = [\n",
    "    'returnsClosePrevRaw1',\n",
    "    'returnsOpenPrevRaw1',\n",
    "    'returnsClosePrevMktres1',\n",
    "    'returnsOpenPrevMktres1',\n",
    "    'returnsClosePrevRaw10',\n",
    "    'returnsOpenPrevRaw10',\n",
    "    'returnsClosePrevMktres10',\n",
    "    'returnsOpenPrevMktres10',\n",
    "    'open_close',\n",
    "    'oc_average',\n",
    "    'turnover',\n",
    "    'open_close_relative',\n",
    "    'turnover_relative',\n",
    "    'volume_relative',\n",
    "    'returnsOpenPrevMktres1_relative',\n",
    "    'returnsOpenPrevMktres10_relative',\n",
    "    'returnsClosePrevRaw1_stan',\n",
    "    'returnsOpenPrevRaw1_stan',\n",
    "    'returnsClosePrevMktres1_stan',\n",
    "    'returnsOpenPrevMktres1_stan',\n",
    "    'returnsClosePrevRaw10_stan',\n",
    "    'returnsOpenPrevRaw10_stan',\n",
    "    'returnsClosePrevMktres10_stan',\n",
    "    'returnsOpenPrevMktres10_stan',\n",
    "    'open_close_stan',\n",
    "    'oc_average_stan',\n",
    "    'turnover_stan',\n",
    "    'open_close_relative_stan',\n",
    "    'turnover_relative_stan',\n",
    "    'volume_relative_stan',\n",
    "    'returnsOpenPrevMktres1_relative_stan',\n",
    "    'returnsOpenPrevMktres10_relative_stan',\n",
    "]\n",
    "\n",
    "\n",
    "additional_features_list = [\n",
    "    'urgency_min',\n",
    "    'urgency_count',\n",
    "    'takeSequence_min',\n",
    "    'takeSequence_max',\n",
    "    'bodySize_mean',\n",
    "    'wordCount_mean',\n",
    "    'sentenceCount_mean',\n",
    "    'companyCount_mean',\n",
    "    'marketCommentary_mean',\n",
    "    'relevance_mean',\n",
    "    'sentimentNegative_mean',\n",
    "    'sentimentNeutral_mean',\n",
    "    'sentimentPositive_mean',\n",
    "    'sentimentWordCount_mean',\n",
    "    'sentimentNegative_mean_stan',\n",
    "    'sentimentNeutral_mean_stan',\n",
    "    'sentimentPositive_mean_stan',\n",
    "    'sentimentWordCount_mean_stan',\n",
    "    'noveltyCount12H_mean',\n",
    "    'noveltyCount24H_mean',\n",
    "    'noveltyCount3D_mean',\n",
    "    'noveltyCount5D_mean',\n",
    "    'noveltyCount7D_mean',\n",
    "    'volumeCounts12H_mean',\n",
    "    'volumeCounts24H_mean',\n",
    "    'volumeCounts3D_mean',\n",
    "    'volumeCounts5D_mean',\n",
    "    'volumeCounts7D_mean',\n",
    "]\n",
    "\n",
    "label_encoded_features = [\n",
    "    'dayofweek',\n",
    "    'month',\n",
    "]\n",
    "\n",
    "continuous_features_list = lstm_features_list + additional_features_list\n",
    "features_list = lstm_features_list + additional_features_list + label_encoded_features\n",
    "\n",
    "for train_end_year in train_end_years:\n",
    "#   # a trick to get testing stats\n",
    "#   if train_end_year == 2016:\n",
    "#     train_end_year = 2015\n",
    "#     train_start, train_end = pd.to_datetime('2011-01-01'), pd.to_datetime('2016-06-30')\n",
    "#     test_start, test_end = pd.to_datetime('%d-07-01' % (train_end_year + 1)), pd.to_datetime('%d-12-31' % (train_end_year + 1))\n",
    "#     train_end_year = 2016\n",
    "#   else:\n",
    "#     train_start, train_end = pd.to_datetime('%d-01-01' % train_end_year), pd.to_datetime('%d-12-31' % train_end_year)\n",
    "#     test_start, test_end = pd.to_datetime('%d-01-01' % (train_end_year + 1)), pd.to_datetime('%d-06-30' % (train_end_year + 1))\n",
    "  \n",
    "  \n",
    "  # divide into train and test periods\n",
    "\n",
    "  train_data = pd.read_pickle(\"./Data/train_cluster_standardised_%d.p\" % train_end_year)\n",
    "  test_data = pd.read_pickle(\"./Data/test_cluster_standardised_%d.p\" % train_end_year)\n",
    "    \n",
    "  def remove_duplicates_from_df(df):\n",
    "    header_list = []\n",
    "    for i in range(100):\n",
    "        header_list.append(\"word2vec_\" + str(i))\n",
    "        header_list.append(\"word2vec_\" + str(i) + \"_stan\")\n",
    "\n",
    "    df.drop(header_list, axis=1, inplace=True) #remove word2vec_%d_stan columns\n",
    "    \n",
    "    column_names = df.columns.tolist()\n",
    "    dup_dic = {}\n",
    "    for i in range(len(column_names)):\n",
    "        curr_pos = i\n",
    "        if column_names[i] not in dup_dic:\n",
    "            dup_dic[column_names[i]] = [1, curr_pos]\n",
    "        elif column_names[i] in dup_dic:\n",
    "            initial_pos = dup_dic[column_names[i]][1]\n",
    "            dup_dic[column_names[i]][0] + 1\n",
    "            dup_dic[column_names[i]] = [dup_dic[column_names[i]][0] + 1, initial_pos, curr_pos]\n",
    "\n",
    "    columns_to_drop_index = []\n",
    "    for k, v in dup_dic.items():\n",
    "        if v[0] == 2:\n",
    "            columns_to_drop_index.append(v[1])\n",
    "    \n",
    "    df_column_range = list(range(df.shape[1]))\n",
    "    unique_columns_index_list = [index for index in df_column_range if index not in columns_to_drop_index]\n",
    "    df = df.iloc[:, unique_columns_index_list] #return all unique columns\n",
    "    \n",
    "    return df\n",
    "    \n",
    "  train_data = remove_duplicates_from_df(train_data)\n",
    "  test_data = remove_duplicates_from_df(test_data)\n",
    "    \n",
    "  print(\"Train_data shape:\", train_data.shape)\n",
    "  print(\"Test_data shape:\", test_data.shape)\n",
    "    \n",
    "#   from sklearn.preprocessing import StandardScaler\n",
    "#   scaler = StandardScaler()\n",
    "#   train_data[continuous_features_list] = scaler.fit_transform(train_data[continuous_features_list])\n",
    "#   test_data[continuous_features_list] = scaler.transform(test_data[continuous_features_list])\n",
    "\n",
    "  # reshape data\n",
    "  word2vec_features_list = []\n",
    "\n",
    "  def reshape_data(data, sequence_len=21, features_list=features_list):\n",
    "    data_list, ann_list, embedding_list, word2vec_list, target_list, returns_list, date_list = [], [], [], [], [], [], []\n",
    "\n",
    "    for assetCode, asset_data in data.groupby('assetCode'):\n",
    "      # binarize the target variable\n",
    "      returns = asset_data['returnsOpenNextMktres10']\n",
    "      target = (asset_data['returnsOpenNextMktres10'] > 0).astype(int)\n",
    "      dates = asset_data.index\n",
    "\n",
    "      # arrays\n",
    "      lstm_data = asset_data[lstm_features_list].values\n",
    "      ann_data = asset_data[additional_features_list].values\n",
    "      embedding_data = asset_data[label_encoded_features].values\n",
    "      word2vec_data = asset_data[word2vec_features_list].values\n",
    "    \n",
    "      col_mean = np.nanmean(lstm_data, axis = 0) \n",
    "      inds = np.where(np.isnan(lstm_data)) \n",
    "      lstm_data[inds] = np.take(col_mean, inds[1])\n",
    "\n",
    "      reshaped_data = np.repeat(lstm_data[:, np.newaxis, :], sequence_len, axis=1)\n",
    "      for i in range(reshaped_data.shape[1]):\n",
    "        reshaped_data[:, i, :] = np.roll(reshaped_data[:, i, :], i, axis=0)\n",
    "\n",
    "      # discard the top rows and reverse the order of the shifted sequence to reflect the correct order\n",
    "      reshaped_data = reshaped_data[sequence_len-1:, ::-1, :]\n",
    "      ann_data = ann_data[sequence_len-1:, :]\n",
    "      embedding_data = embedding_data[sequence_len-1:, :]\n",
    "      word2vec_data = word2vec_data[sequence_len-1:, :]\n",
    "      target = target.values[sequence_len-1:]\n",
    "      returns = returns.values[sequence_len-1:]\n",
    "      dates = dates[sequence_len-1:]\n",
    "\n",
    "      data_list.append(reshaped_data)\n",
    "      ann_list.append(ann_data)\n",
    "      embedding_list.append(embedding_data)\n",
    "      word2vec_list.append(word2vec_data)\n",
    "      target_list.append(target)\n",
    "      returns_list.append(returns)\n",
    "      date_list.append(dates)\n",
    "\n",
    "    reshaped_data, ann_data, embedding_data, word2vec_data, target, returns, dates = np.concatenate(data_list), np.concatenate(ann_list), np.concatenate(embedding_list), np.concatenate(word2vec_list), np.concatenate(target_list), np.concatenate(returns_list), np.concatenate(date_list)\n",
    "\n",
    "    return (reshaped_data, ann_data, embedding_data, word2vec_data), target, returns, dates\n",
    "\n",
    "  x_train_tuple, y_train, train_returns, train_dates = reshape_data(train_data)\n",
    "  x_test_tuple, y_test, test_returns, test_dates = reshape_data(test_data)\n",
    "\n",
    "  x_train_lstm, x_train_ann, x_train_embedding, x_train_word2vec = x_train_tuple\n",
    "  x_test_lstm, x_test_ann, x_test_embedding, x_test_word2vec = x_test_tuple\n",
    "  \n",
    "  # create model\n",
    "  from keras.layers import Input, Embedding, LSTM, Dense\n",
    "  from keras.models import Model\n",
    "\n",
    "  lstm_params = {\n",
    "      'batch_size': 252,\n",
    "      'epochs': 5,\n",
    "  }\n",
    "\n",
    "  train_len = x_train_lstm.shape[0]\n",
    "  test_len = x_test_lstm.shape[0]\n",
    "\n",
    "  # tweak to match with batch_size\n",
    "  train_until = train_len - train_len % lstm_params['batch_size']\n",
    "  test_until = test_len - test_len % lstm_params['batch_size']\n",
    "\n",
    "  x_train_lstm, x_train_ann, x_train_embedding, x_train_word2vec = x_train_lstm[:train_until], x_train_ann[:train_until], x_train_embedding[:train_until], x_train_word2vec[:train_until]\n",
    "  y_train = y_train[:train_until]\n",
    "  train_returns = train_returns[:train_until]\n",
    "  train_dates = train_dates[:train_until]\n",
    "  x_test_lstm, x_test_ann, x_test_embedding, x_test_word2vec = x_test_lstm[:test_until], x_test_ann[:test_until], x_test_embedding[:test_until], x_test_word2vec[:test_until]\n",
    "  y_test = y_test[:test_until]\n",
    "  test_returns = test_returns[:test_until]\n",
    "  test_dates = test_dates[:test_until]\n",
    "\n",
    "  train_len = x_train_lstm.shape[0]\n",
    "  test_len = x_test_lstm.shape[0]\n",
    "\n",
    "  np.random.seed(1)\n",
    "  train_shuffle = np.random.permutation(range(train_len))\n",
    "  test_shuffle = np.random.permutation(range(test_len))\n",
    "  x_train_lstm, x_train_ann, x_train_embedding, x_train_word2vec, y_train, train_returns, train_dates = x_train_lstm[train_shuffle], x_train_ann[train_shuffle], x_train_embedding[train_shuffle], x_train_word2vec[train_shuffle], y_train[train_shuffle], train_returns[train_shuffle], train_dates[train_shuffle]\n",
    "  x_test_lstm, x_test_ann, x_test_embedding, x_test_word2vec, y_test, test_returns, test_dates = x_test_lstm[test_shuffle], x_test_ann[test_shuffle], x_test_embedding[test_shuffle], x_test_word2vec[test_shuffle], y_test[test_shuffle], test_returns[test_shuffle], test_dates[test_shuffle]\n",
    "\n",
    "  def create_model():\n",
    "    sequence_len = x_train_lstm.shape[1]\n",
    "\n",
    "    lstm_input = Input(shape=(sequence_len, len(lstm_features_list)), name='lstm_input')\n",
    "    lstm_out = LSTM(21)(lstm_input)\n",
    "\n",
    "    additional_input = Input(shape=(len(additional_features_list),), name='additional_input')\n",
    "  #   word2vec_input = Input(shape=(len(word2vec_features_list),), name='word2vec_input')\n",
    "\n",
    "  #   concat_layers = [lstm_out, additional_input, word2vec_input]\n",
    "    concat_layers = [lstm_out, additional_input]\n",
    "\n",
    "  #   input_layers = [lstm_input, additional_input, word2vec_input]\n",
    "    input_layers = [lstm_input, additional_input]\n",
    "\n",
    "    # embeddings\n",
    "    for i in range(x_train_embedding.shape[1]):\n",
    "      vocab_size = x_train_embedding[:, i].max() + 1\n",
    "      y = Input(shape=(1,), dtype='int32', name='embedding_input_%d' % i)\n",
    "      input_layers.append(y)\n",
    "      y = Embedding(input_dim=vocab_size, output_dim=21)(y)\n",
    "      concat_layers.append(Flatten()(y))\n",
    "\n",
    "    x = keras.layers.concatenate(concat_layers)\n",
    "\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dropout(.2)(x)\n",
    "\n",
    "    main_output = Dense(1, activation='sigmoid', name='output')(x)\n",
    "\n",
    "    model = Model(inputs=input_layers, outputs=[main_output])\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "  \n",
    "  def get_sigma(scaled_scores, test_returns, test_dates):\n",
    "    '''\n",
    "    scaled_scores: numpy array\n",
    "    test_returns: numpy array\n",
    "    test_dates: numpy array\n",
    "    '''\n",
    "\n",
    "    grouped_dict = pd.Series(scaled_scores).index.groupby(test_dates)\n",
    "\n",
    "    day_scores = []\n",
    "    for ts in grouped_dict:\n",
    "      grouped_index = grouped_dict[ts]\n",
    "      day_score = np.dot(test_returns[grouped_index], scaled_scores[grouped_index])\n",
    "      day_scores.append(day_score)\n",
    "\n",
    "    return np.mean(day_scores) / np.std(day_scores, ddof=1)\n",
    "\n",
    "\n",
    "  prev_sigma = None\n",
    "  combined_model = create_model()\n",
    "\n",
    "  # train for at most 500 epochs\n",
    "  for i in range(500):\n",
    "    # x_train_list = [x_train_lstm, x_train_ann] + [x_train_word2vec] + [x_train_embedding[:, i] for i in range(x_train_embedding.shape[1])]\n",
    "    x_train_list = [x_train_lstm, x_train_ann] + [x_train_embedding[:, i] for i in range(x_train_embedding.shape[1])]\n",
    "    # x_test_list = [x_test_lstm, x_test_ann] + [x_test_word2vec] + [x_test_embedding[:, i] for i in range(x_test_embedding.shape[1])]\n",
    "    x_test_list = [x_test_lstm, x_test_ann] + [x_test_embedding[:, i] for i in range(x_test_embedding.shape[1])]\n",
    "\n",
    "    combined_model.fit(x_train_list,\n",
    "                   y_train,\n",
    "                   batch_size=lstm_params['batch_size'],\n",
    "                   epochs=1,\n",
    "                   verbose=1,\n",
    "                   validation_data=(x_test_list, y_test),\n",
    "                   shuffle=False)\n",
    "\n",
    "    scaled_scores = combined_model.predict(x_test_list, batch_size=lstm_params['batch_size']).reshape(-1) * 2 - 1\n",
    "    current_sigma = get_sigma(scaled_scores, test_returns, test_dates)\n",
    "    \n",
    "    print('current sigma: %f' % current_sigma)\n",
    "\n",
    "    # early stopping if sigma score does not improve for 1 epoch\n",
    "    if prev_sigma is not None and current_sigma < prev_sigma:\n",
    "      break\n",
    "    else:\n",
    "      prev_sigma = current_sigma\n",
    "  \n",
    "  pred_classes = (scaled_scores > 0).astype(int)\n",
    "  from sklearn.metrics import confusion_matrix\n",
    "  conf_mat = confusion_matrix(y_test, pred_classes)\n",
    "  \n",
    "  print('Year %d: sigma=%f' % (train_end_year, current_sigma))\n",
    "  print(conf_mat)\n",
    "  \n",
    "  if v_str not in results:\n",
    "    results[v_str] = {}\n",
    "  results[v_str][train_end_year] = {}\n",
    "  results[v_str][train_end_year]['y_test'] = y_test\n",
    "  results[v_str][train_end_year]['scaled_scores'] = scaled_scores\n",
    "  results[v_str][train_end_year]['sigma_score'] = current_sigma\n",
    "  results[v_str][train_end_year]['conf_mat'] = conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"growing_final_output_from_results_dict_V8.p\"\n",
    "fileobj = open(filename, \"wb\")\n",
    "pickle.dump(results, fileobj)\n",
    "fileobj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DNN (2 April Running Version).ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
